---
layout: post
title: Attempting to Alleviate Some of the Anxiety Brought by LLMs (Gemini 2.5 Pro Translated Version)
date: 2023-04-24 21:04:00
description: Mai-Haishin · April 24, 2023, 21:04 · Guangdong
tags: philosophy
disqus_comments: true
categories: English
---

As everyone knows, LLMs have currently caused immense panic, a wave of reflection, and a sense of anxiety. These conditions are particularly evident in how (self-)media, whether or not their main business relies on dissemination, have begun to ubiquitously spread news about ChatGPT, tutorials on LLMs, industry dynamics, and so on. Simultaneously, roughly several hundred large, medium, and small domestic corporate entities (including those not yet registered) have started to emerge, rebranding their focus to conversational LLMs or benchmarking themselves against OpenAI. Concurrently, a certain directly-administered municipal government organization published a "White Paper on the Development of the Artificial Intelligence Industry," proposing support for "leading enterprises to build large models benchmarked against ChatGPT." Of course, the worst of all was actually that open letter to "Pause Giant AI Experiments," which fully conveyed a sentiment of "If you don't let me on the bus, I'll just tear up the road for you."

In fact, as the title of this article suggests, I am only willing (and only able) to try to help alleviate some anxiety for everyone. As for panic and reflection, those are things I'm rather happy to see. Or rather, I have ample reason to explain why panic and reflection are what rational people should do, but anxiety is not. The main reasons why we believe LLMs, or more accurately, OpenAI's GPT series, have brought us such immense panic and require us to engage in paradigm-shifting reflection, are roughly threefold:

1. OpenAI achieved the direct deployment of ultra-large models that were previously considered (especially by domestic tech companies) to have no practical value, even if it meant losing money on every call.
2. OpenAI achieved incredibly pure labor exploitation, thereby obtaining immeasurable training data, and through a preemptive strategy (also losing money on every call), obtained even more immeasurable user feedback data.
3. OpenAI proved that if something is done well enough, it can afford to be "bad enough" in other aspects (e.g., relying on open-source datasets, or the aforementioned points I & II) without needing a specific product methodology.

Of course, this list of facts could extend to hundreds if one wished, much like how Operation Desert Storm served as a wake-up call for a certain large East Asian administrative organization. But that's not today's topic, so if readers wish to express their opinions amicably, please feel free to do so. Turning back to today's theme, I will discuss in three points why anxiety is unnecessary (or why I cannot provide an argument that anxiety is necessary).

I. Whether You Are Anxious or Not, You Can't Do Much About It Now

On a personal level, a certain大佬's (big shot's) "Dos and Don'ts in the LLM Era" has been widely circulated in social feeds. I won't go into detail, but my assessment is that it's positive thinking amidst a negative situation; in other words, ultimately, you can't do much, so doing your own thing well is the right approach.

Of course, what I really want to talk about is the organizational level. Even if an individual does their own things well, if you truly have the ambition to solve "it," you still have to find an organization to realize it. Unfortunately, I currently hold an extremely pessimistic view of domestic organizations. In other words, everyone seems to be fiercely competing ("involution"), but in the end, I'm afraid no one can produce anything that would truly make the West look over its shoulder. In my understanding, for domestic organizations to even create something comparable to GPT, given they are already significantly behind, there are only two models to choose from—and these are not the "Yanhong (Robin Li of Baidu) model" (bad money drives out good) or the "Huiwen (Wang Huiwen, ex-Meituan, now LLM entrepreneur) model" (Wagang Army - a historical rebel group, implying a scrappy but perhaps ultimately outmatched effort). Instead, they are the MBS model or the "Benevolent Father" (Stalin) model.

The MBS (Mohammed bin Salman) model refers to limitless investment. Hardware investment starts directly from buying land to build nuclear power plants. On the software side, use money to poach all of OpenAI's core members. If that's not possible, then use money to poach domestic big shots in droves. Everyone, along with their most brilliant Ph.D. students, gets directly involved in hands-on work. Finally, build a Faustian data factory that produces data relentlessly year-round. Then, multiply all of the above by three, conduct multiple backups like the Manhattan Project, and leverage the entire conglomerate's power to support this endeavor. Don't even think about mere tens of millions of dollars; start with at least Microsoft's $10 billion. What else?

The "Benevolent Father" (Ио́сиф Виссарио́нович Ста́лин - Joseph Vissarionovich Stalin) model, correspondingly, is for if you don't have an endless budget. In that case, simply find a group of young people, regardless of their background, build a city in the wilderness of Siberia (OK, Lop Nor), solve all their life's problems, let them innovate freely, and slowly and steadily cultivate a batch of genuinely talented individuals. Eliminate the current浮躁 (fúzào - impetuousness, superficiality) in academia. Strive to form a complete knowledge system from fundamentals to engineering within 5 to 10 years, and then compete with the West.

II. LLMs Themselves Might Not Be a Feasible Path to Strong AGI

We won't repeatedly discuss what strong AGI is here. Of course, I can't help but add: if "being able to converse like a general human" is the standard, then GPT-4 is not just strong AGI, it's practically a god in this world. If you ask it to write a white paper promoting the large model industry in a certain domestic administrative region, the result would likely be many times better than what an average human in our administrative organizations could produce. My main point here concerns the inherent characteristics of language, which can be summarized in the following three levels:

1. As a representation of the real world, language itself is ambiguous. The thoughts generated from the same language and the outputs generated from the same thought both vary. For a simple example: your child scores 60 on an exam. You calmly say, "That's a really good score." Your child, overjoyed, asks if you can reward him by letting him pay for two in-game purchase bundles. So, you reward him with a "beating" (literally "alpaca stampede" - a euphemism).
2. Precisely because of ambiguity, humans are inherently biased in their use of language, which allows for consistency in the thoughts an individual associates with language. LLMs, learned from vast amounts of data, may lose this bias. Continuing the example above: if we consider the phrase to be sarcastic at a score of 60, at what score does it cease to be sarcastic? There's no definitive standard; it follows the inherent bias in a person's understanding of the world. In other words, someone who likes to say "That's a really good score" might do so simply because they enjoy using it for sarcasm, irrespective of the actual score. Big data, naturally, will erase this bias. An LLM might learn a normal distribution based on scores, and we would determine whether the phrase is encouraging or mocking based on sampling.
3. Once bias is lost, an LLM cannot create new knowledge or concepts based on a particular thought, thereby deviating from the definition of strong AGI. In other words, new knowledge or concepts based on language are essentially products of old knowledge or concepts combined with bias. Just like the character "翔" (xiáng), which purely meant "to soar" a few years ago, only acquired its current meaning ("shit," through internet slang) due to an individual's linguistic bias and the dramatic effect this bias itself created, leading to its emergence and spread.

III. Anxiety Itself Is Your Very Being

Anxiety itself is not an inherently negative concept. People devalue anxiety more often due to mundane biases. From a physiological perspective, anxiety is an emotional upheaval and unpleasant feeling caused by uncertainty about expectations (usually about potentially negative outcomes), an extremely normal reaction for humans. Distinguished from anxiety disorders, moderate anxiety is a condition that drives human self-improvement. So, you might as well drink this bowl of "anxiety chicken soup for the soul" and battle those NaNs all night for another year.

From the perspective of Heidegger's existential philosophy, what LLMs bring me, rather than being called anxiety, is more accurately a concretized Angst (dread/anguish), which is Dasein's (being-there's) being itself. Although LLMs don't bring me unavoidable death (though LLMs have indeed brought social death to many), they bring a primal pursuit of wisdom. If I am not on the path of seeking wisdom, then I am, in effect, already dead.

The above, I hope everyone likes it.
