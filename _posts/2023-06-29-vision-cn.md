---
layout: post
title: 视觉大模型一无是处
date: 2023-06-29 16:40:16
description: 蚂蚁海星 · 2023年06月29日 21:15 · 广东
tags: philosophy
disqus_comments: true
categories: Chinese
---
最近各种机缘巧合，反复被人问到一个让我非常反感的问题：你的视觉模型有多少参数？虽然Ph.D的职业训练能让我的表情依旧处在1940年欧洲版图那种毫无波兰的状态，但是刨根问底地讲，这个问题所诱发的，对meaninglessitude的观念已经让我形成了一种理性上的反感。

一、22B与175B的差距差不多是175B

首先我还是不否定，随着视觉模型参数量的提升，其在传统的视觉任务上的绝对数值表现也有相应的提升。近期出现的最好的一个例子就是Google的ViT-22B，虽然用了少量的没有开源的数据以及~~保守的~~炼丹技巧才让这个模型跑起来。但是这不妨碍圈内大体保持一个乐观的态度，认为计算机视觉依旧是配得上大模型的。然而在我看来，这个存在感薄的宛如arxiv主页一般的视觉最大往生模型，恰巧完成了“堆积参数与数据量对于纯视觉这个领域毫无意义”的证明。

稍微思考一下，这个模型的训练量实际上达到了(JFT-)4B \* 256token/img \* 3epoch约等于3T的tokens，这已经比LLaMa最大的模型的训练量还要大一倍。但是ViT-22B上并没有什么真的有意义的结论，除了在经典旧世中的数据集上又多涨了三五斗，也就是这个模型更偏向于sketch而不是texture这些泛空的内容。拿着这个ROI堪比春夏季大反攻的结论，视觉还会有接下来的一个大模型吗，我看是不会有了。然而别忘了，22B与175B的差距差不多是175B。

二、视觉模型再大也依旧愚钝

在进入下一段的形上学内容之前，我先从经验主义的角度出发，复述三个“现代计算机视觉头顶的乌云”。当然，这些问题并不是没有得到重视，因为每年都持续的会有海量相关的文章这些问题上吃SR——这也确实印证了它们是难以被解决的。

首先要拿出来讲的自然是陈词滥调的adversarial attack问题。平心而论虽然NLP领域也有这个问题，但是当NLP迈入(generative)LLM时代之后这个问题也几乎不再被重视了。反观CV领域，这个问题就像是怨魂一样纠缠着每一个倒霉的审稿人，因为没有人站出来写这样一篇文章，说给他们一个大模型他们就能终结这个领域。而且甚至于现在都没有人愿意去测试这些视觉大模型的对抗攻击问题，因为人们已经普遍接受了这么一个理念，就是视觉模型就应该被攻击。

其次则是房间里的大象的问题，于2018年被提出来用以质疑detection模型与算法的有效性。大意是说，即便我将一只大象p到一个普通房间模样的照片中，我们充满智慧的模型也可以IOU>0.9地将它检测出来，即便这从经验之理中来看是荒谬的。这其实是个充满哲理而且可能影响到CV根基的问题，只不过解决它与否并不影响一个算法在coco上的性能（或者说只会有负影响），所以人们也只管当它是房间里的大象。

最后第三朵乌云就更现实一点，我愿意称之为样本空间的无序化问题。换言之，视觉信息在样本的特征空间中无法形成体系性结构。这里，视觉信息就是指图片中存在的可以抽象出的语义，体系性结构指的则是这些语义相互之间的关系。即便现在先进的自监督训练技术，可以很好的让某一类事物在“不需要知晓其语义的”情况下抱团取暖（体现为t-SNE的良好可区分性），但是从语义的角度来看，航天飞机的一个很小的r-neighbourhood内既可以有短毛橘猫也可以有果酱面包。往好的讲，这提高了基于视觉特征的创造力，往不好的讲，这倒逼视觉的研究者只能不断去寻求基于提高（特定benchmark的性能）力来创造视觉特征。

三、NLP的知识与CV的知识

首先我们需要确认两个概念，因为这两个概念的精准定义是一个长久以来都纠缠不清的问题，所以这里我们采取ChatGPT提供的通识性定义：

Representation（表象，也作表征）：在心灵哲学和认识论中，"representation"指的是一种心智状态或实在，用以指代某一件事物。表象的定义中蕴涵了我们的心灵具备引用或表达存在于世界中的物体、思想或情况的能力。表象可以采取多种形式，包括心理图像、信念、思想或语言符号，可以被看作是心灵与外部世界之间的中介，使我们能够对世界有知识、感知和理解。

Concept（概念）：另一方面在认识论中，"concept"是一个抽象或一般性的心理表象，包括一种或一类的事物、事件或观念。概念是思维和语言的基本构建单元，使我们能够对经验和知识进行分类和组织。对照单一的表象，概念的范围更为抽象和概括。概念通过抽象和概括的过程形成，我们在多个实例中识别共同的特征或属性，并创建一个心理范畴来代表这些共享的特征。

武断地讲，“表象”和“概念”都涉及到表达和传达某些信息或意义，但它们的本质和来源是不同的。表象更多的可以被看作是一种心理层面的现象，而概念则可以被认为是一种思维层面的产物。

事实上我认为搞明白这两个概念了之后，在认知层面上NLP与CV模型所获得的知识便很容易区分理解。我认为在语言（特别是generative）模型的框架内，我们为了训练一个模型，所提供的数据实质上展现的是Relations of Concepts（这里借用了Hume的理念，但是区别于纯粹的演绎，语言模型中的Relations依旧是通过归纳获得）。在此之上，对于某个Concept本身的理解，语言模型也是通过其与其他的Concepts之间的关联来获得一个间接的认知，而并没有一个直接的类似于人类心灵上的直接理解（特别是诸如“时间”等概念）。换而言之，语言模型所学习到的，是诸多由Concepts之间的关联所组成的Representations，以及由这些表象之间的关联所呈现出来更深刻更复杂的表象，而人类（语言）世界中所存在的或者所能被理解的表象多若繁星，我们需要万亿的参数去记忆这些表象，以及理解这些表象之间的关联性（或者说一个由表象为量子的N-gram）。

反观视觉模型，在不论有监督或者自/无监督的（几乎所有的）训练框架内，其优化目标实质上是将特定的具体的数据，转化为特定的唯一的Representation，并经由特定的视觉任务（即人为的经验）最终将这些表象抽象为Concept，换言之是一个学习Matter of Facts的过程。这里再额外解释一下为什么需要经由特定的视觉任务，有过（自监督）视觉大模型训练经验的人大多会有这样一个认知，就是视觉模型的效果很难去做evaluatation，大家只是从k-NN/Linear Probe/Attentive Probe/Finetune（分类或者其他下游任务）中选择性能好的来写论文，但实际上这里的本质原因是，在没有人为的经验来帮助视觉模型去做抽象的时候，一个算法不会去主动的来做这样的抽象（它没有Loss也没有义务去做），它只需要将数据与表象去做对应即可。反过来讲如果在算法的设计上就已经考虑到了抽象Concept的路径，那么这个模型在Linear Probe的性能自然会好一些（例如杨立昆君的I-JEPA，下面会再讲到）。

四、参数量解决不了抽象概念的问题

现在再回到那该死的参数量上，对于现阶段的视觉任务，其对应模型的参数量恐怕并不能成为让「视觉大模型出现chatGPT时刻」的主要指标。再回到现代计算机视觉头顶的那三朵乌云：

参数量只能在概念归纳的强度上获得边际的收益。一般来讲，视觉模型在经由表象来归纳概念时，脱离不了一个大致的聚类的概念。换句话说，我们预先会有一个“同一概念的表象会聚集在一起”的先验知识，例如Gaussian Prior，我们再经由实际的表象的分布来获得后验的结果。但是通过增加参数量来更精准的表达表象在向量空间的位置，其收益注定会逐渐边际化，因为参数量并不会与误差有一个线性的关系。以至于在某一个时刻开始，在有限的样本数量下，误差更多来源于先验的分布，最终提升参数量只能表现为ImageNet上那0.1%的性能提升。除此之外，增加参数量所引发的副效果，更高的向量空间的维度，也会增加归纳一个概念时所需的表象的数量（维度诅咒），从而也就会为对抗样本提供了更多的存在的空间。

参数量并不能脱离归纳来知晓表象或概念之间的存在逻辑。从道理上来讲，神经网络可以Universal Approaximation，而且这个能力会随着参数量的增加而变强。事实上以我个人的经验，大体上来说一个参数量更多的模型，是可以用单一的特征向量来表达某一个相对复杂的场景的，例如一个盘子里面装着各种不同的水果摆件，只不过我们无法提供高质量的人为的经验去概念化此类东西，在这个例子里面通常来说我们只能提供诸如“静物”“果盘”或者“一个盘子里面装着各种不同的水果摆件”之类的概念或其组合。只不过这种概念对于实际上模型所能表达的表象来说过于的抽象，模型通常只能去概括的统计更初级表象的量来组合成此类高度复杂的表象。所以如果缺乏可以枚举“与”“或”“非”等关联的数据的引导，模型自然也不会去主动因为存在或者不存在某一个初级表象，就将一个复杂的表象去彻底归为不同的概念。例如，在缺乏数据的情况下，模型不太会因为一个“森林”的图片里面有一辆自行车，就把它归类为“公园”，或者因为缺少一条河流，就把它归类为“青山”。

参数量也不能解决样本空间的无序化问题，从而无法让模型在脱离人为的经验的情况下，自发地学到由概念之间的关联而产生的新的概念。在自然语言当中，概念本身是有很好的层级化关联结构的，故而由概念或者表象的关联而形成的表象（例如一句话）也会因此获得一个结构性的表达，基于这种结构性的表达，我们会很容易继续创造新的概念或者表象。反观视觉中信息缺乏此类的层级化关联结构，视觉感官上相似的表象对应的概念可以完全不一样（例如字母l与数字1），所以在缺乏人为经验的情况下（自/无监督），在（表象的）特征空间里面也难以自发地形成结构性的表达，继而无法自发地形成新的概念。一个显著的例子就是，在跨模特的Align模型出现之前，几乎没有什么视觉算法可以做generalized zero-shot learning，即便是在某个特定的domain上（CUB鸟或者oxford花），zero-shot的性能也很难看。在大语言模型可以通过纯粹的语言概念解读画出独角兽的同时，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”~~（除非毛子们能给它提供足够多的人为的经验）。~~

五、Naive Cross-Modality Alignment并不解决根本问题

有碰过Stable Diffusion的孩子们应该会注意到这样一个现象，就是绝大多数的prompt都是宛如咒语一般的独立的单词，而真正用自然语言描述的场景却很难被准确的作画。有直接碰过CLIP的孩子们也应该会注意到这样一个现象，绝大多数图文匹配的分数（无论正负样本）都分布在一个较为狭小的区间，而且难以采用一个直观的阈值来判断两者的匹配度。排除掉模型训练相关的原因，这凸显了两个问题：

视觉端能精准学到的Concept的量是有限的，且更多的是简单的单词/短语级别的Concept。实际上在本人实际训练中文CLIP的时候，就遇到了学习到的top 100万个Concept里面里面有80万都是人名的状况（是的，再往后人名的比重会更高）。而这个其实是符合真实世界的分布，诸君可以现场站起来，用语言描述一下自己所能看见的物件，就会发现真实生活中接触到的相对简单的视觉Concept相当的贫乏，而更多的是身边的人的名字。

视觉端并不是不可以学到一个相对复杂的表象，但是视觉端无法像文本端一样把这种复杂的表象归纳称为概念，这就导致了视觉文本两侧的匹配会出现偏差，而实际上训练数据中能供视觉端归纳出一个复杂表象所对应的概念的数据是有限的，也就是说，即便有语言的帮助，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”（除非毛子们能给它提供足够多的人为的经验）。

所以，为什么Lock image Tuning现在会成为一个比较合理的CLIP训练模式，因为用语言将视觉端学到的表象归纳为概念这件事情的难度，远小于用视觉去理解语言中由多个抽象的概念构成的复杂表象。

六、皈依 or not皈依语言模型

最后讨论一下视觉模型往大做的出路在哪里。当然这里是要排除掉纯粹以学习表象为目的的模型的，例如围棋或者天气的模型，这些模型本身的能力本就展现在学习到人类无法用语言抽象的神秘的表象上，这些模型随着数据规模的增大，参数量自然需要scale-up来提升表象的能力（例如在99x99的围棋盘上，一个千亿参数的模型的性能理论上来讲必然好过一个百亿的）。

现在的学术大致上寻找到了两个出路，其一便是让视觉模型彻底的皈依大的语言模型的能力（例如Google丢出来的PaLM-E以及今年的CVPR最佳论文Visual Programming），让视觉模型回归一个为概念提供视觉表象的功能，让大语言模型来完成更为复杂的概念的解读。这个时候视觉模型的参数量即可以用来学习一些人类无法用语言抽象的表象上（例如depth map，optical flow，hyperspectral singal），从而补齐语言模型在空间推理上的一些弱点，让其更加的接近真实世界的AGI。但是这种模式依旧依赖于毛子去打坦克搜集大量的数据用以提供人为的经验，未来的路就仿佛路易十六39岁时的样子——一眼望不到头。

另外一个思路则体现在杨立昆君的I-JEPA以及feifei大佬的SiamMAE里面，我们强行的去让视觉模型去理解表象之间的关联，而这种任务本身对有注意力机制的ViT模型来说并不是特别困难。这样做最大的优点在于，它可以部分地解决前述的第二三朵乌云，但是这些方案由于突出强调个体表象，就比较难以学习到基于多种表象与概念而形成的复杂表象，特别体现在ImageNet的Finetune效果会比较差（ImageNet有一些相对复杂的场景类别）。而事实上，当下视觉的学术圈不怎么容得下一个性能不怎么好的算法，任何一点开放的探索都会被操着一口Chinglish的审稿人百般刁难，新的算法仿佛被困在了一个无尽的黑洞中难以诞生。

当然如果你们关心我是怎么回答“你的视觉模型有多少参数？”这个问题的话，我一般是会冷漠地讲“ViT-B，88M，再大的模型他们不给上线了”。
