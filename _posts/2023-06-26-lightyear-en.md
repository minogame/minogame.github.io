---
layout: post
title: A Sarcastic Take on a Company 0.3 Parsecs Away (DeepSeek Translated Version)
date: 2023-06-26 13:32
description: Mai-Haishin · June 26, 2023, 13:32 · Guangdong
tags: misc
disqus_comments: true
categories: English
---

First, let’s clarify three things:

"0.3" is an approximation—the precise definition of a parsec is 648,000/π astronomical units (fun fact).

Depression is no joke. As someone who has struggled with it, if this illness is real, I wish it were fake; if it’s fake, then I hope it’s real.

There’s no particular stake here. The reason for the sarcasm is simply because this company once evaluated me as "not outstanding enough." In the spirit of reciprocity, I figured I’d return the favor with a similar assessment.

Honestly, I think everyone saw this day coming for the company—just not this soon. Burning through $50 million one bill at a time (literally) should’ve taken at least two years. Of course, I wouldn’t dare openly claim that the company is completely doomed, but following the Chinese cultural notion of "not boasting about strength or plotting survival," a company that’s suffered such setbacks will likely find it incredibly hard to rebuild its (already questionable) reputation. I just feel bad for the algorithm engineers who’ve already joined.

Truth be told, I wasn’t particularly motivated to write this, but after skimming through a certain community where the average annual income is supposedly a million yuan, I noticed no one had clearly articulated the company’s core issues. So, out of a sincere desire to save the domestic general AI startup scene, I’ll share my two cents.

The problem is simple: they tried to emulate OpenAI—not the OpenAI of a few years ago, struggling on the brink of collapse, mocked by critics yet persistently producing high academic value—but the OpenAI already standing at the pinnacle of success, ready to monetize. They even reduced the latter’s success to a formula:
Success = Large Language Model = Hardware Infrastructure + Software Architecture + Top-Tier Algorithms + Massive Data.

Then, they built their team and executed accordingly:

Frantically buying Nvidia’s entry-level industrial junk.

Recruiting (widely regarded as) low-impact architecture teams.

Unrealistically high hiring standards (I suspect they’d rather leave positions unfilled than risk hiring someone the boss might dislike).

Tasking their algorithms team with manually scraping data (just a guess—feel free to correct me if you have insider info).

Is there anything wrong with this approach? From a capital perspective, probably not. But from a skeptic’s view, this "success formula" is a naive induction drawn solely from OpenAI’s outcome—hardly a certainty. In fact, if you ask me, the most critical element in this formula, what Augustine called "Divine illumination," was completely overlooked. Or, in layman’s terms, the company did everything right except hiring a grand mage to bless the endeavor. I know you’re all materialists, and "divine illumination" sounds like nonsense, but during its long struggle and academic output, OpenAI likely cultivated an internal culture akin to this "divine light." Under its glow, even the greenest algorithm newbies could gain wisdom, and the collective crystallization of that wisdom ultimately birthed what might be today’s closest miracle to AGI (or at least the closest to making money—no shame in that).

One last clarification: The cover image, with its "light," "festive vibe," and "otherworldly" utopian feel, was generated by a certain two-month-old large model, following the same logic as donkey meat火烧 (驴肉火烧).