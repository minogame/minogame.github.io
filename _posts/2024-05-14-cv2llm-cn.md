---
layout: post
title: CV转LLM：请用强化学习的世界观理解GPT
date: 2024-05-14 18:05
description: 蚂蚁海星 · 2024年05月14日 18:05・广东
tags: philosophy, misc
disqus_comments: true
categories: Chinese
---

最近被折磨到有些受不了，什么做人脸分割检测OCR的，一个一个都突然间变成了多模态大语言模型专家，然后用着上一个十年的CV世界观来对我评头论足。我其实特别理解这群住着一线城市大平层的人，因为除了没有大平层之外，我也是从传统CV+传统ML反复横跳到这个领域的。而且刚来了的时候会很自卑，因为甚至习以为常的数学符号体系在强化学习的世界中都有着异样的表达，好在我本来就拥有自我低贱型的人格，会抓着刚毕业的校招生教我各种细节。然而有着大平层的这群人却不一样，大平层带来的心理优越感使得他们没办法低下高傲的头颅。

往简单的理解，LLM某种意义上来说就是这么一个游戏：1 2 4 8 16 (?)，有人会填32，有人会填31，也有人会有理有据的往里填114514，这些答案在各种不同的情况下都是对的，所以你只需要看心情任意选一个即可。现在你填了32，因为它看起来最有可能让你通过行测，那么这个游戏现在变成了：1 2 4 8 16 32 (?)，于是你又继续往下填了64。但是接下来你犹豫了，你开始思考数盲症是否可以理解三位数，又或如此循环下去最后是否会成为指数型庞氏骗局，所以你踌躇再三填下了<|endofresponse|>，这个游戏便如此结束了。

而市面上所谓的LLM三阶段训练大致也是按照这个逻辑在运行，预训练阶段你从<http://oeis.org找到了大量的数列去学习其中的规律，比如2> 3 5 7 11 ([A000040](https://oeis.org/A000040))，比如2 5 11 17 29 ([A007491](https://oeis.org/A007491))，奇怪一点的比如1 2 3 5 7 12 ([A326083](https://oeis.org/A326083))，从中你掌握了一个数列是如何构成的底层逻辑，现在你可以凭借心情写出任意的有规律的数列了。随后在指令微调阶段，你学习到了如何在一个给定的数列中去接续下一个或者多个数，并且在适当的时候写下eor以结束这个游戏。最后在RLHF阶段，你被告知你要参加的是行测，所以要减少填入114514的冲动，即便你有证据证明它是可行的。

通过以上两段粗糙的文字，我希望我已经用了最白的话语来传达到了什么是强化学习的世界观。如果做CV的你能大致理解，那么你也一定能理解以下观点：

1. 无论是预训练亦或指令微调，GPT的工作对象都只是the next token，或者说action based on current state，只是因为QKV注意力变压器与注意力掩码的精妙设计，使得这种工作可以被并行执行。这与ViT的工作模式截然不同，后者的工作模式就是同时处理全部的token。故而在将ViT的诸多token强行塞到GPT模型中的时候，多少要考虑一下what is the action, what is the current state.
2. 在采样模式下，GPT模型所预测的token只是一个可能的action的表现，用民科的话来说就是action的概率密度函数因为观测而坍塌到了某一个具体的数值，而ViT输出的embedding那就是具体的确定的一个embedding。所以GPT模型最理想的训练模式就是on-policy sub-optimal的采样，而ViT就可以直接SFT。
3. 接上述观点，GPT模型所能预测的token的组合方式在预训练阶段就已经确定了，后续的指令微调与RLHF都不应该让其获得新的action space，如果不幸发生了，那么可以把数据前置去预训练阶段，否则action space上的shift大概率会导致模型性能无法被充分展现或者直接裂化。而ViT在我的理解中没有类似的问题，这也是为什么很多文章提示在SFT阶段放开ViT的权重会有性能提升。
4. Reward Model不是Discriminator，更不是好数据坏数据的二分类器。Reward Model是一个评估在某个特定state下，你的action/trajectory是否满足特定目标（比如人类偏好）的一个弱估计器。根据目前pair-wise的训练模式，RM能够工作的前提就在于所有的回答都基于同样的prompt，这样便可以尽可能的降低prompt本身带来的偏差。还是举前面的例子，在相同的1 2 4 8 16的state下，RM觉得action=32比action=114514更可能让你通过行测。但如果现在没有相同的state，直接给RM呈现的就是1 2 4 8 16 114514与837 130 5 391 3281这两个数列，那么RM大概率还是觉得前者更可能通过行测。所以请不要陷入“用RM来清洗SFT数据”这种卑微的思考模式里。
5. RLHF不等于正样本的SFT loss减去负样本SFT loss，因为正负样本的出处都是同一个policy模型，我们只是在评估其某一个action所导致的reward的大小，从而调整在current state下采用这个action的概率。当然，RLHF现在在这方面依旧是理论与实践的天坑，但是如果陷入了正样本负样本的思路，那这个topic就没有可以做的点了，调GPT4o刷数据去吧。
6. 接上述观点，同一个prompt所采样出来的回答也不应该用正样本与负样本的contrastive loss的方式来解读，即便现在把这些tricks加进去看上去在benchmark数据上能涨点，但是我认为这依旧不是一个合理的思考模式，因为它还是切割了“正负样本的出处都是同一个policy”的核心问题，将问题转化为了“不同的policy之间的择优录用”。

至此，想到再补充，也请大家批评指教。
