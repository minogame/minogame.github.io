---
layout: post
title: 当你的模型与你的人生一样糟心
date: 2023-09-06 19:33
description: 蚂蚁海星 · 2023年09月06日 19:33・北京
tags: philosophy
disqus_comments: true
categories: Chinese
---

**我有一个朋友。**

他很苦恼的对我讲，说自己的女神总是不理睬自己，理由是忙着调参数训模型，而且这个模型的效果目前还很糟心。他说这个东西给他的感觉，搞不好比起女神的模型更要糟心，因为这个理由就仿佛红头文件一样，会让人产生一种畏惧感，久而久之他已经不敢直视女神的头像，生怕自己的发言影响到了人家的超参，从而影响到了人家的实验结果，人家的paper，人家的毕业，人家百万美刀offer，人家的加州大house。

我听罢淡定地坐到了床头，又随手从放安眠药的盒子中抽出了《存在与时间》，翻到夹着书签的那一页，用一种专注于十字绣的感觉凝视着纸上的每一个符号。许久，在朋友不安的呼吸声中，我缓缓道：当你在讲模型一个模型很糟心的时候，你最好先搞清楚自己是在讲哪个模型。虽然这个系统并不是我自己独立创造的，但是你不妨听我再教你一遍。当然也不妨碍你对这个系统进行质疑，它只是一个启发式的方法论，如果可以的话我希望你自己可以总结出来更好的。言而总之，作为一个数学程度好到只能做黑盒深度学习的研究者，你口中的模型可能有五种含义，这里不妨用NLP的任务来举例子：

$$ \mathcal{M}_D $$ (Ding an sich)：借用康德理念的物自体概念的模型，是一种独立于基于我们所有的观察与理解的客观的基础存在。用NLP的例子来讲，我们可以认为NLP的世界背后有一套完美的模型在支撑所有NLP任务的运转逻辑， 我们虽然不知道这个模型是什么，但是我们可以认知一些NLP世界中的现象，比如我们可以预测一段输入之后的下一个token，只不过最终我们认知到的这些现象也不能告诉我们这个完美的模型究竟是“什么“，这个不知道是“什么”的，但是又可以影响到我们认知的“什么”，即所谓物自体模型。

$$ \mathcal{M}_P $$ (Perception)：虽然我们无法直接应用一个不知道是“什么”的模型，但是我们可以基于我们的感官，形成对其背后的运转逻辑的认知、印象或者观念等。换句话说，在这第二层的模型，即是我们对第一层那个不知道的模型的一种基于经验的近似。还用NLP的例子来讲，这个近似经验可以叫做“Attention is all your need”，也就是说，注意力机制即是我们对NLP背后运转逻辑，那个完美又不知道的“什么”的模型化。显而易见，这种认知模型是基于人的感官与经验而形成，自然是一种物自体模型的劣化。

$$ \mathcal{M}_I $$ (Instantiation)：当我们通过建立认知模型之后，我们下一步要做的自然是将其实例化，换言之，我们要将这种泛化的观念，转化为实际可以操作或计算的事物。再放回NLP的例子，在我们认为“Attention is all your need”之后，我们就要开始去写变形金刚的代码了。我们采用了多头的normalized softmax QKV的方式来实现注意力机制，我们又堆积了很多的全连接层来增加模型的容量。很明显，我们并不是总知道自己为什么要怎么做，有一部分是归纳来的经验，而更多的都是启发式的瞎蒙，所以一个实例化之后的模型较之我们认知到的模型是一个劣化。

$$ \mathcal{M}_R $$ (Reachable)：当我们有一个实例化的模型框架之后，这个模型的性能上限其实是已经确定了的，因为它最好也只能去近似实现我们的认知模型。回过头来，我们其实是没有办法达到这个上限性能的，因为我们只能进行参数的初始化(Initialize)而不是真理化(Enlighten)，所以我们应该总是需要一个训练模型的过程。也就是说，当我们撸(git clone)完了变形金刚的代码后，我们就需要开始搞数据调超参等事情了（很显然一个模型的性能好坏是是直接取决于此的），在我们做好我们一切能做的时候，我们就获得了可取得模型，显然这个模型较之实例化模型也是一个劣化。

$$ \mathcal{M}_O $$ (Observation)：最后，在你训练好了你的模型之后，你依旧需要一个方法去测试你取得的这个模型的性能究竟几何。通常来说，你会用一系列大家较为认可的，具有“公理”意味的任务来去评价这个模型。但是这从本质上来说，我们只是通过用多个任务的性能评价，以一种可行度较高的归纳推理，推测了这个模型的性能好或者坏。但是事实上，即便是穷尽市面上的所有任务，我们也没有办法完全展现出来你取得的模型的一切能力，或许你在初始化模型时用的随机种子，能让这个模型在描述你糟心的人生时效果拔群，可是你不知道，你评价的仅仅是一个基于观测的模型，这显然也是劣化于可取得模型的。
所以，当你发现你的模型性能很糟心的时候，你的心里应该要有个很清楚的逆向推理的过程，我们的终极目标是获得一个物自体模型的理想近似，而我们在这个过程中每一步都有模型性能的劣化。

我的模型性能不好，是不是仅仅是由于自己采用了错误的观测手段，我是否应当设计新的实验来检验模型内部已经取得的能力？或者说，我是否应该以当前的观测手段作为唯一的评价依据？比如说，当初国内那一众大模型，哪个不是在SuperGLUE上暴打GPT3，可是chatGPT并不是一个需要用这些个数据集来评价的东西。另外，如果市面上没有一个合理的手段，那么我自己是否能提出的提出一套更合理的，再去认定自己的模型的性能？
如果确认了观测的方案本身没有问题，那么下一步就要思考是不是在取得程度上出了问题，也就是说，数据足够不足够，数据噪音去除了没有，优化器选的对不对，优化器参数对不对，训练时长足够不足够等等等等，一般被戏谑为炼丹的项目。然而事实上，目前大部分人的层级也就停留在这一步了，最忌讳的就是在模型都没有充分取得的情况下，直接去改实例化甚至是认知。我见过太多学生在根本没有充分了解到自己在可取得性的问题的时候就开始瞎改网络构架，乱加Loss，甚至胡乱创造概念，在这里奉劝大家不要抱有侥幸心理，因为即便你通过这些手段，成功让你的模型在观测性能上提升了，也很容易会被当作是水文。
那么接下来，我们要处理的就是实例化的问题了。实例化这个事情很困难，因为我们需要实例化的观念通常来说不止一个，例如变形金刚里面，注意力的实现就需要兼顾“注意力机制本身”“正则化以便于训练”“多头注意力以增加多样性”等等诸多观念，这背后也有长序列的内存与计算复杂度的妥协，所以最终大家看到的结果是一个非常繁琐的工程框架。故而我们讲，我们在检查实例化模型的时候，首先应当理解我们融合了哪些观念，哪些更为重要亦或哪些可以用一个两层的MLP来替代，理解这么做可行的原因是什么。如果只是通过黑箱化的参数堆积，那么你在实例化一步造成的模型劣化就会相当严重。
接下来你要怀疑的就是，自己是不是一开始就想错了，物自体界里的运行逻辑其实不是我脑中的印象/观念/认知那样的。举一个例子，我们是否真的需要positional embedding，我们不知道的那个“什么”，真的只是位置关系，还是注意力本身的衰减(参考AliBi)？一般来说能理解到这个层面上的，已经脱离了水文的范畴了，因为没有什么比一个更清晰更准确的对那个“什么”的认知更重要的。但是要如何提升这个层面的认知，却不得不回到第一层的观测模型上来，因为所有的认知都需要我们的观察来获得，这也是为什么要坚持一线做工作的原因，脱离了这些观察，认知渐渐的会收到一层一层模型的劣化传导而变得扭曲起来。
讲到这里，我似乎大概又完成了一次对联结主义的传教。而我的朋友听了后脸上洋溢着幸福感，说谢谢你，我这就去找我的女神告诉她要如何在加州买大house。我合上了手中的《存在与时间》，抬头望向他：我这只是在教你，你对女朋友的认知跟实例化都有问题。朋友的思路突然就被从加州的大house里拉了回来，一脸错愕得反应道：我？りしれ供さ小？

我认真地点点头：我说你啊，

{% include figure.liquid loading="eager" path="assets/img/20230906/640.png" sizes = "400px" zoomable=true class="img-fluid rounded z-depth-1" %}
