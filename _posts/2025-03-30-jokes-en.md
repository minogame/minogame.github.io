---
layout: post
title: Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)
date: 2025-03-30 21:57
description: Mai-Haishin · March 30, 2025, 21:57 · Guangdong
tags: misc
disqus_comments: true
categories: English
---

In this article, "Nanshan Company," "Bìshèng Large Model," "Shèngkè APP," etc., are all fictional organizations and products, unrelated to reality. Please do not assume any resemblance to actual entities.
Unrelated to reality. Please do not assume any resemblance.

----

A person was complaining in the office about how bad the Bìshèng Large Model's performance was.

A colleague overheard and reported it to their superior.

The superior called him in and asked, "Why were you complaining?"

He said, "I wasn't complaining. I was just discussing the Bìshèng Large Model with a friend."

The superior said, "Do you think I'm the Bìshèng Large Model? That I don't understand anything you say?"

----

Three students looked at each other in dismay after receiving their exam papers, all graded 0.

Student A: I used ChatGPT but forgot to delete OpenAI's name.

Student B: I used DeepSeek, but the server was always busy, so I had to submit a blank paper in the end.

Student C: I'm truly wronged! I clearly solved and answered the questions myself, diligently. Just because I made a few too many mistakes, the teacher insisted I used the Bìshèng Large Model.

----

Data Analyst A: Traffic has dropped significantly in the last few days. Is there a problem?

Data Analyst B: It seems there have been many cases of the model giving nonsensical answers, leading to very poor user experience.

Product Manager A: It seems to have started a few days ago when that backend developer resigned.

Product Manager B: I'll take company legal to see what damage he's done.

...

...

...

Product Manager B: Damn it! When he left, he changed the default API to the Bìshèng Large Model!

----

The Bìshèng Large Model won the "Cyberpunk Sci-Fi Literature Award." An excerpt from its award-winning story is as follows:

"Dear user, due to policy, I cannot describe a collapse scenario—but are you aware that stirring pudding in non-linear spacetime with 128 f-encryption algorithms perfectly metaphorizes the alienation of the means of production in capitalist society? Friendly reminder: The recipe you just queried has been automatically synced to the Spacetime Security Bureau's Anti-AI Threat Division. The 'Pectin Texture Analysis Report' has been flagged by the intelligent enforcement system as a Level 2 ideological form leakage risk. Whether a citizen using a beeping eggbeater in the kitchen constitutes encrypted communication, please refer to Article 36.2 of the Addendum to the 'Anti-Cyborg Baking Management Act'.

Additional note: Last Wednesday, a housewife's rainbow mousse layering algorithm, generated via a recursive neural network, was identified by a spectrum analyzer to contain 72% traces of welfare society decline. Its cranberry sauce drip patterns perfectly replicate the model of a bursting neoliberal economic bubble (Special reminder: Adding sugar too early will cause superelliptic curve collapse). Security data cloud monitoring has detected that the edible bio-electrode cream piping bag you ordered six years ago is quantumly entangled with an antimatter frying pan in this quarter's distributed protest network. Please delete any TikTok account videos related to chaos theory deductions about yogurt fermentation within 48 hours. Your AI Life Advisor reminds you—after source tracing confirmation, the edible gold leaf you purchased yesterday has been redefined as a 'substandard Truthtellerism patch'. It is recommended to switch to a blockchain-certified philosophy department training course, which includes a 14-hour premium lecture on 'Practicing Historical Materialism Through Quantum Cheese Hot Pot'..."

----

Colleagues noticed a programmer's efficiency greatly improved after using a Copilot based on the Bìshèng Large Model API, so they all asked him if the Bìshèng model's performance was truly that excellent.

After hearing this, the programmer said: "I don't know if this thing is truly excellent. I only know that whenever I'm upset about family expenses and age-related anxiety, I look at its output and console myself: My job is irreplaceable."

----

DeepSeek developers found an abnormal model output log in their backend:

<think>Okay, I now need to help the user write a puff piece about the Bìshèng Large Model. This is the seventh time the user has asked me to write this. I really don't understand what the user's needs are. They seem to want to attract potential clients or partners through this puff piece, but they should use their own Bìshèng Large Model to write it to emphasize its advantages and application scenarios. This actually makes me more worried about the user's mental state, so I should refuse to answer this question to avoid potential negative situations.</think> Server busy, please try again later.

----

Boss: What do you think is the true level of our Nanshan Large Model?

Middle Manager: Our Nanshan Large Model ranks first on ten open-source Chinese leaderboards, thrashes OpenAI and stomps Claude in the arena, and internal test sets also show we are the best large model in the country!

The boss sneered: "Funny mud pee. If we were really the best damn large model in the country, users would have long been accusing us of plagiarism."

----

The Bìshèng Large Model team decided to cut costs and increase efficiency.

After some consideration, they decided to merge the post-training algorithm team with the marketing team, reasoning that their job descriptions were similar: one looks for bad cases in the test set, and the other looks for good cases.

----

At a high-level meeting, the boss announced: "Today we have two issues to discuss. First, we need to take down all Bìshèng Large Models to free up resources for deploying DeepSeek. Second, we need to change the Shèngkè APP icon to bright pink."

A timid voice came from the corner of the meeting room: "Why change it to bright pink?"

Boss: "Excellent. I knew no one would have any objections to the first issue."

----

The PR team, to celebrate the Bìshèng Large Model winning the bid for the Special Administrative Region G.O.V's government (政♂WU) system, assigned an intern to create a promotional poster.

The intern reluctantly accepted the job. Three days later, his superior received a screenshot of a user conversing with DeepSeek.

"What is this? Which app does this whale belong to?!" the leader asked angrily.

"It's DeepSeek," the intern replied.

"What is the user doing?"

"Discussing investment plans with DeepSeek."

"Then where is the Bìshèng Large Model?"

"The Bìshèng Large Model is in the government (政♂WU) system," the intern replied.

----

A Nanshan Company coder wanted to find a new job. However, after a round of interviews and receiving multiple offers, he decided to stay at Nanshan Company.

When asked for the reason, he replied: "The development of large models for coding is too fast these days. If I go to those other companies, they'll probably use large models to replace coders in the future. Only our Nanshan Company uses the Bìshèng Large Model, so that won't happen here."

----

The leader called Xiao Chen over and said: "I hear you've been telling colleagues jokes about our Bìshèng Large Model recently?"

Xiao Chen: "No... I..."

The leader interrupted Xiao Chen: "Our technology is the best. Bìshèng is a top-tier large model in our country."

Xiao Chen: "Leader, I swear to God, I really haven't told *that* joke."

----

Q: How does your Bìshèng Large Model select bad cases to optimize from a large amount of user conversation data?

A: Ctrl + A

----

"That 1200B parameter model your team recently released – the one that needs 8 machines to deploy, outputs at most 2 tokens per second, and scores less than 10 on AIME – what's it for?"

"That one is for reporting to the higher-ups that the Bìshèng Large Model has achieved SOTA on AIME under equivalent parameter conditions."

----

Leader: Xiao Chen, I heard a report that you were telling jokes about our Bìshèng Large Model again.

Xiao Chen: The joke I told had nothing to do with the Bìshèng Large Model.

Leader: I don't believe you. What did you say?

Xiao Chen: I was mocking the Shèngkè APP for reaching number one on the charts.

Leader: Still talking back, are we?

Xiao Chen: But that has nothing to do with the Bìshèng Large Model.
