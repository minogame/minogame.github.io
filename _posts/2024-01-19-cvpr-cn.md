---
layout: post
title: 审稿CVPR而致的伤痕文学：没落的顶会与空虚的文章
date: 2024-01-19 18:57
description: 蚂蚁海星 · 2024年01月19日 18:57・广东
tags: philosophy, misc
disqus_comments: true
categories: Chinese
---

1.24 update，查看了一下，5个给1分的，其他review也都是1～2分，另外一个给5分的，其他rewiew也是4～5分。所以，圣母们可以闭嘴了吧？

澄清一下，请不要断章取义理解内容，10分钟决定给reject跟整个审稿只花了10分钟是两个概念，一个质量很一般的文章，看完abstract跟introduction就知道是什么类型的了，接下来就是调研跟其相关性最强的几篇文章，寻找素材来写评论，到最后每一篇文章我都会不得不累计花上几个小时来写comments。

另外为什么6篇就给了5个reject？因为顶会的中稿率本来就在20～25%这个区间，手上拿到6篇文章肯定会要明确拒绝掉3～5篇，这难道不再正常不过？你给1个reject 1个accept 4个borderline，你觉得AC会不会想砍人？

然后还有因为我给了5个reject开始PTSD的，还有把几个审稿结果当独立随机事件的？首先我拍着胸脯讲这几篇其他审稿意见必然也是负向（我还很认真的给它们每个都写了一到两个strength跟改进意见），如果到了讨论阶段我发现评分有误那肯定也会改过来，其次我就是受不了怎么这次文章质量这么差，才来说说文章到底要怎么写。

此外我还收到了6篇emergency review的request，简单读了一下摘要都是质量一般的，要是我都接下来12篇估计要拒9～10篇，就是不想折磨自己所以全都decline了（当然可能上限是8篇，不敢点）。

还想再多强调一句，对于一个junior reviewer（大致来判断就是收到稿件数量少于8篇的，有误差），您们的工作就是按时、准确、明确的提供审稿意见，做完这一步您们的工作就已经很好很出色的完成了。不要怀着菩萨心肠去普度众生，这种想法只有嘴上的快感没有实际的意义。

还得被逼着再说一句，咱如果有认真度过guideline，一般来说人家要求的是审稿的时候除非你确实没有把握，否则要尽可能表达清晰的意思，不要给borderline，这也是为什么之前iclr有尝试1368制度，为什么今年cvpr把那个“strong”去掉了，就是为了降低审稿人给两端分数时的罪恶感，换言之就是鼓励审稿人尽可能清晰的表达意思。但架不住你们就是喜欢啊，你们给越多的borderline，就会造成AC的meta-review随机性越强，就越是会有人产生投机心理以及获得好的投机结果，这会的水平就越烂。

而且我这人还真就挺傲慢，就现在CVPR的文章平均质量，一篇文章要读一天，您怕是没那个水平给别人审稿吧？（手动狗头，不要误伤

以下原文：

事实上，我已经拒绝审稿很多年了。投稿/审稿这个事情（特别是深度学习领域的），本质上一种基于科学信仰而对某一研究标的进行辩经的宗教行为，是一个本应十分有门槛的事情。然而渐渐地，当我发现自己不得不为了诸如“resnet是最伟大的神经网络”、“全民大炼数据就能超英赶美”、“对抗公鸡也是一种对抗攻击”之类的鬼畜素材花那么几个小时的时候，自然就放弃了参与这种活动。

结果这一次被PC刷了大概有那么五六封审稿邀请邮件（The quality of the conference strongly depends on the willingness of reviewers like you to provide thoughtful, high-quality reviews on time. 但也没见邀请我做个AC啊？），于是就怀着稍微猎奇的心态接受了，于是就被分到了ID最大接近18000号的稿件，带给了我那么一点小小的AI震撼。审稿的体验确实大概是比几年前更差了，经我手的6篇中有5篇都是10分钟之内就给了1分Reject（就是看完了abstract跟introduction）。当然有人会质疑我怎么跟知乎上描述的reviewer不太一样，这个确实是每个人对优秀文章的定义都不怎么一样，偶尔有我这种几乎不会去看实验性能的人对有些人是福音（自然对大多数人都是灾难），不过喜欢在康托尔集上刷sota的朋友们完全可以尝试多投几次，总会遇到不是我这种的审稿人。

所以我想从这个角度出发来讨论两个问题，其一是当今顶会何以混乱至此境地，其二是我们应当用什么朴素的标准来判断文章的质量。

#### 一、顶会中充斥灾难级质量稿件的因由
套用经济学的概念来分析，这件事情的本质原因是学术市场本身并不足够有效。换句话来说，一个学术研究者的价值无法迅速且准确地被他的各种学术行为所反映，故而如果一个学术研究者希望收获学术成果，就不得不依赖于对有可能形成超额回报的顶会，来进行投机性投稿。当然，如何设计学术研究者的价值评价制度这种大话题，并不是我的能力所及（当然也没人会听我的）。这里只是希望探讨一下市场不够有效以及投机行为成能够立的原因。

首先，从理论上来讲，即便为了形成一个弱式有效市场，对一个学术研究者的价值评估也应当在充分了解其公开信息（所有的publication等）的基础上进行。但这个工作量以及判断的标准化都不是一般组织可以完成的，故而在当下的学术评价体系中，才会采用一种近似于“效率优先、兼顾公平”的路径，例如通过判断其顶会主要作者频次以及citation等可量化指标，来快速的为一个学术研究者贴上价值标签。如果考虑到现实因素，特别是在教授兼顾行政的学术系统中，这个做法本身来说是中性的。然而，如果在一个有着大量行政人员的学术系统中，毫无作为的直搬这种路径就显得相当的没有作为。特别是通过这个以论文为核心的价值评估模式，获得了行政帽子的人群，如果依旧坚持以同样的评估模式来对其下一代研究者进行价值评估，那么就会直接导致这个市场中的下一代研究者依照着最大自我利益的模式开始内卷。

发散一点地说，比起制度性原因，学术行为所满足需求层次的差异（也就是人性）恐怕才是市场失效的核心因素。在我看来，学术行为至少要从满足认知需求（求知欲、好奇心）这个层次开始起步，而获得高级的帽子也应当是在满足审美、自我实现等高层次需求（价值观、道德观）。如果一个学术研究者只是为了生理或者安全需求（职业保障、福利待遇），而获得帽子的目的是满足特定的尊严需求（成就、名声、地位），那么势必会造成学术价值评判的扭曲。但是我在这里依旧保持一个较为缓和的态度，即一个研究者的需求层次大体上取决于外部环境而非个人因素，例如我们的破五唯专项行动，本质上就是提供一个研究者可以不用担心低级需求的外部环境（这里并不对其成果进行评价，个人认为还是有积极意义的）。

另外一个层面，有大量的相对初级的研究者在尝试此类投机行为（这里先叠个甲，虽然有来自某东方大国的因素在，但是近两年来自于另一个东方大国的垃圾稿件明显增多了），个人理解其本质的原因就是所谓的顶会的信誉严重下降。换句话来说，这些研究者失去了对“Top”、“Tier 1”、“S-class”这些描述词的敬畏之心，将中稿纯粹的抽象成一个随机概率事件。所表现出来的，就是作为一个多年服务在这些顶会审稿人，我也没有办法确定自己或者学生的某一篇投稿可能被接收，我甚至于都没有办法讲出它们可能会是因为什么理由被拒稿的，而我他妈的居然还在审其他人的稿，可想而知当下审稿的随机性有多么的离谱（就别提什么AI/ML三大会了，就连UAI/AISTATS都已经接近类似的状况）。在我理想的观念里，一个顶会应当有的信誉应当可以让低质量的稿件在投递之前就已经知道自己无法被接收而放弃，就好似不会什么人都会试图投数学四大神刊一样。而真的到了每年去看poster的时候心里不断地在骂这都什么垃圾的时候，我也找不出来什么理由不去做这种投机性的投稿。

但是区别于学术市场中因为人性所导致的无效性，个人理解顶会的信誉下降反倒是制度原因造成的。排除掉那些将Reviewer/AC当作帽子来追求功利的人群（我也不知道是不是排除了大多数人），一个用爱发电的Reviewer/AC应当更多的是抱着为了让我们环境更好才去做这份工作，否则他/她大可以拒绝也不用受到任何惩罚。细致来讲，有两个主要的问题存在：其一，真的是审稿人的水平有限，而且这个有限不仅仅来自于资历不足（比如某ML顶会开创的投稿者志愿充当审稿人的奇葩先河），更来自于当下研究领域以及领域内论文数量的爆发性增长，以至于不去花几个小时做个survey，你可能压根都不知道你审的论文处在什么位置，对这个领域（更多的是交叉的领域）有哪些真正的贡献。其二，目前的审稿流程中AC的作用相当单薄（尤其是那种拿AC当帽子的人），再考虑到一个AC手上的论文可能多达数十篇，所以很多时候AC不得已只能依照审稿分数的正负向来做简单的决定；而这反过来更深的加剧了审稿人的Borderline倾向，也就是说审稿人不愿意为AC的简单正负向决策来负责，宁可给一个中庸的分数来等待某个明确表态的审稿意见。于是死亡螺旋就这么形成了，本应是群体讨论获得结论的审稿流程，成了某一个crazy reviewer的个人表演场所，如果他不喜欢这个文章给了一个低分，那么这个文章就几乎确定会被拒稿，哪怕这个reviewer并不qualified，他的comment也是幼稚无意义的，甚至于copy-paste一个投稿在其他学会的审稿意见（我就遇到过多次）。

这里我总结一下曾经失败的制度性尝试，并且提供一个可能的解决方案。首先，最失败的就是open review，因为其本质上就是为本来就已经无效的学术价值评价市场上提供更多的信息，所以从一开始它就注定是毫无意义的；开放审稿就结果来看，不仅没有能提高稿件以及对应审稿意见的质量（身边统计学的结论，如果有不同意见欢迎指教），反倒造成了一些类似于审稿意见同质化、学术成果被人“借用”等糟糕的影响。当然这里并不完全否定这个方案，开放审稿就高质量的论文而言，阅读其审稿意见以及rebuttal有助于提升学术水平，但是本身作为优化审稿流程制度来说是失败的。其次，就是multi-round review或者journalization（期刊化），其本意是想提升作者与审稿人之间的交流，但是执行下来大多数审稿人都不会再给第二轮的意见（特别是很明确的拒绝意见），导致作者喊没有人也破喉咙回应，严重拉低了投稿的体验（例如我打死都不会再投某些会/刊）。另外，还有一种No rating的审稿模式尝试，然而执行下来的结果基本等同于3档：接收/滚粗/随便，如果AC缺乏责任心则更容易导致一票否决的情况发生；而且我都不会被明着reject羞辱了，为什么不投个只因试试呢？万一中了呢？

所以有什么制度性的方案可能提升此类顶会的稿件质量？我虽然没有做过会议的运营，但是就个人的长期伤痕体验来看，认为采用“摘要-约稿”的投稿流程可以部分解决这个问题。这个流程分为三个步骤：首先，所有希望投稿的作者，都应该提交一个简短的（CVPR双栏2～4页程度）的摘要（摘要允许dual submission），这个摘要交由一般志愿者审稿人来进行评分。随后，AC从中选择一部分评分较高或者其个人认为有价值的摘要，向其作者发出投稿邀请。最后，对有约稿作者所投递的完整稿件（完整稿件不允许dual submission），交由资深审稿人带队的团队进行评审，择优录取一部分。这个流程既免除了审稿人需要为糟糕的稿件书写正式的审稿意见，也可以让有了约稿的作者有一个比较明确的接收预期，如果真的有哪位大佬看到了不妨测试一下执行效果。

#### 二、从知识论的角度来衡量文章的价值
接下来我想谈谈那5篇10分钟之内就给了1分Reject的倒霉蛋（当然换个角度来讲，我可能才是那个倒霉蛋，要给浪费这么多时间给明摆着的“征求意见稿”去做没有回报的指导），这个要从康德对于知识的解读开始，因为在我的理念里如果一个学术研究稿件无法提供知识，那么它就应该被拒绝。依照我对邓晓芒译本的粗浅理解，康德在《纯粹理性批判》提出了区分了先验（观念）和后验（经验）的理论框架。前者指代的是经验之前就存在的（知识），它是关于形式和逻辑结构的，不依赖于具体的经验内容。而后者则是基于感官经验的，它依赖于我们与外部世界的互动（这里有一些翻译的问题可以参考下面的问答）。康德认为，虽然纯粹理性可以产生必然的知识，如逻辑和数学定律，但这些知识并不直接告诉我们关于物理世界的事实。反过来讲，物理世界的知识都来源于经验，但是知识的可能性却是由先验的条件所决定的，故而需要将感性直观的输入和理性概念的结合（做综合判断）。

我知道你们可能会反感这种引用绕口哲学经文的行为，所以这个方向的人话来说，首先，单一的概念之间的分析（“XXX是造成XXX问题的原因，所以采用XXX方法就能解决”）或者单一的实验结果（“我们采用了XXX所以我们拿到了XXX的sota”），都不足以让文章产生必然性的知识。其次，通过做一些无意义的恐吓性的数学推导（“需要十个假设条件才能证明的定理”），或者看上去很完备但是十分空洞的消融实验（“我们把每一个部件都拿掉然后跑一次实验”），也很难将文章中的观点或者断言整合为必然性的知识（即便大多数人都这么做）。

我相信你们依旧会觉得上述解说晦涩难懂，所以接下来我直接提供一些论文的写作范式，来讨论某种文章在我心中是否真的是有意义的：

基于理论T或者实验E，得出A成立：这种文章一眼看上去似乎是不成立的，但是实际上来说此类文章的标的一般都是一个比较新的领域，例如第一篇提出对抗攻击的文章，而能提供“一个新的领域”本身就是一个理性与经验共同作用的结果，所以此类文章通常被认为是非常有价值的。

基于理论T，得出A成立，同时基于实验E，得出A'成立：这种文章普遍存在于经典的机器学习领域中，是一种非常solid的论证方式。虽然A'是A的近似，但是已经足以提供合理的可靠的经验来形成确定的知识。

基于实验E，得出A成立，同时基于理论T，得出A'成立：这个写作范式便是我前面所提到的通过恐吓性的数学来提升文章品质的错误路径，经常出现在一些已知领域的算法类深度学习的文章中。但是在我看来，如果需要理性来“完成”一个知识，那么它应该参与在通过实验得出A成立的这个过程当中，你的理论应该是对实验结果的分析。而分析一个近似的A'是与经验相分离的一个过程，本身就不形成知识。所以这篇文章的强度依旧取决于实验E的强度，所以很多人觉得自己“做着实验写着理论突然间就被拒了”很冤，但其实一点都不冤。

已知基于实验E，A成立，基于实验E'得出A'成立：这里区分于基于理论获得A成立，如果基于一个相似的实验去验证一个相似的结果，其强度上限就就取决于E->A这一步的推论，而这个并不会是一个必然的知识（可能会有瑕疵但是会被接受），所以这类写作范式本身的立意就很薄弱，缺乏理性的分析则让其更加的空洞。

已知基于实验E，A、B成立，基于实验F得出A+B成立：这种范式是我们目前最常见的“缝合怪”模式，它的问题主要就出在人类的先天思维中，两个本身成立的结果缝合在一起自然就会成立，故而这个实验F就会被认为没有为形成新的知识提供经验（也就是说“用膝盖想都明白的事情有必要做实验吗”的意思）。但是反过来讲，这个逻辑是否正确其实也因人而异，所以有些人也会认为“A+B成立”确实提供了新的知识，但是其强度可能不是很好，也就是平常说的“水文”。当然，这种文章一般我都是直接斩于马下的，请大家安心。

已知基于实验E，A、B、C、D成立，基于实验F得出A'+B+C'+D成立：这种是典型的系统性应用文章，虽然看上去这种也是水文，但是有趣的点在于，寻找A+B+C+D的组合以及替换其中的组建，本身是一种理性在作用的体现：比起单纯的A+B，寻找一套合理的方案来解决应用问题，是一种不错的知识的体现；但是也要视这个应用以及结果的强度来决定它是否应当出现在顶会。

已知基于实验E，A={A1, A2}成立，同时基于实验E'，得出A={A1}成立或不成立：这种范式是其实很有意思的，它不同于缝合A+B，而是在考察A本身成立的可能性，其理性就存在于对A组成的严格分析，而非相对比较松散的不同的A与B的组合。即便结果是A不成立，如果实验的强度足够，例如论证Adam不成立的神文，我依旧会相当乐意给它一个很高的评价。

已知基于实验E，A={A1, A2}成立，同时基于实验E'，得出A={A1, A2'}成立：这种范式（我觉得都不能说是范式）常见于初心研究者的文章中，说白了就是把人家的代码拉下来把其中一个组件改一改（再消融一下之类的）然后效果变好了。在我看来，这个文章中知识成立的前提完全建立在早先对A的实验当中，所以几乎不提供一个新的知识；甚至于即便作者再提供一个理论来论证A'={A1, A2''}成立，它依旧是一种对已知的知识的直白推论（而且强度受限于A本身），这种文章即便效果很好（够“水文”的标准了），我也难以给它一个很高的评价，而且为了拒绝它通常要写一大堆论证其不提供知识的论据，是真的非常的消磨人的意志。

已知基于实验E，A={A1, A2}成立，基于实验F或者理论T得出A={A1,A3}成立：这里A3区别于前述的A2'，是一种完全不一样的组件，只要实验F足够的强，个人认为是一种不错的提供知识的范式。

已知基于实验E，A={A1, A2}、B={B1, B2}成立，同时基于实验得出C={A1, B2}成立：这种写作范式非常地一言难尽的，他好于直接缝合两个方案，但是似乎又看不出来理性在其中发挥了足够多的作用。我不能给一个直接的结论，但是通常来说此类文章可以通过重整框架（故事），来获得一个近似于前述范式的效果。

