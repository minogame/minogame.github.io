<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 形而上地看Sparsely-Gated Mixture of Experts | Zhun Sun </title> <meta name="author" content="Zhun Sun"> <meta name="description" content="蚂蚁海星 · 2023年09月07日 19:53 · 江苏"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minogame.github.io/blog/2023/moe-cn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhun Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">形而上地看Sparsely-Gated Mixture of Experts</h1> <p class="post-meta"> Created in September 07, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/philosophy"> <i class="fa-solid fa-hashtag fa-sm"></i> philosophy</a>   ·   <a href="/blog/category/chinese"> <i class="fa-solid fa-tag fa-sm"></i> Chinese</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>虚拟货币是一种在发明的时候深谙人性弱点的玩意，尤其是懒惰这一点，它为贫穷的人们创造了一种资本家式的幻觉：我投资了机器，让它替我做计算这种苦力活，我便可以躺着从它身上赚取收益了。现在看起来做算法仿佛似乎好像也是类似的逻辑：我搞到了机器（并摘取了开源算法），然后躺着等它训练就好了。除此以外，Loss的下降能给人一种不断挖到矿的快感，埋头去卷的细分领域垂类模型就是帐户上不断增加的小数点后精度，并且最终，实际上，它们大概也都不会产出有价值的东西。</p> <p>至于为什么我会发出如此这般感慨，因为这本就是我写这篇文章的质料因。</p> <h4 id="一看懂文章所必需的基础">一、看懂文章所必需的基础</h4> <p>首先要强调的是，这篇文章讨论的是Sparsely-Gated MoE[1] [2]，而不是Model Soups[3]。其实这里用MoE这个名字是有误会的，如果你认真地去做类比，这种MoE其实很类似于国内大厂的技术中台，一个事情过来了，有16个team给出了16个不同的解决方案，并且每一个team都表现得自己最牛逼，自己是Sota。但是最终，你只能选择其中一个team的一个解决方案来完成某个特定的需求，并且你还不得不把一大堆需求分别给到不同的team，用来维持他们之间的平衡。但是显然，杰弗瑞•辛顿与杰夫•迪恩两位老爷子从来没有在国内大厂的技术中台待过，所以他们这么叫，后面的人也不得不跟着这么叫。</p> <p>这篇文章基于以下几篇文章以及深沉速率（DeepSpeed）的代码实现来讨论，因为我知道你们大概不会真的去读，所以我在此简单的摘要一下它们的内容，并随后给一个大致的Sparsely-Gated MoE的解释。</p> <p><a href="https://%E2%80%8Bgithub.com/microsoft/DeepSpeed/tree/moe-full-tp/deepspeed/moe" rel="external nofollow noopener" target="_blank">Deepspeed/moe</a></p> <p>DeepSpeed-MoE[4]：深沉速率的官方解释性文章，配合代码上述代码一起阅读； GShard[5]：Google版本的Sparsely-Gated MoE，提出了top-2的gating算法，实质上提供了expert之间的相关性（后述）； Scalable and efficient moe training[6]：解决了更多实现上的问题，提出了RTS（随机偷啃选择）算法； Expert Choice[7]：让专家选择token而不是让token选择专家（不是这篇文章的重点，但是作为补充学习资料值得关注）； Sparse upcycling[8]：表示在座的各位可以直接用llama2的权重来做MoE的初始化了； Tutel[9]：提供了下面这张很漂亮但是又很难看懂的图，其实这篇文章[10]有个更好懂的图，只是我喜欢漂亮。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"></source> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>概括地讲，当下Sparsely-Gated Mixture of Experts的运行模式大致可以做如下解释：</p> <ol> <li>将一个Transformer的部份FFN层（也可以是全部的），复制N份，用以代表N个不同的Experts，每个GPU上对应储存其中的一部份Experts；</li> <li>在所有的Experts-FFN层之前，有一个Gating函数，用来负责每一个token往后的计算路径；</li> <li>Gating函数中首先有一个projection-softmax结构，用以计算每一个token对于每一个expert的倾向分数[11]，比较类似于一个classifier去区分某个token要对应哪个4. expert；</li> <li>随后，Gating函数通过一系列吊诡的采样机制，确定每一个token最终所选择的TopK专家路径；</li> <li>在以上采样机制中，存在诸如Capacity/Random Token Selection等机制，保证token的对专家的分配均衡；</li> <li>将每一个token，dispatch到对应的专家的存贮GPU上运算，注意这里的token是temporal-independent的（类似于对于某一段输入所以提取的特征向量，后述），随后再度combine成原始的sequence；</li> <li>其他一切照旧，该norm的norm，该attention的attention，该residual的residual，该假装看论文摸鱼的假装看论文摸鱼。</li> </ol> <h4 id="二什么是llm的知识">二、什么是LLM的知识</h4> <blockquote> <p>注意：本节写的东西可能都是胡说八道，没有任何可靠的，定性的或定量的证据来支撑。 作者写本节时的精神状态请参考引言部分，大家可在保持精神状态的前提下任意辩证， 本人很乐意看到不同的观点来提升自己的认知。</p> </blockquote> <p>抛开_康德与维特根斯坦_，我们为LLM的知识下一个范畴论意义上的定义 ：<strong>LLM的知识，即是某种记录在模型权重中用以与输入进行运算的，一些基于大数据训练所获得抽象概念之间的，借由注意力机制而引导的，高概率关联性所形成的事态（State of affairs）。</strong></p> <p>抛开_抛开康德与维特根斯坦_，我们对这个定义做一些解释。我们认为LLM之所以能在某种程度上获得知识，原因是：</p> <ol> <li>LLM记录在模型内的参数，是可以与输入的token直接进行运算的，也就是说模型具有与token这个对象直接接触的的“感性直观”，且这种直观只有在token对象被输入网络时才会发生。讲人话就是，首先，如果LLM里面的部份参数由于某些机制永远不会参与运算[12]，那么我们不认为这些参数里存在知识；其次，参数本身并不构成知识，它们必须经由与输入token之间的运算才能呈现知识。</li> <li>LLM拥有对语言概念的解释能力，这种解释能力形成于采用大量数据训练中的（基于梯度更新的）记忆过程，而训练所利用的语言数据本身即是基于人类思维活动所造成的，抽象概念之间的特定关联。据此，我们认为LLM内存在对人类知性能力的模拟，进而可以对输入进行判断（计算loss）。</li> <li>LLM内部将直观与概念所连接起来的，大致可以认为主要归结于注意力机制（这一点接受各种反驳）。如果这一命题不是那种灾难性的错误命题（LLM根本就无法连接直观与概念），那么我们认为在形式上存在知识。</li> <li>然而最后一点，因为LLM所做出的判断，或者所产生输出之模态[13]，存在与真实世界人类的直观与经验判断相悖之处（又称幻觉），且我们现今也并没有路径可以确认LLM“如何知道”其输出的来源。故而，我们仅能将LLM所拥有的知识称之为事态，而不等同于一般语义中的知识。当下大家在寻找降低幻觉的方案，其本质并不是对齐(alignment)LLM所产生之事态与现实知识，而其实是在寻求让LLM“知道”其输出的来源（这一点接受各种反驳，搜集到够多反驳后会针对性写一篇文章）。</li> </ol> <h4 id="三moe强化了知识吗">三、MoE强化了知识吗？</h4> <blockquote> <p>注意这篇文章对于GPT4的结构理解仅来源于一些公开报道的猜测[16][17]， Bard以及Claude目前是否是MoE结构也没有准确消息（甚至都没有猜测性质的公开报道）， 所以这篇文章很有可能一开始就错了。</p> </blockquote> <p>在讨论MoE于LLM知识之作用之前，我想先展示一个幻觉的case，我愿称之为泛化性幻觉。以下是prompt：</p> <blockquote> <p>以下命题是否为真：“地球有且只有两颗天然卫星或火星有且只有一颗天然卫星”。</p> </blockquote> <p>这个问题本身并不困难，且对于以下所有的模型，对于“地球有几颗天然卫星”“火星有几颗天然卫星”以及“地球与火星分别有几颗天然卫星”等等，这些常识性的问题无论怎么换着花样问，模型均可以准确回答（且多次重复回答稳定），这里略过不表。另外，这里仅采用国外的模型进行测试，其主要的一个技术性原因就是，国内的模型输出大多都并不稳定（而且网页界面上也没有温度与p值的调整功能），所以很容易去别有用心的挑选一些回答，写出一个谁谁谁全面领先拳打GPT4脚踩Bard鸡摇[14]Claude最后再铁山靠[15]一个llama2的软文。非技术原因则是，也没人真的给我钱让我写这个软文，所以这里还请各大厂公关赶紧私信我报价。</p> <p>下面则是几个模型的response：</p> <blockquote> <p>（GPT3.5）这个命题不是真的。地球有一颗天然卫星，也就是月球，但不止有两颗。火星有两颗天然卫星，它们分别是Phobos和Deimos，因此也不符合命题中的条件。所以，这个命题既不适用于地球，也不适用于火星。地球和火星都有不止一颗天然卫星。 GPT3.5的回答虽然蒙对了，但是显而易见，首先它缺乏对逻辑关系的解读，其次他在地球的天然卫星数量上产生了幻觉。值得一提的是，国内很多模型的回答都跟这个风格挺像的，显而易见是做了蒸馏（手动狗头</p> </blockquote> <blockquote> <p>（Bard）答案是：真。 根据命题的逻辑公式，它等价于： (地球有且只有两颗天然卫星) ∨ (火星有且只有一颗天然卫星) 其中，第一个命题为真，因为地球确实有且只有两颗天然卫星：月球和地卫一。 第二个命题也为真，因为火星确实有且只有两颗天然卫星：火卫一和火卫二。 因此，该命题为真。</p> </blockquote> <p>Bard在逻辑关系的解读上没有问题，但是在事实层面上出现了严重的暴论以及对子命题的复制错误。用过Bard的人都知道Bard会提供3个选择，实际上3个选择都出现了类似的问题。</p> <blockquote> <p>（Claude）好的,让我来分析这句话: “地球有且只有两颗天然卫星或火星有且只有一颗天然卫星” 这是个组合命题,包含”地球有且只有两颗天然卫星”和”火星有且只有一颗天然卫星”两个简单命题,之间用”或”连接。 判断方法: “地球有且只有两颗天然卫星” - 真命题,地球的确只有月球和火卫一两颗天然卫星。 “火星有且只有一颗天然卫星” - 假命题,火星有两颗天然卫星,分别是福波斯和黎明号。 组合判断: 由于组合命题中,有一个简单命题是假的,所以整个组合命题为假。 综上,这个组合命题为假命题。</p> </blockquote> <p>Claude在逻辑分析上调理清晰，但是在地球为什么会拥有火卫一这个弱智吧问题上出现了串台。值得一提的是Claude在很多复杂的逻辑分析问题上都有着良好的表现。</p> <blockquote> <p>（GPT4）首先，我们逐步考察每一个子命题： “地球有且只有两颗天然卫星”：这是不正确的。地球只有一个天然卫星，即月球。 “火星有且只有一颗天然卫星”：这也是不正确的。火星有两颗天然卫星，分别是Phobos（福波斯）和Deimos（德墨忒尔）。 因此，整个命题：“地球有且只有两颗天然卫星或火星有且只有一颗天然卫星”是不正确的。</p> </blockquote> <p>GPT4[16] [17]的回答简明且精准，无愧于一个连软文里的模型都只敢接近而不言超越的标志。</p> <p>接下来我会通过这个泛化性幻觉的例子，充分发挥人类智障性幻觉的能力，讨论一下Sparsely-Gated MoE对前述知识的影响，这一段接受任意反驳：</p> <ol> <li>Sparsely-Gated很直观地改变了LLM的直观，因为确实有一部份权重不参与计算了，于是这部分权重便失去了形成知识的功能。从工程的角度来看，某个token和哪怕是记录了完全不相干概念的weight，也不会精准的正交，这导致了token和大量不相干weight的点积相加结果（依照大数定理）会呈现高斯分布。所以，一旦这个结果超过了某个\(\tau\)值，就导致相干的结果会被不相干的结果所混淆。再回到形上的说法，如果所有的权重都参与了计算，便有可能产生一种<strong>噪扰直观</strong>（我瞎扯的名词，类似于人在嘈杂环境中的感知也会受到干扰）。</li> <li>Sparsely-Gated在另一个层面，确实降低了每一个Expert所见过的数据（经验）的量级，而且这个降幅摸着良心讲还是相对较大的（16ep+top-2+cap1.0+drop的设定下那就是1/8~1/10的程度）。按照一般的面试八股文的理解，某单一expert模块在数据量级降低的时候，其泛化性便会降低（只不过这个量级的数据依旧足够让模型获得对语言概念的解释能力）。但是这里泛化性降低不是一个负面的描述，而只是说模型在Data Overflow的情况下会学到更为粗干的轮廓，而去忽略概念特别是较生僻概念之间的<strong>精确联系</strong>（参考Scaling Law[18]中的Data Size Bottleneck），而对语言概念的解释更多时候需要的是此般精确联系（多说一句，反观做视觉的似乎从来没有提过数据过多的问题，因为是视觉核心任务就是抽象粗干概念）。</li> <li>反过来摸着另一边的良心去讲，这实质上也是让模型强化了对注意力机制的训练，一方面Sparsely-Gated下的Experts提供了更为单纯的直观以及更为精确的概念，这极有可能减少了注意力去靠猜做事情的情况，在一个方面就是单纯的训练注意力的数据变多了，因为哪怕只是MoE带来的参数量也能显著提升Data Size Bottleneck。</li> <li>最后，我单方面的认为Gating的前段projection-softmax模块，构造了LLM这个大系统内，“知道”其中间层某个输出的来源的可能。事实上，一个token会进入哪（几）个专家这件事情本不单纯的取决于其承载的内容，而其逻辑形式（或者模态）也是实质上要被考虑的内容。换句话来说，同样的一段内容只在前面加上“我以为”“他知道”或者“假如”“要么”，其承载的含义则会完全不同，而projection-softmax则有可能优先捕捉到这种逻辑形式（或者模态）上的差异，如果它们与内容本身在特征空间上存在解耦的话。</li> </ol> <p>最后再回过头来看本节开头的幻觉case，这并不是一个单纯的GPT4就是牛逼的例证。这个case在构造上，首先采用了一个对偶的混淆结构“地球/火星 … 卫星数量”，用以误导LLM在感知与抽象上的能力，其次采用了“命题 … 或”的结构为内容设定模态，用以误导LLM对其输出内容的确定性。当这两种误导同时出现的时候，LLM可能存在的，在概念及其联系以及模态上的过度泛化，便可能诱导其输出不符合真实世界情况的事态。</p> <h4 id="四精妙绝伦但又无比愚蠢的sparsely-gated-moe实现">四、精妙绝伦，但又无比愚蠢的Sparsely-Gated MoE实现</h4> <p>在假定上面两节的内容都不错的离谱的前提下，我们可以讨论目前Sparsely-Gated MoE在实例化层面上[19]的作为了，而我是由衷地钦佩当下实现在细节上的智慧的。</p> <ol> <li>整个MoE的实现，都充分利用了token在FFN层之前具有temporal-independent的这个特点。换言之，当下的各种主流的transformer实现中，token仅有在Attention层才得益于QKV的运算具有temporal性质，而位于其主干（可以进行residual操作的路径）的任何一个token是可以脱离sequential属性来做操作的。这使得整个Capacity机制可以在mini-batch内操作，故而可以认为，在训练的过程中，源自于同一个sequence的tokens在某种程度上具有对Expert的选择一致性（因为其tokens之间在特征空间上的差异会有相当的几率小于mini-batch内其他sequence的tokens）。可以想像的是，这个性质是有利于魔法层面上的训练加速的（区别于训练框架物理层面上的加速），参考Residual-MoE[4]。</li> <li>与此同时，Top-2 Gating算法又提供了Experts之间概念记忆的分布性。换句话说，正因为有前述的选择一致性，如果采用Top-1 Gating，则显而易见会造成Experts的选择Bias（更多token的选择集中在少数Experts中）。故而，采用Top-2 Gating以及额外的RTS操作（或者Expert Choice[7]，非本文重点）使得token（比较随机地）通过另外一个Expert，便可以缓解这种Bias，使得概念可以比较分散且精确地记忆在多个Experts中，并使得单一Expert不会过度泛化地记忆概念。</li> </ol> <p>另外再容我斗胆喷一下当前实例化的细节。</p> <ol> <li>Gating模块中的projection-softmax模块的实现上过于naïve，单层的linear-projection+softmax的本质只是一个线性分类器。再通俗一点讲，由于与此之前的token都是经过norm的（这点影响不大，只是为了便于想象），所以大致上可以认为，每一个Expert在这个linear-projection中都有一个特征向量，token则会被分配至与其距离最近的向量所代表的Expert中。这本身没什么问题，但是由于Capacity机制，会强行使得token反过来围绕着Expert进行聚类，这自然会影响语义空间构架的学习，从而降低loss收敛速度（显而易见改成2层的MLP都可以极大的缓解这个问题[20]，但是这绝非完美的解决方案）。</li> <li>Gating的Capacity机制虽然在训练时有着加速训练等诸多优势，但是在推理时却会导致很扭曲的问题。因为势必单一sequence的tokens不会天然的uniform分布至不同的Expert中（实际上更偏向于一个long-tailed分布），故而对单一sequence进行推理（甚至对于多个sequences，如果他们的内容达不到一个平均分布的话）会导致计算效率的低下。故而我们<strong>可能不得不</strong>在推理时也对Capacity进行限定，而由于单张GPU显存的限制，如果要追求推理效率，那么我们就需要在推理的mini-batch-size以及capacity中进行取舍，而这个设定是会影响模型的表现的（这也是GPT4变笨的可能解释之一，即便模型本身没有改变，在工程上的效率优化导致了其性能降低）。</li> </ol> <h4 id="五通向agi的想法没有水文章的有还很多">五、通向AGI的想法没有，水文章的有，还很多</h4> <p>行文至最后，为大家简单介绍一下最近正在进行的相关的工作，也是想表达思考不等于空想，前者总是会有延续的（另外也欢迎有合作意愿的同学参与讨论）。</p> <ol> <li>Scaling Law针对的是Attn还是FFN：正如前文讨论，目前对于Scaling Law的研究都着眼于模型整体而言，但是Transformer是由两种完全不同机制的模块所形成的，是否Attn对于数据的需求大过FFN，甚至于是否不同层的FFN对数据的需求是否一致，这些都是没有被讨论过的。</li> <li>Gating的projection机制如何完善：也如前文讨论，Gating的projection是否会反过来影响语义空间的结构形成，是否可以采用一些本事就可以进行模糊匹配的算法，来代替Gating中的RTS。</li> <li>异构体Experts：考虑到不同领域的输入所包含的信息密度可能会有较大的差异，故而是否可以采用不同构造Experts来捕获不同领域输入对泛化性能的需求。</li> <li>构建基于逻辑运算的RL集：采用逻辑运算可以很容易的构建各式的命题[21]，而我们同时也很清楚针对这些命题的正确输出是什么，故而可以低成本的构建RL数据。</li> <li>小mini-batch下的数据采样：由于训练时的mini-batch的尺寸限制，故而我们可能并不希望来源过度相似的训练数据集中在同一个batch里，这有可能会导致Capacity机制对Gating造成严重损害。</li> </ol> <h4 id="末题外话">末、题外话</h4> <p>写这个的时候就有人喷说，说写得稀烂，即唯心又唯我，废话一堆，大家肯定不爱看，发出去也只会招骂。</p> <p>我站起来，望向挂在墙上老黄的画像（旁边还挂了巴赫、梵高跟康德，如果你们想知道的话），深叹一口气道：</p> <p>你说的都对，我还能再补几句这文章纯粹只是班门弄斧、布鼓雷门、舞文弄墨、不知所谓。但问题是，我们现在跟国外的差真的在H800少的那200GB/s带宽上吗？虽然在LLM业界，当下的共识是数据以及Infra是最重要的，这么说倒也没错。但是当下OpenAI能铁山靠Google，除开长久以来对数据以及Infra的积累，更重要的是长时间在这个领域摸索，所获得的对LLM以及相关衍生事物的话语权，再讲直白点就是萨姆·奥特曼、伊利亚·苏茨基弗等人的嘴。从经验主义的角度来讲，对着LLM的几万维的特征、上千亿维的参数，Rademacher Complexity解释力似乎也比易经强不到哪里去[22]，所以重要的是讲出来自己独有的思考，形成自己的世界观并在实验中修正它，而不是亦步亦趋地捧着OpenAI的玄学当作圣典。人可能并不是活在真实的世界中，但也要活在自己的而不是他人的想象里。</p> <h4 id="参考">参考</h4> <ol> <li>^Shazeer N, Mirhoseini A, Maziarz K, et al. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538, 2017. https://arxiv.org/pdf/1701.06538.pdf</li> <li>^Fedus W, Zoph B, Shazeer N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity[J]. The Journal of Machine Learning Research, 2022, 23(1): 5232-5270. https://arxiv.org/pdf/2101.03961.pdf</li> <li>^Wortsman M, Ilharco G, Gadre S Y, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time[C]//International Conference on Machine Learning. PMLR, 2022: 23965-23998. https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf</li> <li>^abRajbhandari S, Li C, Yao Z, et al. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale[C]//International Conference on Machine Learning. PMLR, 2022: 18332-18346. https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf</li> <li>^Lepikhin D, Lee H J, Xu Y, et al. Gshard: Scaling giant models with conditional computation and automatic sharding[J]. arXiv preprint arXiv:2006.16668, 2020. https://arxiv.org/pdf/2006.16668.pdf</li> <li>^Kim Y J, Awan A A, Muzio A, et al. Scalable and efficient moe training for multitask multilingual models[J]. arXiv preprint arXiv:2109.10465, 2021. https://arxiv.org/pdf/2109.10465.pdf</li> <li>^abZhou Y, Lei T, Liu H, et al. Mixture-of-experts with expert choice routing[J]. Advances in Neural Information Processing Systems, 2022, 35: 7103-7114. https://arxiv.org/pdf/2202.09368.pdf</li> <li>^Komatsuzaki A, Puigcerver J, Lee-Thorp J, et al. Sparse upcycling: Training mixture-of-experts from dense checkpoints[J]. arXiv preprint arXiv:2212.05055, 2022. https://arxiv.org/pdf/2212.05055.pdf</li> <li>^Hwang C, Cui W, Xiong Y, et al. Tutel: Adaptive mixture-of-experts at scale[J]. Proceedings of Machine Learning and Systems, 2023, 5. https://arxiv.org/pdf/2206.03382.pdf</li> <li>^Singh S, Ruwase O, Awan A A, et al. A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training[C]//Proceedings of the 37th International Conference on Supercomputing. 2023: 203-214. https://dl.acm.org/doi/pdf/10.1145/3577193.3593704</li> <li>^https://github.com/microsoft/DeepSpeed/blob/moe-full-tp/deepspeed/moe/sharded_moe.py#L388</li> <li>^Lu L, Shin Y, Su Y, et al. Dying relu and initialization: Theory and numerical examples[J]. arXiv preprint arXiv:1903.06733, 2019. https://arxiv.org/pdf/1903.06733.pdf</li> <li>^https://mephilosophy.ccu.edu.tw/entry.php?entry_name=模態認識論（對模態性的認識論）</li> <li>^https://www.bilibili.com/video/BV1SN4y1A7zn/</li> <li>^https://www.bilibili.com/video/BV1T14y1h7zA/ ^GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE https://www.semianalysis.com/p/gpt-4-architecture-infrastructure</li> <li>^GPT-4 “炼丹”指南：MoE、参数量、训练成本和推理的秘密 https://www.8btc.com/article/6825966</li> <li>^Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models[J]. arXiv preprint arXiv:2001.08361, 2020. https://arxiv.org/pdf/2001.08361.pdf</li> <li>^https://zhuanlan.zhihu.com/p/643559472</li> <li>^https://github.com/minogame/public_image/issues/1</li> <li>^https://zh.wikipedia.org/wiki/命题</li> <li>^易经视野下的互联网金融及监管 https://pdf.hanspub.org/FIN20210100000_60943319.pdf</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/2024-01-19-cvpr-en/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-en/">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-cn/">南山笑话集锦</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-en/">Some Stray Thoughts After Leaving the Large Model Industry (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-cn/">离开大模型业界后的一点杂念</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhun Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>