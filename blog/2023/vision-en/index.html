<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Vision Foundation Models Are Utterly Useless (DeepSeek Translated Version) | Zhun Sun </title> <meta name="author" content="Zhun Sun"> <meta name="description" content="Mai-Haishin · June 29, 2023, 21:15 · Guangdong"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minogame.github.io/blog/2023/vision-en/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="./"> Zhun Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Vision Foundation Models Are Utterly Useless (DeepSeek Translated Version)</h1> <p class="post-meta"> Created in June 29, 2023 </p> <p class="post-tags"> <a href="./blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="./blog/tag/philosophy"> <i class="fa-solid fa-hashtag fa-sm"></i> philosophy</a>   ·   <a href="./blog/category/english"> <i class="fa-solid fa-tag fa-sm"></i> English</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recently, due to various coincidences, I’ve been repeatedly asked a question that irritates me immensely: How many parameters does your vision model have? Although my Ph.D. training allows me to maintain an expression as unreadable as the 1940 European map (utterly devoid of Poland), digging deeper, this question evokes a conceptual meaninglessitude that has bred a rational aversion in me.</p> <p>I. The Gap Between 22B and 175B Is Roughly 175B First, I won’t deny that as the parameter count of vision models increases, their absolute performance on traditional vision tasks also improves. A recent prime example is Google’s ViT-22B, which, despite relying on a small amount of non-open-sourced data and <del>conservative</del> alchemy tricks to get running, hasn’t stopped the field from maintaining an optimistic outlook—that computer vision is still worthy of foundation models. However, in my view, this “largest vision model to date,” whose presence is as thin as arXiv’s homepage, ironically proves that scaling parameters and data volume is meaningless for pure vision tasks.</p> <p>A little reflection reveals that this model was trained on ~3T tokens (JFT-4B × 256 tokens/img × 3 epochs), already twice the training volume of the largest LLaMA model. Yet ViT-22B offers no truly meaningful conclusions beyond marginal gains on legacy datasets—essentially, it’s more sketch-oriented than texture-oriented. With an ROI comparable to a failed military campaign, will there be another “big vision model”? I doubt it. And let’s not forget: the gap between 22B and 175B is roughly 175B.</p> <p>II. Vision Models Remain Dull, No Matter Their Size Before diving into metaphysics, let’s empirically recap three “dark clouds looming over modern computer vision.” These issues aren’t ignored—they attract endless papers yearly, all failing to solve them (which ironically confirms their intractability).</p> <p>Adversarial Attacks: While NLP also faces this, the advent of (generative) LLMs has largely sidelined the issue. In CV, however, it haunts every unfortunate reviewer like a vengeful ghost, because no one has dared to claim that a foundation model can end this problem. Worse, nobody even bothers testing vision foundation models for adversarial robustness anymore—it’s now accepted that vision models are meant to be broken.</p> <p>The Elephant in the Room: Proposed in 2018 to question detection models’ validity, this problem highlights how even if an elephant is photoshopped into an ordinary room, our “brilliant” models can detect it with IoU &gt; 0.9—despite being empirically absurd. It’s a philosophical issue threatening CV’s foundations, but since solving it won’t boost COCO metrics (or might hurt them), everyone just treats it like the elephant in the room.</p> <p>Disorder in Sample Space: Visual information lacks systematic structure in feature space. Even state-of-the-art self-supervised learning can cluster objects without understanding their semantics (evidenced by clean t-SNE separability), but semantically, a tiny r-neighborhood around a space shuttle might include orange tabbies or jam sandwiches. Optimistically, this boosts creativity; pessimistically, it forces researchers to chase benchmark gains rather than meaningful features.</p> <p>III. Knowledge in NLP vs. Knowledge in CV First, let’s define two terms (long-debated, so we’ll use ChatGPT’s colloquial definitions):</p> <p>Representation: In philosophy of mind/epistemology, a mental state that stands for something else. It implies our minds can refer to objects, ideas, or situations, acting as intermediaries between mind and world (e.g., mental images, beliefs, language symbols).</p> <p>Concept: An abstract/general mental representation of a category (objects, events, ideas). Concepts organize thought/language via abstraction/generalization.</p> <p>Crudely, representations are psychological phenomena, while concepts are cognitive constructs.</p> <p>With these clarified, we can distinguish the knowledge acquired by NLP and CV models:</p> <p>NLP (Generative Models): Training data essentially presents Relations of Concepts (à la Hume, but inductively learned). Language models understand concepts indirectly via their relations—not through direct human-like comprehension (e.g., of “time”). Thus, they learn representations woven from concept relations, plus deeper layers of such representations. The human (linguistic) world’s representations are countless, requiring trillions of parameters to memorize and relate them (a quantum N-gram of representations).</p> <p>CV (Supervised/Self-Supervised): The optimization goal is to map concrete data to specific representations, then—via manual task design—abstract these into concepts (a Matter of Facts process). Why “manual”? Because without human guidance (e.g., linear probing), models won’t自发抽象; they’ll just align data to representations. Algorithms like I-JEPA (discussed later) bake in abstraction paths, hence their better linear probe performance.</p> <p>IV. Parameters Can’t Fix Abstract Concepts Back to the damned parameter count: for current vision tasks, scaling up won’t trigger a “ChatGPT moment.” Revisiting the three dark clouds:</p> <p>Diminishing Returns on Concept Induction: More parameters marginally improve vector-space precision for clustering representations (e.g., under Gaussian priors). But with finite samples, error soon stems from prior distributions, leaving gains as meager as +0.1% on ImageNet. Worse, higher dimensions (from scaling) exacerbate the curse of dimensionality, demanding more samples per concept and expanding adversarial attack surfaces.</p> <p>No Logic Without Induction: Neural nets are universal approximators, and capacity grows with parameters. Empirically, larger models can encode complex scenes (e.g., a fruit platter) into single feature vectors—but we lack high-quality human labels to conceptualize them. At best, we supply coarse tags (“still life,” “fruit bowl”), too abstract for the model’s granular representations. Without data teaching “AND/OR/NOT” relations, models won’t infer that a “forest + bicycle” should be “park,” or “forest − river” equals “barren hill.”</p> <p>Disorder Persists: Visual data lacks language’s hierarchical concept relations, so models can’t自发 form structural representations or derive new concepts. For instance, before aligned cross-modal models, zero-shot learning was dismal even in narrow domains (CUB birds/Oxford flowers). While LLMs can draw unicorns from pure text, a vision model may never recognize “a destroyed Leopard 2 tank”<del>—unless Russians supply enough labeled data</del>.</p> <p>V. Naive Cross-Modality Alignment Isn’t the Cure Users of Stable Diffusion notice that prompts work best as咒语-like word lists, not natural sentences. CLIP users see that image-text scores cluster narrowly, lacking clear thresholds. Beyond training quirks, this reveals:</p> <p>Visual Concepts Are Sparse: Top learned concepts are often simple words/phrases. When I trained Chinese CLIP, 80% of the top 1M concepts were names—reflecting real-world scarcity of describable visual concepts versus proper nouns.</p> <p>Complex Representations ≠ Concepts: Vision models can learn complex representations but can’t abstract them into concepts like text can, causing misalignment. Even with language, few training examples teach, say, “Leopard 2 tank wreckage.” Hence, Locked Image Tuning makes sense: using text to abstract visual representations is easier than vice versa.</p> <p>VI. Surrender to LLMs—Or Not? Finally, where do scaled-up vision models go? (Excluding pure representation learners like Go/weather models, where scaling does help.)</p> <p>Full Surrender (PaLM-E, Visual Programming): Let vision models serve LLMs by feeding them visual representations (depth maps, optical flow) to patch spatial reasoning gaps. But this relies on endless human-labeled data—like Louis XVI at 39, the road ahead is interminable.</p> <p>Force-Fed Relations (I-JEPA, SiamMAE): Make ViTs model inter-representation relations. This partly solves the 2nd/3rd dark clouds but struggles with complex scenes (e.g., poor ImageNet fine-tuning). Sadly, the academic gatekeepers—Chinglish-speaking reviewers—torture any underperforming idea, trapping innovation in a black hole.</p> <p>P.S. How do I answer “How many parameters?” Coldly: “ViT-B, 88M. They won’t deploy bigger ones.”</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="./blog/2025/jokes-en/">Compilation of Nanshan Jokes (DeepSeek Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="./blog/2025/jokes-cn/">南山笑话集锦</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="./blog/2023/model-en/">When Your Model is as Frustrating as Your Life (DeepSeek Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="./blog/2023/model-cn/">当你的模型与你的人生一样糟心</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="./blog/2023/vision-cn/">视觉大模型一无是处</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhun Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>