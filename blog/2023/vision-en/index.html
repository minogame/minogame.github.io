<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Vision Large Models Are Utterly Useless (Gemini 2.5 Pro Translated Version) | Zhun Sun </title> <meta name="author" content="Zhun Sun"> <meta name="description" content="Mai-Haishin · June 29, 2023, 21:15 · Guangdong"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minogame.github.io/blog/2023/vision-en/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhun Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Vision Large Models Are Utterly Useless (Gemini 2.5 Pro Translated Version)</h1> <p class="post-meta"> Created in June 29, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/philosophy"> <i class="fa-solid fa-hashtag fa-sm"></i> philosophy</a>   ·   <a href="/blog/category/english"> <i class="fa-solid fa-tag fa-sm"></i> English</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recently, due to various coincidences, I’ve been repeatedly asked a question that I find extremely repulsive: How many parameters does your vision model have? Although my Ph.D. professional training allows my expression to remain in that “1940 map of Europe without Poland” state (i.e., completely impassive/poker-faced), to get to the root of it, the concept of “meaninglessitude” induced by this question has already formed a rational aversion in me.</p> <p>I. The Gap Between 22B and 175B is Approximately 175B</p> <p>First, I don’t deny that as the parameter count of vision models increases, their absolute numerical performance on traditional vision tasks also improves accordingly. The best recent example is Google’s ViT-22B, although it required a small amount of non-open-source data and <del>conservative</del> “alchemy” skills (i.e., meticulous, perhaps arcane, empirical tuning) to get this model running. However, this doesn’t prevent the community from generally maintaining an optimistic attitude, believing that computer vision is still worthy of large models. Yet, in my view, this largest vision “afterlife model” (a model so large it’s as if sent to the great beyond, perhaps implying it’s impressive in scale but not practical impact), with an existence as thin as the arXiv homepage, has coincidentally proven that “piling up parameters and data volume is meaningless for the field of pure vision.”</p> <p>Think about it for a moment: the training volume of this model actually reached (JFT-)4B images * 256 tokens/img * 3 epochs, approximately 3T tokens. This is already double the training volume of the largest LLaMa model. But ViT-22B didn’t yield any truly meaningful conclusions, apart from a slight increase of “three to five pecks” (a tiny amount) on datasets from the “classic old world” (older, standard benchmarks). That is, the model is more biased towards sketch rather than texture—these rather vacuous findings. With this conclusion, whose ROI is comparable to a spring/summer counteroffensive (implying massive effort for little gain), will there be another large model in vision? I don’t think so. However, don’t forget, the gap between 22B and 175B is approximately 175B.</p> <p>II. Vision Models Remain Dull Even When Large</p> <p>Before moving on to the metaphysical content in the next section, I will first, from an empiricist perspective, recount three “dark clouds hanging over modern computer vision.” Of course, these problems have not gone unnoticed, as every year there is a continuous flood of related papers milking these issues for publications/citations—which indeed proves they are difficult to solve.</p> <p>The first to be discussed is naturally the clichéd problem of adversarial attacks. To be fair, although the NLP field also has this problem, it has largely been ignored since NLP entered the era of (generative) LLMs. In contrast, in the CV field, this problem haunts every unlucky reviewer like a vengeful spirit, because no one has stepped up to write a paper claiming that a large model could put an end to this area of research. Moreover, nowadays, no one is even willing to test the adversarial attack vulnerability of these large vision models, because people have generally accepted the idea that vision models <em>should</em> be attackable (i.e., it’s an inherent, accepted flaw).</p> <p>Secondly, there’s the “elephant in the room” problem, proposed in 2018 to question the effectiveness of detection models and algorithms. The gist is that even if I photoshop an elephant into a picture of an ordinary room, our “intelligent” models can detect it with an IOU &gt; 0.9, even though this is absurd from an experiential standpoint. This is actually a philosophical problem that could affect the very foundations of CV, but solving it or not doesn’t affect an algorithm’s performance on COCO (or it might only have a negative impact), so people just treat it as the elephant in the room.</p> <p>Finally, the third dark cloud is a bit more practical, which I like to call the problem of the disordered sample space. In other words, visual information fails to form a systematic structure in the feature space of samples. Here, visual information refers to the abstractable semantics present in images, and systematic structure refers to the relationships between these semantics. Even with advanced self-supervised training techniques that allow certain types of objects to “huddle together for warmth” (cluster well, reflected by good t-SNE separability) without needing to know their semantics, from a semantic perspective, a very small r-neighborhood of a space shuttle can contain both an orange tabby cat and a jam sandwich. On the bright side, this enhances creativity based on visual features. On the downside, it forces vision researchers to constantly seek to create visual features based on improving performance on specific benchmarks.</p> <p>III. Knowledge in NLP and CV</p> <p>First, we need to confirm two concepts. Since the precise definition of these two concepts has long been a tangled issue, here we adopt the general definitions provided by ChatGPT:</p> <p>Representation: In philosophy of mind and epistemology, “representation” refers to a mental state or entity that stands for something. The definition of representation implies that our minds have the ability to refer to or express objects, ideas, or situations existing in the world. Representations can take various forms, including mental images, beliefs, thoughts, or linguistic symbols, and can be seen as intermediaries between the mind and the external world, enabling us to have knowledge, perception, and understanding of the world.</p> <p>Concept: On the other hand, in epistemology, a “concept” is an abstract or general mental representation that includes a type or class of things, events, or ideas. Concepts are the basic building blocks of thought and language, enabling us to categorize and organize our experiences and knowledge. Compared to a single representation, concepts are more abstract and general in scope. Concepts are formed through processes of abstraction and generalization, where we identify common features or properties across multiple instances and create a mental category to represent these shared characteristics.</p> <p>To put it bluntly, both “representation” and “concept” involve expressing and conveying certain information or meaning, but their nature and origin are different. Representation can be seen more as a psychological-level phenomenon, while a concept can be considered a product at the thinking level.</p> <p>In fact, I believe that after understanding these two concepts, the knowledge acquired by NLP and CV models at the cognitive level becomes easy to distinguish and comprehend. I believe that within the framework of language (especially generative) models, the data we provide to train a model essentially exhibits Relations of Concepts (here I borrow Hume’s idea, but unlike pure deduction, Relations in language models are still obtained through induction). Beyond this, for the understanding of a Concept itself, language models also gain an indirect cognition through its associations with other Concepts, without a direct understanding akin to human mental comprehension (especially for concepts like “time”). In other words, what language models learn are numerous Representations formed by the associations between Concepts, as well as more profound and complex representations presented by the associations between these representations. The representations that exist or can be understood in the human (linguistic) world are as numerous as stars; we need trillions of parameters to memorize these representations and understand their interrelations (or, an N-gram where representations are the quanta).</p> <p>In contrast, for vision models, within (almost all) supervised or self/unsupervised training frameworks, the optimization objective is essentially to transform specific concrete data into a specific unique Representation, and then, through specific vision tasks (i.e., human experience), to ultimately abstract these representations into Concepts. In other words, it’s a process of learning Matters of Fact. Let me further explain why specific vision tasks are needed. Most people with experience training (self-supervised) large vision models will have this understanding: it’s very difficult to evaluate the effectiveness of vision models. People just choose the ones with good performance on k-NN/Linear Probe/Attentive Probe/Finetune (classification or other downstream tasks) to write papers. But the essential reason here is that without human experience to help the vision model perform abstraction, an algorithm won’t proactively do such abstraction (it has no Loss function or obligation to do so); it only needs to map data to representations. Conversely, if the path to abstracting Concepts is already considered in the algorithm’s design, then the model’s performance on Linear Probe will naturally be better (e.g., Yann LeCun’s I-JEPA, which will be discussed later).</p> <p>IV. Parameter Count Can’t Solve Conceptual Abstraction</p> <p>Now, let’s return to that damned parameter count. For current vision tasks, the parameter count of corresponding models probably cannot be the main indicator for a “ChatGPT moment” in large vision models. Back to those three dark clouds over modern computer vision:</p> <p>Parameter count can only yield marginal gains in the strength of concept induction. Generally, when vision models induce concepts from representations, they cannot escape a general notion of clustering. In other words, we have prior knowledge that “representations of the same concept will cluster together,” e.g., a Gaussian Prior, and then we obtain posterior results through the actual distribution of representations. However, the benefit of increasing parameter count to more precisely express the position of representations in vector space is destined to become marginal, as parameter count does not have a linear relationship with error. So much so that from a certain point, with a limited number of samples, errors originate more from the prior distribution, and ultimately, increasing parameter count only manifests as that 0.1% performance boost on ImageNet. Besides, the side effect of increased parameter count—higher vector space dimensionality—also increases the number of representations needed to induce a concept (curse of dimensionality), thereby providing more space for adversarial examples to exist.</p> <p>Parameter count cannot, independent of induction, know the existential logic between representations or concepts. Theoretically, neural networks can perform Universal Approximation, and this ability strengthens with increasing parameter count. In fact, from my personal experience, a model with more parameters can generally use a single feature vector to represent a relatively complex scene, such as a plate holding various fruit ornaments. However, we cannot provide high-quality human experience to conceptualize such things. In this example, we can usually only provide concepts like “still life,” “fruit plate,” or “a plate holding various fruit ornaments,” or combinations thereof. However, such concepts are too abstract for the representations the model can actually express. The model usually can only statistically generalize quantities of more primary representations to compose such highly complex representations. So, lacking guidance from data that can enumerate associations like “AND,” “OR,” “NOT,” etc., the model naturally won’t proactively categorize a complex representation into entirely different concepts just because a primary representation is present or absent. For example, lacking data, a model is unlikely to classify an image of a “forest” as a “park” just because there’s a bicycle in it, or classify it as “green mountains” just because a river is missing.</p> <p>Parameter count also cannot solve the problem of disordered sample space, thus preventing the model from spontaneously learning new concepts arising from associations between concepts, in the absence of human experience. In natural language, concepts themselves have good hierarchical association structures. Therefore, representations formed by the association of concepts or representations (e.g., a sentence) also acquire a structural expression. Based on this structural expression, we can easily continue to create new concepts or representations. In contrast, visual information lacks such hierarchical association structures. Visually similar representations can correspond to completely different concepts (e.g., the letter ‘l’ and the number ‘1’). So, in the absence of human experience (self/unsupervised), it’s difficult to spontaneously form structural expressions in the (representation’s) feature space, and consequently, new concepts cannot be formed spontaneously. A prominent example is that before the emergence of cross-modal Align models, almost no vision algorithm could perform generalized zero-shot learning. Even on specific domains (CUB birds or Oxford flowers), zero-shot performance was very poor. While large language models can draw unicorns by purely interpreting linguistic concepts, a large vision model probably will never recognize “a destroyed Leopard 2 tank” <del>(unless the Russkies can provide it with enough human experience).</del></p> <p>V. Naive Cross-Modality Alignment Doesn’t Solve Fundamental Issues</p> <p>Those who have played with Stable Diffusion should have noticed this phenomenon: most prompts are like incantations of independent words, while scenes described in true natural language are difficult to depict accurately. Those who have directly worked with CLIP should also have noticed this: the vast majority of image-text matching scores (for both positive and negative samples) are distributed within a relatively narrow range, and it’s difficult to use an intuitive threshold to determine their match. Excluding reasons related to model training, this highlights two problems:</p> <p>The number of Concepts that the visual end can accurately learn is limited, and they are mostly simple word/phrase-level Concepts. In fact, when I was actually training a Chinese CLIP, I encountered a situation where 800,000 out of the top 1 million learned Concepts were personal names (yes, the proportion of names becomes even higher further down). And this actually conforms to real-world distribution. Gentlemen, you can stand up right now and describe the objects you see using language; you’ll find that the relatively simple visual Concepts encountered in real life are quite scarce, while names of people around you are more common.</p> <p>It’s not that the visual end cannot learn a relatively complex representation, but the visual end cannot generalize such complex representations into concepts like the textual end can. This leads to discrepancies in matching between the visual and textual sides. In reality, the data in training sets that allows the visual end to generalize a concept corresponding to a complex representation is limited. That is to say, even with the help of language, a large vision model will probably never recognize “a destroyed Leopard 2 tank” (unless the Russkies can provide it with enough human experience).</p> <p>So, why has “Lock Image Tuning” now become a relatively reasonable CLIP training mode? Because the difficulty of using language to generalize representations learned by the visual end into concepts is far less than that of using vision to understand complex representations composed of multiple abstract concepts in language.</p> <p>VI. To Convert or Not to Convert to Language Models</p> <p>Finally, let’s discuss the way forward for scaling up vision models. Of course, we must exclude models purely aimed at learning representations, such as Go or weather models. The capabilities of these models themselves lie in learning mysterious representations that humans cannot abstract with language. As the data scale of these models increases, the parameter count naturally needs to scale up to enhance representational ability (e.g., on a 99x99 Go board, a model with hundreds of billions of parameters will theoretically perform better than one with tens of billions).</p> <p>Current academia has generally found two ways out. One is to have vision models completely convert to the capabilities of large language models (e.g., Google’s PaLM-E and this year’s CVPR best paper, Visual Programming), letting vision models return to the function of providing visual representations for concepts, and allowing large language models to perform the interpretation of more complex concepts. At this point, the parameter count of vision models can be used to learn some representations that humans cannot abstract with language (e.g., depth maps, optical flow, hyperspectral signals), thereby compensating for some weaknesses of language models in spatial reasoning, bringing them closer to real-world AGI. However, this mode still relies on the “Russkies” to destroy tanks and collect vast amounts of data to provide human experience. The road ahead is like Louis XVI at age 39—no end in sight.</p> <p>Another line of thought is reflected in Yann LeCun’s I-JEPA and Fei-Fei Li’s SiamMAE, where we forcibly make vision models understand the associations between representations. This task itself is not particularly difficult for ViT models with attention mechanisms. The biggest advantage of doing this is that it can partially solve the aforementioned second and third dark clouds. However, because these solutions emphasize individual representations, they find it relatively difficult to learn complex representations formed from multiple representations and concepts, which is particularly evident in poorer finetuning performance on ImageNet (ImageNet has some relatively complex scene categories). And in fact, the current vision academia is not very tolerant of algorithms with mediocre performance. Any bit of open exploration will be relentlessly harassed by reviewers speaking Chinglish. New algorithms seem trapped in an endless black hole, struggling to be born.</p> <p>Of course, if you’re interested in how I answer the question “How many parameters does your vision model have?”, I generally say coolly, “ViT-B, 88M. They won’t let us deploy anything bigger.”</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/2024-05-14-cv2llm-en/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/2024-01-24-rebuttal-en/"></a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-en/">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-cn/">南山笑话集锦</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-en/">Some Stray Thoughts After Leaving the Large Model Industry (Gemini 2.5 Pro Translated Version)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhun Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>