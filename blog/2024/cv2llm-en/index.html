<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CV to LLM, Please Understand GPT Through the Worldview of Reinforcement Learning (Gemini 2.5 Pro Translated Version) | Zhun Sun </title> <meta name="author" content="Zhun Sun"> <meta name="description" content="Mai-Haishin · May 14, 2024 18:05・Guangdong"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minogame.github.io/blog/2024/cv2llm-en/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhun Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">CV to LLM, Please Understand GPT Through the Worldview of Reinforcement Learning (Gemini 2.5 Pro Translated Version)</h1> <p class="post-meta"> Created in May 14, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/philosophy"> <i class="fa-solid fa-hashtag fa-sm"></i> philosophy,</a>   <a href="/blog/tag/misc"> <i class="fa-solid fa-hashtag fa-sm"></i> misc</a>   ·   <a href="/blog/category/english"> <i class="fa-solid fa-tag fa-sm"></i> English</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Lately, I’ve been tormented to the point of being somewhat fed up. People doing face segmentation, detection, OCR—one by one, they’ve suddenly transformed into multimodal large language model experts, and then they proceed to critique me using a Computer Vision (CV) worldview from the last decade. I actually empathize a lot with these folks living in their large flats in first-tier cities because, apart from not having a large flat myself, I also repeatedly jumped back and forth from traditional CV and traditional Machine Learning (ML) into this field. And when I first arrived, I felt quite insecure, because even the mathematical notation system I was accustomed to had different expressions in the world of Reinforcement Learning (RL). Fortunately, I inherently have a self-deprecating personality and would grab newly graduated campus recruits to teach me various details. However, these people with their large flats are different; the psychological superiority that comes with a large flat prevents them from lowering their arrogant heads.</p> <p>To put it simply, an LLM, in a sense, is like this game: 1 2 4 8 16 (?). Some will fill in 32, some 31, and others will provide sound reasoning to fill in 114514. All these answers can be correct under different circumstances, so you just need to pick one based on your mood. Now, you’ve filled in 32 because it seems most likely to help you pass an administrative aptitude test. So the game becomes: 1 2 4 8 16 32 (?). And you continue by filling in 64. But then you hesitate. You start to wonder if someone with dyscalculia can understand three-digit numbers, or if continuing this cycle will eventually become an exponential Ponzi scheme. So, after much hesitation, you write <code class="language-plaintext highlighter-rouge">&lt;|endofresponse|&gt;</code>, and the game ends thus.</p> <p>The so-called three-stage LLM training prevalent in the market roughly operates on this logic. In the pre-training stage, you find numerous sequences from <a href="http://oeis.org" rel="external nofollow noopener" target="_blank">http://oeis.org</a> (On-Line Encyclopedia of Integer Sequences) to learn their patterns, such as 2 3 5 7 11 (<a href="https://oeis.org/A000040" rel="external nofollow noopener" target="_blank">A000040</a>), or 2 5 11 17 29 (<a href="https://oeis.org/A007491" rel="external nofollow noopener" target="_blank">A007491</a>), or stranger ones like 1 2 3 5 7 12 (<a href="https://oeis.org/A326083" rel="external nofollow noopener" target="_blank">A326083</a>). From this, you grasp the underlying logic of how a sequence is constructed. Now you can write any regular sequence based on your mood. Subsequently, in the instruction fine-tuning stage, you learn how to continue a given sequence with one or more numbers, and to write ‘eor’ (end of response) at the appropriate time to end the game. Finally, in the RLHF (Reinforcement Learning from Human Feedback) stage, you are told you are participating in an aptitude test, so you must reduce the impulse to fill in 114514, even if you have evidence that it’s feasible.</p> <p>Through the above two crude paragraphs, I hope I have conveyed, in the plainest language possible, what the worldview of reinforcement learning is. If you, coming from CV, can roughly understand this, then you will also surely understand the following points:</p> <ol> <li>Whether in pre-training or instruction fine-tuning, GPT’s object of operation is always <em>the next token</em>, or in other words, <em>action based on current state</em>. It’s only due to the ingenious design of the QKV attention transformer and attention masks that this work can be executed in parallel. This is fundamentally different from how Vision Transformers (ViT) operate; the latter processes all tokens simultaneously. Therefore, when forcibly stuffing ViT’s numerous tokens into a GPT model, one must somewhat consider: what is the action, what is the current state.</li> <li>In sampling mode, the token predicted by a GPT model is merely a manifestation of a <em>possible</em> action. In “folk science” terms, it’s as if the probability density function of the action collapses to a specific value upon observation. In contrast, a ViT’s output embedding is a specific, deterministic embedding. Therefore, the most ideal training mode for a GPT model is on-policy sub-optimal sampling, whereas a ViT can be directly SFTed (Supervised Fine-Tuned).</li> <li>Following the above point, the combinations of tokens that a GPT model can predict are already determined in the pre-training phase. Subsequent instruction fine-tuning and RLHF should not allow it to acquire a <em>new</em> action space. If this unfortunately happens, the data should be moved to the pre-training phase; otherwise, a shift in the action space will most likely lead to the model’s performance not being fully demonstrated or directly degrading. In my understanding, ViT does not have a similar problem, which is why many papers suggest that unfreezing ViT weights during the SFT phase leads to performance improvements.</li> <li>A Reward Model (RM) is not a Discriminator, much less a binary classifier for good and bad data. An RM is a weak estimator that assesses whether your action/trajectory satisfies a specific objective (e.g., human preference) in a particular state. Based on the current pair-wise training mode, the prerequisite for an RM to work is that all responses are based on the same prompt, thereby minimizing the bias introduced by the prompt itself as much as possible. Using the previous example, in the same state “1 2 4 8 16”, the RM deems action=32 more likely to help you pass an aptitude test than action=114514. But if there is no identical state, and the RM is directly presented with the sequences “1 2 4 8 16 114514” and “837 130 5 391 3281”, the RM would likely still consider the former more likely to pass an aptitude test. So please don’t fall into the meek thought pattern of “using RM to clean SFT data.”</li> <li>RLHF is not equivalent to (SFT loss of positive samples) minus (SFT loss of negative samples), because both positive and negative samples originate from the <em>same</em> policy model. We are merely evaluating the magnitude of the reward resulting from one of its actions, thereby adjusting the probability of taking that action in the current state. Of course, RLHF in this aspect is still a theoretical and practical sinkhole, but if you fall into the mindset of positive and negative samples, then there’s nothing left to do in this topic but to go tune GPT-4o and grind data.</li> <li>Following the above point, responses sampled from the same prompt should also not be interpreted using a contrastive loss approach between “positive” and “negative” samples. Even if adding these tricks now seems to boost benchmark scores, I still believe this is not a reasonable thought pattern because it still severs the core issue that “positive and negative samples originate from the same policy,” transforming the problem into “preferential selection among different policies.”</li> </ol> <p>This is all for now. If I think of anything else to add, I will. Please also offer your criticisms and guidance.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-en/">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-cn/">南山笑话集锦</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-en/">Some Stray Thoughts After Leaving the Large Model Industry (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-cn/">离开大模型业界后的一点杂念</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/coinb-en/">Large Models and Coin Minting, Continued (Gemini 2.5 Pro Translated Version)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhun Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>