<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Labor-Intensive Large Models Have No Future (Gemini 2.5 Pro Translated Version) | Zhun Sun </title> <meta name="author" content="Zhun Sun"> <meta name="description" content="Mai-Haishin · July 18, 2024 12:49・Guangdong"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://minogame.github.io/blog/2024/human-en/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Zhun Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Labor-Intensive Large Models Have No Future (Gemini 2.5 Pro Translated Version)</h1> <p class="post-meta"> Created in July 18, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/misc"> <i class="fa-solid fa-hashtag fa-sm"></i> misc</a>   ·   <a href="/blog/category/english"> <i class="fa-solid fa-tag fa-sm"></i> English</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Ever since Louis XIV ascended the throne 380 years ago, people have constantly come to ask me: what is the biggest difference between the “Turbo” type and the “Omni” type models?</p> <p>Actually, for this question, I have prepared three tiers of answer templates: If the other party is a highly-cited scholar, I will pretend to answer cautiously, “The Turbo type has only one modality, but the Omni type has multiple modalities.” This way, they will narrow their eyes, already squeezed into slits by their chubby faces, and with a foolish grin, condescendingly instruct you, “Yes, there are four modalities, do you know what they all are?” – This is what’s known as dismounting to fight an opponent who is mounted (a tactical mismatch to expose arrogance). If the other party is a newly joined intern, I will robotically recite some technical details discussed in their papers and tell them that in another six months, we will have an open-source Omni type for them to experiment with – this is like a middle-tier horse stably outperforming a lower-tier one. But if I’m facing you, my most sagacious and diligent readers, I will try to impart this most meaningful answer:</p> <p><strong>Turbo type</strong> is a technology-intensive product, while <strong>Omni type</strong> is a labor-intensive product.</p> <p>The confidence with which I can say this comes from GPTs, which perhaps are on the verge of dying out: although there are some issues not inherent to GPTs themselves (e.g., Bing-based search functionality being crippled by various search ranking hacking websites), the core reason is still that GPT itself cannot follow the various complex instructions of the real world. This means that among the numerous GPTs, only an extremely small number, in very specific domains, performing repetitive standard tasks, can be executed well by it. In other words, our current technology has not yet evolved to provide a precise, general-purpose instruction interpreter (which is a core component of GPTs), to the extent that we need to build a system akin to QEC (Quantum Error Correction) to allow this interpreter to function fully. And the construction of this system itself is non-standardized and dependent on experience and time costs. So, after the initial hype faded, GPTs gradually disappeared from sight, fully signifying the bankruptcy of the “provide a technology-intensive base model and let users customize it” approach. At this point, OpenAI had no choice but to follow the industry’s footsteps and train an Omni type: if you use this model enough, you will naturally and fully experience the results optimized for “niche domains, fine-grained categories.” Even if it might not be as good as the Turbo type at understanding longer instructions, it is genuinely fast and good on commonly used (especially leaderboard-gaming) commands.</p> <p>And why has the industry taken such steps? According to my conjectural model above, the sequence of events was probably like this: initially, someone said they were two months behind others. But words need proof, so let’s do some benchmarking. However, upon checking the academic leaderboards, they were shocked to find they had inadvertently crushed everyone else a year ago, yet the actual user experience was almost as bad as a widely mocked internet meme (like “Kun Kun”). So, the solution was this: let’s divide user scenarios into a dozen major categories and hundreds of subcategories. This way, as long as we win by a slight margin in any category, we get a point. Then, we merge the subcategories where we lose badly into “other scenarios.” Thus, we can finally experience the feeling of winning again on the “User Capability Experience Leaderboard.” Consequently, development became oriented towards categorization optimization. Each category was assigned to a few people, who then “picked” some “reference answers” from others’ models, integrated them into a self-developed training framework, and used a self-developed PPO algorithm (“Pippi O”) to tirelessly improve the scores for each small category. Gradually, the production process for new-quality labor-intensive large models was born.</p> <p>Setting aside discussions of technology and ideals for now, let’s talk about this model itself. I realized that its biggest advantage is that it gives capitalists and their management a sense of security:</p> <ol> <li>It effectively combines the politically correct concept of “real-world application” (落地 - luòdì) with their work direction. Because indeed, human creativity, on average, is very poor, including the diversity and complexity of questions asked. So, there’s nothing wrong with optimizing for the essence of a targeted repeater (i.e., common, specific queries). If we don’t do this, nobody knows what we’ll end up with, which would lead to a sharp decline in user experience, severely deviating from the objective business truth that you need real-world application to make money.</li> <li>It effectively provides a basis for writing sponsored articles (“soft content”), and the investment in soft content can, at some future time of financial consolidation, be “converted” into goodwill. This forms a virtuous cycle of “the more investment in soft content -&gt; the higher the valuation; the higher the valuation -&gt; the more attention received; the more attention received -&gt; the less investment in soft content is needed” (a cycle described perhaps ironically as “more soft content investment leads to less soft content investment”). Simultaneously, it also provides a stable monetization model for major evaluation agencies, creating a good ecosystem. After all, boss, you wouldn’t want your own large model to score second to last in our programming category, right?</li> <li>It effectively creates identifiable intangible assets—data. From an accounting perspective, the hardware required for large models depreciates in a few years, frameworks are open-source turned self-developed, people leave or burn out after a while, and trained models become outdated in two months. Only data, high-quality data with a good classification system, is the most solid, never-degenerating asset on my books.</li> </ol> <p>However, OpenAI has the confidence to create a labor-intensive model.</p> <p>Firstly, they genuinely have a technology-intensive model. They can not only acquire vast amounts of data almost cost-free using <strong>the data from <em>your</em> calls to GPT-4 to process <em>your</em> data</strong> (and a large part of this is real user cases encountered online), but they can also conduct strictly logits-based distillation experiments. The Omni type is OpenAI’s Omni type, but it’s <em>your</em> labor-intensive type. And I think the name “Omni” is still debatable. If I were Sam Altman, I’d definitely call it the “<strong>Lightwing Type</strong>” Close Support Cruel GPT. Secondly, OpenAI has a large number of algorithm personnel who truly understand the technology. Many influential algorithms like GPT, PPO, CLIP, etc., all originated there. To use an analogy, I could say they have a “complete technology supply chain.” In contrast, your company’s “Senior/Super” titled experts/researchers/scientists are basically “big lovelies” (often used sarcastically for those out of touch) who feasted on the open-source dividends of CV/NLP for the past ten years. The “technology” they can supply is merely tricks to improve performance on some already beaten-to-death benchmark by 0.X%. Their entire thinking system is out of touch with the times; they can’t even read or derive some of the most basic formulas. In the end, it seems everyone is working very hard, but they are just constantly calling other people’s models to process data day after day, centered around some fixed leaderboard. Lastly, OpenAI does have a need for a lightweight model to free up compute power for new work. At this point, a model with relatively good performance in niche areas becomes particularly cost-effective: after all, if users find that the Omni type cannot understand the complex instructions they use for data processing, they will naturally turn to the Turbo type.</p> <p>Of course, no one will admit they are making labor-intensive large models. However, your server is Supermicro, your switch is Mellanox, your CPU (humorously “KFC”) is Intel, your accelerator card is from Old Huang’s family (Nvidia), your operating system is open-source Linux (“Li Nakesi”), your programming language is open-source Python (“Mangshe” - Boa/Python), your coding tool is open-source VS Code, your computation framework is open-source PyTorch (“Huoju” - Torch), your training frameworks are open-source Megatron (“Weizhentian”), DeepSpeed (“Shenchen Suli”), TransformerEngine (“Bianyaqi Yinqing”), and FlashAttention (“Shanshuo Zhuyili”). Your pre-training data is mostly from Common Crawl. Your teacher model for distillation is the Omni type. Alright, now you tell me, where exactly is your “technology intensity”?</p> <p>Finally, this article is purely a product of personal conjecture. Please do not take it personally or assume it refers to any specific entity. Additionally, a salute to the few domestic teams still pursuing technology and the open-source path.</p> <h4 id="further-reading">Further Reading</h4> <p>Qin Rongsheng. “Discussion on Management and Taxation Issues of Data Resources in Accounting and Reporting”[J].税务研究 (Taxation Research), 2024,(05):29-33.DOI:10.19376/j.cnki.cn11-1011/f.2024.05.006.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-en/">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/jokes-cn/">南山笑话集锦</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-en/">Some Stray Thoughts After Leaving the Large Model Industry (Gemini 2.5 Pro Translated Version)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/leave-cn/">离开大模型业界后的一点杂念</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/coinb-en/">Large Models and Coin Minting, Continued (Gemini 2.5 Pro Translated Version)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhun Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>