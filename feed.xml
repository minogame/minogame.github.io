<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://minogame.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minogame.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-21T04:13:31+00:00</updated><id>https://minogame.github.io/feed.xml</id><title type="html">Zhun Sun</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">视觉大模型一无是处</title><link href="https://minogame.github.io/blog/2023/vision-cn/" rel="alternate" type="text/html" title="视觉大模型一无是处"/><published>2023-06-29T16:40:16+00:00</published><updated>2023-06-29T16:40:16+00:00</updated><id>https://minogame.github.io/blog/2023/vision-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/vision-cn/"><![CDATA[<p>最近各种机缘巧合，反复被人问到一个让我非常反感的问题：你的视觉模型有多少参数？虽然Ph.D的职业训练能让我的表情依旧处在1940年欧洲版图那种毫无波兰的状态，但是刨根问底地讲，这个问题所诱发的，对meaninglessitude的观念已经让我形成了一种理性上的反感。</p> <p>一、22B与175B的差距差不多是175B</p> <p>首先我还是不否定，随着视觉模型参数量的提升，其在传统的视觉任务上的绝对数值表现也有相应的提升。近期出现的最好的一个例子就是Google的ViT-22B，虽然用了少量的没有开源的数据以及<del>保守的</del>炼丹技巧才让这个模型跑起来。但是这不妨碍圈内大体保持一个乐观的态度，认为计算机视觉依旧是配得上大模型的。然而在我看来，这个存在感薄的宛如arxiv主页一般的视觉最大往生模型，恰巧完成了“堆积参数与数据量对于纯视觉这个领域毫无意义”的证明。</p> <p>稍微思考一下，这个模型的训练量实际上达到了(JFT-)4B * 256token/img * 3epoch约等于3T的tokens，这已经比LLaMa最大的模型的训练量还要大一倍。但是ViT-22B上并没有什么真的有意义的结论，除了在经典旧世中的数据集上又多涨了三五斗，也就是这个模型更偏向于sketch而不是texture这些泛空的内容。拿着这个ROI堪比春夏季大反攻的结论，视觉还会有接下来的一个大模型吗，我看是不会有了。然而别忘了，22B与175B的差距差不多是175B。</p> <p>二、视觉模型再大也依旧愚钝</p> <p>在进入下一段的形上学内容之前，我先从经验主义的角度出发，复述三个“现代计算机视觉头顶的乌云”。当然，这些问题并不是没有得到重视，因为每年都持续的会有海量相关的文章这些问题上吃SR——这也确实印证了它们是难以被解决的。</p> <p>首先要拿出来讲的自然是陈词滥调的adversarial attack问题。平心而论虽然NLP领域也有这个问题，但是当NLP迈入(generative)LLM时代之后这个问题也几乎不再被重视了。反观CV领域，这个问题就像是怨魂一样纠缠着每一个倒霉的审稿人，因为没有人站出来写这样一篇文章，说给他们一个大模型他们就能终结这个领域。而且甚至于现在都没有人愿意去测试这些视觉大模型的对抗攻击问题，因为人们已经普遍接受了这么一个理念，就是视觉模型就应该被攻击。</p> <p>其次则是房间里的大象的问题，于2018年被提出来用以质疑detection模型与算法的有效性。大意是说，即便我将一只大象p到一个普通房间模样的照片中，我们充满智慧的模型也可以IOU&gt;0.9地将它检测出来，即便这从经验之理中来看是荒谬的。这其实是个充满哲理而且可能影响到CV根基的问题，只不过解决它与否并不影响一个算法在coco上的性能（或者说只会有负影响），所以人们也只管当它是房间里的大象。</p> <p>最后第三朵乌云就更现实一点，我愿意称之为样本空间的无序化问题。换言之，视觉信息在样本的特征空间中无法形成体系性结构。这里，视觉信息就是指图片中存在的可以抽象出的语义，体系性结构指的则是这些语义相互之间的关系。即便现在先进的自监督训练技术，可以很好的让某一类事物在“不需要知晓其语义的”情况下抱团取暖（体现为t-SNE的良好可区分性），但是从语义的角度来看，航天飞机的一个很小的r-neighbourhood内既可以有短毛橘猫也可以有果酱面包。往好的讲，这提高了基于视觉特征的创造力，往不好的讲，这倒逼视觉的研究者只能不断去寻求基于提高（特定benchmark的性能）力来创造视觉特征。</p> <p>三、NLP的知识与CV的知识</p> <p>首先我们需要确认两个概念，因为这两个概念的精准定义是一个长久以来都纠缠不清的问题，所以这里我们采取ChatGPT提供的通识性定义：</p> <p>Representation（表象，也作表征）：在心灵哲学和认识论中，”representation”指的是一种心智状态或实在，用以指代某一件事物。表象的定义中蕴涵了我们的心灵具备引用或表达存在于世界中的物体、思想或情况的能力。表象可以采取多种形式，包括心理图像、信念、思想或语言符号，可以被看作是心灵与外部世界之间的中介，使我们能够对世界有知识、感知和理解。</p> <p>Concept（概念）：另一方面在认识论中，”concept”是一个抽象或一般性的心理表象，包括一种或一类的事物、事件或观念。概念是思维和语言的基本构建单元，使我们能够对经验和知识进行分类和组织。对照单一的表象，概念的范围更为抽象和概括。概念通过抽象和概括的过程形成，我们在多个实例中识别共同的特征或属性，并创建一个心理范畴来代表这些共享的特征。</p> <p>武断地讲，“表象”和“概念”都涉及到表达和传达某些信息或意义，但它们的本质和来源是不同的。表象更多的可以被看作是一种心理层面的现象，而概念则可以被认为是一种思维层面的产物。</p> <p>事实上我认为搞明白这两个概念了之后，在认知层面上NLP与CV模型所获得的知识便很容易区分理解。我认为在语言（特别是generative）模型的框架内，我们为了训练一个模型，所提供的数据实质上展现的是Relations of Concepts（这里借用了Hume的理念，但是区别于纯粹的演绎，语言模型中的Relations依旧是通过归纳获得）。在此之上，对于某个Concept本身的理解，语言模型也是通过其与其他的Concepts之间的关联来获得一个间接的认知，而并没有一个直接的类似于人类心灵上的直接理解（特别是诸如“时间”等概念）。换而言之，语言模型所学习到的，是诸多由Concepts之间的关联所组成的Representations，以及由这些表象之间的关联所呈现出来更深刻更复杂的表象，而人类（语言）世界中所存在的或者所能被理解的表象多若繁星，我们需要万亿的参数去记忆这些表象，以及理解这些表象之间的关联性（或者说一个由表象为量子的N-gram）。</p> <p>反观视觉模型，在不论有监督或者自/无监督的（几乎所有的）训练框架内，其优化目标实质上是将特定的具体的数据，转化为特定的唯一的Representation，并经由特定的视觉任务（即人为的经验）最终将这些表象抽象为Concept，换言之是一个学习Matter of Facts的过程。这里再额外解释一下为什么需要经由特定的视觉任务，有过（自监督）视觉大模型训练经验的人大多会有这样一个认知，就是视觉模型的效果很难去做evaluatation，大家只是从k-NN/Linear Probe/Attentive Probe/Finetune（分类或者其他下游任务）中选择性能好的来写论文，但实际上这里的本质原因是，在没有人为的经验来帮助视觉模型去做抽象的时候，一个算法不会去主动的来做这样的抽象（它没有Loss也没有义务去做），它只需要将数据与表象去做对应即可。反过来讲如果在算法的设计上就已经考虑到了抽象Concept的路径，那么这个模型在Linear Probe的性能自然会好一些（例如杨立昆君的I-JEPA，下面会再讲到）。</p> <p>四、参数量解决不了抽象概念的问题</p> <p>现在再回到那该死的参数量上，对于现阶段的视觉任务，其对应模型的参数量恐怕并不能成为让「视觉大模型出现chatGPT时刻」的主要指标。再回到现代计算机视觉头顶的那三朵乌云：</p> <p>参数量只能在概念归纳的强度上获得边际的收益。一般来讲，视觉模型在经由表象来归纳概念时，脱离不了一个大致的聚类的概念。换句话说，我们预先会有一个“同一概念的表象会聚集在一起”的先验知识，例如Gaussian Prior，我们再经由实际的表象的分布来获得后验的结果。但是通过增加参数量来更精准的表达表象在向量空间的位置，其收益注定会逐渐边际化，因为参数量并不会与误差有一个线性的关系。以至于在某一个时刻开始，在有限的样本数量下，误差更多来源于先验的分布，最终提升参数量只能表现为ImageNet上那0.1%的性能提升。除此之外，增加参数量所引发的副效果，更高的向量空间的维度，也会增加归纳一个概念时所需的表象的数量（维度诅咒），从而也就会为对抗样本提供了更多的存在的空间。</p> <p>参数量并不能脱离归纳来知晓表象或概念之间的存在逻辑。从道理上来讲，神经网络可以Universal Approaximation，而且这个能力会随着参数量的增加而变强。事实上以我个人的经验，大体上来说一个参数量更多的模型，是可以用单一的特征向量来表达某一个相对复杂的场景的，例如一个盘子里面装着各种不同的水果摆件，只不过我们无法提供高质量的人为的经验去概念化此类东西，在这个例子里面通常来说我们只能提供诸如“静物”“果盘”或者“一个盘子里面装着各种不同的水果摆件”之类的概念或其组合。只不过这种概念对于实际上模型所能表达的表象来说过于的抽象，模型通常只能去概括的统计更初级表象的量来组合成此类高度复杂的表象。所以如果缺乏可以枚举“与”“或”“非”等关联的数据的引导，模型自然也不会去主动因为存在或者不存在某一个初级表象，就将一个复杂的表象去彻底归为不同的概念。例如，在缺乏数据的情况下，模型不太会因为一个“森林”的图片里面有一辆自行车，就把它归类为“公园”，或者因为缺少一条河流，就把它归类为“青山”。</p> <p>参数量也不能解决样本空间的无序化问题，从而无法让模型在脱离人为的经验的情况下，自发地学到由概念之间的关联而产生的新的概念。在自然语言当中，概念本身是有很好的层级化关联结构的，故而由概念或者表象的关联而形成的表象（例如一句话）也会因此获得一个结构性的表达，基于这种结构性的表达，我们会很容易继续创造新的概念或者表象。反观视觉中信息缺乏此类的层级化关联结构，视觉感官上相似的表象对应的概念可以完全不一样（例如字母l与数字1），所以在缺乏人为经验的情况下（自/无监督），在（表象的）特征空间里面也难以自发地形成结构性的表达，继而无法自发地形成新的概念。一个显著的例子就是，在跨模特的Align模型出现之前，几乎没有什么视觉算法可以做generalized zero-shot learning，即便是在某个特定的domain上（CUB鸟或者oxford花），zero-shot的性能也很难看。在大语言模型可以通过纯粹的语言概念解读画出独角兽的同时，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”<del>（除非毛子们能给它提供足够多的人为的经验）。</del></p> <p>五、Naive Cross-Modality Alignment并不解决根本问题</p> <p>有碰过Stable Diffusion的孩子们应该会注意到这样一个现象，就是绝大多数的prompt都是宛如咒语一般的独立的单词，而真正用自然语言描述的场景却很难被准确的作画。有直接碰过CLIP的孩子们也应该会注意到这样一个现象，绝大多数图文匹配的分数（无论正负样本）都分布在一个较为狭小的区间，而且难以采用一个直观的阈值来判断两者的匹配度。排除掉模型训练相关的原因，这凸显了两个问题：</p> <p>视觉端能精准学到的Concept的量是有限的，且更多的是简单的单词/短语级别的Concept。实际上在本人实际训练中文CLIP的时候，就遇到了学习到的top 100万个Concept里面里面有80万都是人名的状况（是的，再往后人名的比重会更高）。而这个其实是符合真实世界的分布，诸君可以现场站起来，用语言描述一下自己所能看见的物件，就会发现真实生活中接触到的相对简单的视觉Concept相当的贫乏，而更多的是身边的人的名字。</p> <p>视觉端并不是不可以学到一个相对复杂的表象，但是视觉端无法像文本端一样把这种复杂的表象归纳称为概念，这就导致了视觉文本两侧的匹配会出现偏差，而实际上训练数据中能供视觉端归纳出一个复杂表象所对应的概念的数据是有限的，也就是说，即便有语言的帮助，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”（除非毛子们能给它提供足够多的人为的经验）。</p> <p>所以，为什么Lock image Tuning现在会成为一个比较合理的CLIP训练模式，因为用语言将视觉端学到的表象归纳为概念这件事情的难度，远小于用视觉去理解语言中由多个抽象的概念构成的复杂表象。</p> <p>六、皈依 or not皈依语言模型</p> <p>最后讨论一下视觉模型往大做的出路在哪里。当然这里是要排除掉纯粹以学习表象为目的的模型的，例如围棋或者天气的模型，这些模型本身的能力本就展现在学习到人类无法用语言抽象的神秘的表象上，这些模型随着数据规模的增大，参数量自然需要scale-up来提升表象的能力（例如在99x99的围棋盘上，一个千亿参数的模型的性能理论上来讲必然好过一个百亿的）。</p> <p>现在的学术大致上寻找到了两个出路，其一便是让视觉模型彻底的皈依大的语言模型的能力（例如Google丢出来的PaLM-E以及今年的CVPR最佳论文Visual Programming），让视觉模型回归一个为概念提供视觉表象的功能，让大语言模型来完成更为复杂的概念的解读。这个时候视觉模型的参数量即可以用来学习一些人类无法用语言抽象的表象上（例如depth map，optical flow，hyperspectral singal），从而补齐语言模型在空间推理上的一些弱点，让其更加的接近真实世界的AGI。但是这种模式依旧依赖于毛子去打坦克搜集大量的数据用以提供人为的经验，未来的路就仿佛路易十六39岁时的样子——一眼望不到头。</p> <p>另外一个思路则体现在杨立昆君的I-JEPA以及feifei大佬的SiamMAE里面，我们强行的去让视觉模型去理解表象之间的关联，而这种任务本身对有注意力机制的ViT模型来说并不是特别困难。这样做最大的优点在于，它可以部分地解决前述的第二三朵乌云，但是这些方案由于突出强调个体表象，就比较难以学习到基于多种表象与概念而形成的复杂表象，特别体现在ImageNet的Finetune效果会比较差（ImageNet有一些相对复杂的场景类别）。而事实上，当下视觉的学术圈不怎么容得下一个性能不怎么好的算法，任何一点开放的探索都会被操着一口Chinglish的审稿人百般刁难，新的算法仿佛被困在了一个无尽的黑洞中难以诞生。</p> <p>当然如果你们关心我是怎么回答“你的视觉模型有多少参数？”这个问题的话，我一般是会冷漠地讲“ViT-B，88M，再大的模型他们不给上线了”。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年06月29日 21:15 · 广东]]></summary></entry><entry><title type="html">Vision Foundation Models Are Utterly Useless (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/vision-en/" rel="alternate" type="text/html" title="Vision Foundation Models Are Utterly Useless (DeepSeek Translated Version)"/><published>2023-06-29T16:40:16+00:00</published><updated>2023-06-29T16:40:16+00:00</updated><id>https://minogame.github.io/blog/2023/vision-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/vision-en/"><![CDATA[<p>Recently, due to various coincidences, I’ve been repeatedly asked a question that irritates me immensely: How many parameters does your vision model have? Although my Ph.D. training allows me to maintain an expression as unreadable as the 1940 European map (utterly devoid of Poland), digging deeper, this question evokes a conceptual meaninglessitude that has bred a rational aversion in me.</p> <p>I. The Gap Between 22B and 175B Is Roughly 175B First, I won’t deny that as the parameter count of vision models increases, their absolute performance on traditional vision tasks also improves. A recent prime example is Google’s ViT-22B, which, despite relying on a small amount of non-open-sourced data and <del>conservative</del> alchemy tricks to get running, hasn’t stopped the field from maintaining an optimistic outlook—that computer vision is still worthy of foundation models. However, in my view, this “largest vision model to date,” whose presence is as thin as arXiv’s homepage, ironically proves that scaling parameters and data volume is meaningless for pure vision tasks.</p> <p>A little reflection reveals that this model was trained on ~3T tokens (JFT-4B × 256 tokens/img × 3 epochs), already twice the training volume of the largest LLaMA model. Yet ViT-22B offers no truly meaningful conclusions beyond marginal gains on legacy datasets—essentially, it’s more sketch-oriented than texture-oriented. With an ROI comparable to a failed military campaign, will there be another “big vision model”? I doubt it. And let’s not forget: the gap between 22B and 175B is roughly 175B.</p> <p>II. Vision Models Remain Dull, No Matter Their Size Before diving into metaphysics, let’s empirically recap three “dark clouds looming over modern computer vision.” These issues aren’t ignored—they attract endless papers yearly, all failing to solve them (which ironically confirms their intractability).</p> <p>Adversarial Attacks: While NLP also faces this, the advent of (generative) LLMs has largely sidelined the issue. In CV, however, it haunts every unfortunate reviewer like a vengeful ghost, because no one has dared to claim that a foundation model can end this problem. Worse, nobody even bothers testing vision foundation models for adversarial robustness anymore—it’s now accepted that vision models are meant to be broken.</p> <p>The Elephant in the Room: Proposed in 2018 to question detection models’ validity, this problem highlights how even if an elephant is photoshopped into an ordinary room, our “brilliant” models can detect it with IoU &gt; 0.9—despite being empirically absurd. It’s a philosophical issue threatening CV’s foundations, but since solving it won’t boost COCO metrics (or might hurt them), everyone just treats it like the elephant in the room.</p> <p>Disorder in Sample Space: Visual information lacks systematic structure in feature space. Even state-of-the-art self-supervised learning can cluster objects without understanding their semantics (evidenced by clean t-SNE separability), but semantically, a tiny r-neighborhood around a space shuttle might include orange tabbies or jam sandwiches. Optimistically, this boosts creativity; pessimistically, it forces researchers to chase benchmark gains rather than meaningful features.</p> <p>III. Knowledge in NLP vs. Knowledge in CV First, let’s define two terms (long-debated, so we’ll use ChatGPT’s colloquial definitions):</p> <p>Representation: In philosophy of mind/epistemology, a mental state that stands for something else. It implies our minds can refer to objects, ideas, or situations, acting as intermediaries between mind and world (e.g., mental images, beliefs, language symbols).</p> <p>Concept: An abstract/general mental representation of a category (objects, events, ideas). Concepts organize thought/language via abstraction/generalization.</p> <p>Crudely, representations are psychological phenomena, while concepts are cognitive constructs.</p> <p>With these clarified, we can distinguish the knowledge acquired by NLP and CV models:</p> <p>NLP (Generative Models): Training data essentially presents Relations of Concepts (à la Hume, but inductively learned). Language models understand concepts indirectly via their relations—not through direct human-like comprehension (e.g., of “time”). Thus, they learn representations woven from concept relations, plus deeper layers of such representations. The human (linguistic) world’s representations are countless, requiring trillions of parameters to memorize and relate them (a quantum N-gram of representations).</p> <p>CV (Supervised/Self-Supervised): The optimization goal is to map concrete data to specific representations, then—via manual task design—abstract these into concepts (a Matter of Facts process). Why “manual”? Because without human guidance (e.g., linear probing), models won’t自发抽象; they’ll just align data to representations. Algorithms like I-JEPA (discussed later) bake in abstraction paths, hence their better linear probe performance.</p> <p>IV. Parameters Can’t Fix Abstract Concepts Back to the damned parameter count: for current vision tasks, scaling up won’t trigger a “ChatGPT moment.” Revisiting the three dark clouds:</p> <p>Diminishing Returns on Concept Induction: More parameters marginally improve vector-space precision for clustering representations (e.g., under Gaussian priors). But with finite samples, error soon stems from prior distributions, leaving gains as meager as +0.1% on ImageNet. Worse, higher dimensions (from scaling) exacerbate the curse of dimensionality, demanding more samples per concept and expanding adversarial attack surfaces.</p> <p>No Logic Without Induction: Neural nets are universal approximators, and capacity grows with parameters. Empirically, larger models can encode complex scenes (e.g., a fruit platter) into single feature vectors—but we lack high-quality human labels to conceptualize them. At best, we supply coarse tags (“still life,” “fruit bowl”), too abstract for the model’s granular representations. Without data teaching “AND/OR/NOT” relations, models won’t infer that a “forest + bicycle” should be “park,” or “forest − river” equals “barren hill.”</p> <p>Disorder Persists: Visual data lacks language’s hierarchical concept relations, so models can’t自发 form structural representations or derive new concepts. For instance, before aligned cross-modal models, zero-shot learning was dismal even in narrow domains (CUB birds/Oxford flowers). While LLMs can draw unicorns from pure text, a vision model may never recognize “a destroyed Leopard 2 tank”<del>—unless Russians supply enough labeled data</del>.</p> <p>V. Naive Cross-Modality Alignment Isn’t the Cure Users of Stable Diffusion notice that prompts work best as咒语-like word lists, not natural sentences. CLIP users see that image-text scores cluster narrowly, lacking clear thresholds. Beyond training quirks, this reveals:</p> <p>Visual Concepts Are Sparse: Top learned concepts are often simple words/phrases. When I trained Chinese CLIP, 80% of the top 1M concepts were names—reflecting real-world scarcity of describable visual concepts versus proper nouns.</p> <p>Complex Representations ≠ Concepts: Vision models can learn complex representations but can’t abstract them into concepts like text can, causing misalignment. Even with language, few training examples teach, say, “Leopard 2 tank wreckage.” Hence, Locked Image Tuning makes sense: using text to abstract visual representations is easier than vice versa.</p> <p>VI. Surrender to LLMs—Or Not? Finally, where do scaled-up vision models go? (Excluding pure representation learners like Go/weather models, where scaling does help.)</p> <p>Full Surrender (PaLM-E, Visual Programming): Let vision models serve LLMs by feeding them visual representations (depth maps, optical flow) to patch spatial reasoning gaps. But this relies on endless human-labeled data—like Louis XVI at 39, the road ahead is interminable.</p> <p>Force-Fed Relations (I-JEPA, SiamMAE): Make ViTs model inter-representation relations. This partly solves the 2nd/3rd dark clouds but struggles with complex scenes (e.g., poor ImageNet fine-tuning). Sadly, the academic gatekeepers—Chinglish-speaking reviewers—torture any underperforming idea, trapping innovation in a black hole.</p> <p>P.S. How do I answer “How many parameters?” Coldly: “ViT-B, 88M. They won’t deploy bigger ones.”</p>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · June 29, 2023, 21:15 · Guangdong]]></summary></entry><entry><title type="html">尝试为大家消除一些LLM带来的焦虑感</title><link href="https://minogame.github.io/blog/2023/anxiety-cn/" rel="alternate" type="text/html" title="尝试为大家消除一些LLM带来的焦虑感"/><published>2023-04-24T21:04:00+00:00</published><updated>2023-04-24T21:04:00+00:00</updated><id>https://minogame.github.io/blog/2023/anxiety-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/anxiety-cn/"><![CDATA[<p>众所周知，LLM目前已经造成了极大的恐慌，反思潮与焦虑感。这几种状况，突出表现在无论是否依靠传播作为主业的（自）媒体，开始铺天盖地地传播ChatGPT的新闻/LLM的教程/业界的动态等等事物。与此同时，大致几百家国内的大中小法人实体（包括还没注册的），开始冒头把自己的赛道改换为对话式LLM或者将自己对标OpenAI。再与此同时，又有某直辖市行政组织发表了《人工智能产业发展白皮书》，提出了“支持头部企业打造对标ChatGPT的大模型” 的言论。当然最后最糟糕的，其实是那封Pause Giant AI Experiments的公开信，充分地传达出一种“你们不带我上车，那我干脆把路给你拆了”的怨气。</p> <p>事实上，正如文章的title，我只愿意（也只能）努力帮大家消除一些焦虑感，而至于恐慌与反思，反倒是乐于看到的。或者说，我有充足理由来说明恐慌与反思是理性人应当做的，但是焦虑却不是。之所以我们认为LLM，或者更准确地说，OpenAI的GPT系列，给我们带来了如此巨大的恐慌，并且需要我们进行颠覆性的反思，主要事实大致有三：</p> <p>OpenAI实现了原本被（特别是国内科技公司）认为不具有实用价值的超大模型的直接部署上线，即便是调用一次赔一次；</p> <p>OpenAI做到了无比纯粹的劳务剥削，并以此获得了不可估量的训练数据，而且通过先发制人的（且调用一次赔一次）策略获得了更加不可估量的用户反馈数据；</p> <p>OpenAI证明了只要东西做得足够好，那么它就可以足够烂（开源数据集，或者上述的I与II），而不需要有一个特定的产品方法论。</p> <p>当然这个事实列表想列的话可以列上百条，这就仿佛沙漠风暴行动打醒了某个东亚大型行政组织一样。但是这不是今天的主题，所以但凡读者们想友善地发表意见，就请随意去友善。话锋转回这次的主题，我将会分三点来讨论为什么焦虑感是没有必要的（或者我不能为大家提供焦虑感是必要的论证）。</p> <p>一、无论你焦虑与否，你现在都没有什么办法解决它</p> <p>于个人来说，朋友圈中盛传了某大佬的在LLM时代你的Dos跟Donts，具体不再冗述，我对其评价为消极想法中的积极思考，也就是说，到底你也做不了什么，做好你自己的事情才是对的。</p> <p>当然，我更想讲的其实是组织层面上的事情。于个人即便你再去做好一些自己的事情，但是你如果真的有那个心气去解决它，那还是不得不去寻找一个组织来实现。不幸的是，我当下对国内的组织有着极端悲观的看法，换句话说，大家看似很卷，但是到头来恐怕也没人能做出什么真正让西方望其项背的东西。就我的理解，国内的组织想要在目前已经大幅落后的情况下，基本只是做出能与GPT比肩的东西，就只有两个模式可以选，而且这两个模式不是艳红（劣币驱逐良币）模式或者慧文（瓦岗军）模式，而是MBS模式或者慈父模式。</p> <p>MBS（Mohammed bin Salman）模式就是指无止境的投入，硬件直接从买地盖核电站开始投入，软件上直接用钱把openAI的核心成员全部挖来，如果做不到那就用钱把国内的大佬们成批的挖过来，所有人都带着最厉害的博士都上一线直接干活儿，最后再盖一座数据工厂浮士德，全年无休无止的产生数据。然后以上所有的都直接乘三，按照曼哈顿计划做多分备份，举整个财阀之力去支撑起这个工程，几千万刀就不要看了，至少按照微软的100亿刀起步。不然呢？</p> <p>慈父（Ио́сиф Виссарио́нович Ста́лин）模式则对应的，如果你没有无穷无尽的预算，那么干脆找一群不问出身的年轻人，在西伯利亚（OK，罗布泊）的荒野盖起一座城，给这些年轻人解决生活上的一切问题，让他们自由的发挥，缓慢而踏实的培养一批有真才实干的人，剔除掉目前学术界的浮躁，争取在5～10年的时间内，从基础到工程形成一套完整知识体系，再去与西方一决雌雄。</p> <p>二、LLM本身可能也不是实现强AGI的可行途径</p> <p>我们在这里不再反复讨论什么是强人工智能，当然我还是忍不住扯一句，如果仅以“能像一般人类一样进行对话“作为标准，那GPT4何止强人工智能，简直是现世神了。你让它去写个促进国内某行政区划的大模型产业的白皮书，那效果怕是好过我们行政组织里面一般人类的很多倍。这里我的主要观点，在于语言本身的特性，按照层次可以用以下三段总结：</p> <p>作为真实世界的表象，语言本身具有模糊性，对同一语言所产生的思想与同一思想所产生的输出均不一样。这里举个简单的例子，比如你的孩子考试考了60分，你面色平静地讲到“考的真不错”，孩子听了后欢欣鼓舞问你可不可以奖励他氪两单，于是你奖励了他一顿羊驼狂奔。</p> <p>正因为模糊性，人生来就对语言的使用就具有偏见性，从而个体对语言所关联的思想才具有一致性，而通过大量数据所学习而来的LLM有可能失去这种偏见性。接着上面的例子，比如60分的时候我们认为这句话是在说反话，那么提高到多少分的时候才不是反话？这并没有一个确定的标准，而遵从人内心对世界的理解本身的偏见性。换句话说，一个人喜欢讲“考得真不错”，可能只是因为他就是喜欢用这句话来做讽刺，而与到底考了多少分没有任何关系。而大数据自然而然地，会抹杀这种偏见性，一个LLM可能会学到一个基于分数的正态分布，我们按照采样来决定这句话到底是鼓励还是嘲讽。</p> <p>一旦失去偏见性，那么LLM便不能基于某一个思想而创造新的知识或者概念，这便脱离了强人工智能的定义。换句话说，基于语言的新知识或者新概念，实质上是一种旧知识或者旧概念与偏见性结合出来的产物。就好像“翔”这个字在几年前还单纯的是一个翔的含义，而基于了某个个体的语言偏见性，以及这种偏见性本身带来的戏剧效果，才导致出现并传播了当下这个意思。</p> <p>三、焦虑本身是即是你之存在</p> <p>焦虑本身并非一个具有负面意义的概念，人会否定焦虑之价值更多原因只是世俗性的偏见。从生理角度来讲，焦虑是一种对预期的不确定性（通常是对可能出现的负面结果）所导致的情绪上的动荡与不愉快的感觉，是一种作为人类的极端的正常的反应。区分于焦虑所形成的disorder，适度的焦虑是推动人类自我进步的一个条件。所以不妨干了这碗焦虑鸡汤，通宵再战它一年的NaN。</p> <p>从海德格尔的存在主义哲学层面来讲，我认为LLM带给我的，与其称之为焦虑，不如说它就是一种具象化的Angst（畏），即是此在的存在自身。虽然LLM带给我的不是无法回避的死亡（当然LLM确实带给很多人了社会性死亡），而是一种对智慧的源生的追求，如果我没有走在探寻智慧的路上，那其实我已经死了。</p> <p>以上，希望大家可以喜欢。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年04月24日 21:04 · 广东]]></summary></entry><entry><title type="html">Attempting to Alleviate Some of the Anxiety Brought by LLMs (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/anxiety-en/" rel="alternate" type="text/html" title="Attempting to Alleviate Some of the Anxiety Brought by LLMs (DeepSeek Translated Version)"/><published>2023-04-24T21:04:00+00:00</published><updated>2023-04-24T21:04:00+00:00</updated><id>https://minogame.github.io/blog/2023/anxiety-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/anxiety-en/"><![CDATA[<p>As we all know, LLMs (Large Language Models) have currently caused immense panic, a wave of reflection, and a sense of anxiety. These phenomena are prominently manifested in the overwhelming spread of news about ChatGPT, tutorials on LLMs, and industry trends by (self-)media outlets, regardless of whether their primary focus is communication or not. At the same time, roughly hundreds of large, medium, and small legal entities in China (including those not yet registered) have begun to pivot their focus toward conversational LLMs or position themselves as competitors to OpenAI. Meanwhile, a municipal administrative organization released the White Paper on the Development of the Artificial Intelligence Industry, proposing to “support leading enterprises in building large models that rival ChatGPT.” Of course, the worst of all is the open letter titled Pause Giant AI Experiments, which fully conveys a sense of resentment: “If you won’t let me on board, I’ll just tear the road apart.”</p> <p>In fact, as the title of this article suggests, I am only willing (and only able) to try to alleviate some of the anxiety, while I am actually pleased to see the panic and reflection. Or rather, I have ample reasons to argue that panic and reflection are rational responses, whereas anxiety is not. The reasons why LLMs—or more accurately, OpenAI’s GPT series—have brought us such immense panic and necessitated disruptive reflection can be summarized in three main points:</p> <ol> <li> <p>OpenAI successfully deployed ultra-large models that were previously considered impractical (especially by domestic tech companies), even if each invocation incurs a loss.</p> </li> <li> <p>OpenAI achieved an incredibly pure form of labor exploitation, amassing an immeasurable amount of training data, and through a preemptive (and loss-incurring) strategy, obtained even more invaluable user feedback data.</p> </li> <li> <p>OpenAI proved that if something is made well enough, it can afford to be “bad” (e.g., open-source datasets, or the aforementioned points I and II), without needing a specific product methodology.</p> </li> </ol> <p>Of course, this list could easily extend to hundreds of points, much like how Operation Desert Storm shocked a certain large East Asian administrative entity. But that’s not today’s topic, so readers are free to share their thoughts in a friendly manner. Returning to the main theme, I will discuss in three parts why anxiety is unnecessary (or why I cannot provide arguments for its necessity).</p> <ol> <li>Whether You’re Anxious or Not, There’s Little You Can Do About It Right Now On a personal level, a certain influencer’s Dos and Don’ts in the Era of LLMs has been widely circulated on social media. I won’t elaborate further, but my take is that it represents positive thinking within a negative framework—ultimately, there’s little you can do, so focusing on your own work is the right approach.</li> </ol> <p>What I really want to talk about, however, is the organizational level. Even if you do your best as an individual, if you genuinely aspire to address this issue, you’ll still need to rely on an organization. Unfortunately, I hold an extremely pessimistic view of domestic organizations at the moment. In other words, while everyone seems to be working hard, it’s unlikely that anyone will produce something truly comparable to Western achievements. In my understanding, for domestic organizations to catch up—let alone rival GPT—there are only two viable models, neither of which is the “Yanhong model” (bad money driving out good) or the “Huiwen model” (rebellious upstarts), but rather the MBS model or the Stalin model.</p> <p>The MBS (Mohammed bin Salman) model entails endless investment: starting with buying land and building nuclear power plants for hardware, and for software, directly poaching OpenAI’s core members with money. If that fails, then poach batches of domestic experts, with everyone—including the top Ph.D. holders—working on the front lines. Finally, build a “data factory Faust” that operates nonstop year-round to generate data. Then multiply all of the above by three, creating multiple backups à la the Manhattan Project, and mobilize the entire conglomerate to support this endeavor. Don’t even look at mere millions—start with Microsoft’s $10 billion benchmark. Otherwise, what’s the alternative?</p> <p>The Stalin (Ио́сиф Виссарио́нович Ста́лин) model, on the other hand, is for those without infinite budgets. Gather a group of young talents regardless of their background, build a city in the wilderness of Siberia (fine, Lop Nur), solve all their living needs, and let them work freely. Slowly but steadily cultivate a group of truly capable individuals, eliminate the浮躁 (frivolity) of the current academic world, and strive to develop a complete knowledge system from fundamentals to engineering within 5–10 years, before competing with the West.</p> <ol> <li>LLMs Themselves May Not Be a Viable Path to Strong AGI We won’t rehash the definition of strong AI here, though I can’t resist noting that if “engaging in conversation like an average human” is the standard, then GPT-4 isn’t just strong AI—it’s practically a deity. Ask it to draft a white paper on promoting the large-model industry in a certain Chinese administrative region, and the result would likely outperform many humans in our administrative bodies. My main argument here revolves around the nature of language, summarized in three layers:</li> </ol> <p>As a representation of the real world, language is inherently ambiguous. The same language can evoke different thoughts, and the same thought can produce different outputs. For example, if your child scores 60 on an exam and you calmly say, “Great job,” the child might cheerfully ask for in-game purchases as a reward—only to be met with a verbal lashing.</p> <p>Due to this ambiguity, humans are born with biases in language use, which ensures individual consistency in associating language with thought. LLMs trained on massive data may lose this bias. Continuing the example: if we interpret “Great job” as sarcasm at 60 points, at what score does it cease to be sarcastic? There’s no fixed standard—it depends on personal biases toward the world. In other words, someone who often says “Great job” might just enjoy using it sarcastically, regardless of the actual score. Big data naturally erases such biases; an LLM might learn a normal distribution based on scores and sample whether the phrase is encouragement or sarcasm.</p> <p>Without bias, an LLM cannot create new knowledge or concepts based on a given thought, which deviates from the definition of strong AI. New linguistic knowledge or concepts are essentially products of old knowledge or concepts combined with bias. For instance, the character “翔” (xiáng) originally meant “to soar,” but due to an individual’s linguistic bias and its comedic effect, it acquired its current slang meaning.</p> <ol> <li>Anxiety Is the Very Proof of Your Existence Anxiety itself is not a negative concept; its devaluation stems mostly from世俗 (secular) biases. Physiologically, anxiety is an emotional turbulence and unpleasant feeling caused by uncertainty about expected outcomes (often negative ones), a perfectly normal human reaction. Unlike anxiety disorders, moderate anxiety is a driver of self-improvement. So why not drink this bowl of anxiety soup and pull an all-nighter battling NaNs for another year?</li> </ol> <p>From Heidegger’s existentialist perspective, what LLMs bring me is less “anxiety” and more a concrete manifestation of Angst (dread)—the very being of Dasein (existence). While LLMs haven’t brought me unavoidable death (though they have socially “killed” many), they evoke a primal pursuit of wisdom. If I’m not on the path to seeking wisdom, then I’m already dead.</p> <p>That’s all. Hope you enjoy it.</p>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · April 24, 2023, 21:04 · Guangdong]]></summary></entry><entry><title type="html">中国学术机构正在以输出垃圾审稿意见的方式污染国际学术圈</title><link href="https://minogame.github.io/blog/2023/review-cn/" rel="alternate" type="text/html" title="中国学术机构正在以输出垃圾审稿意见的方式污染国际学术圈"/><published>2023-03-20T01:13:00+00:00</published><updated>2023-03-20T01:13:00+00:00</updated><id>https://minogame.github.io/blog/2023/review-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/review-cn/"><![CDATA[<p>就按照最简单四象限模式，审稿意见可以分为四种：</p> <p>正向-有价值，负向-有价值，正向-无价值，负向-无价值。</p> <p>通常来说，按照文章质量跟学会质量，正向/负向以及有价值/无价值的比例会略有不同，但是亘古不变的是，绝大多数负向且无价值的审稿意见都操着一口纯粹的Chinglish。</p> <p>这类审稿意见通篇只表达了以下观点：</p> <p>我没认真读你的论文，而且我是拖到审稿deadline最后一天才随便看了一下的； 我其实也没看懂你在说什么，看你的Table 1效果好像不行啊； 我不太擅长英语，更不太会使用键盘，打了3句话意思一下可以了； 我好像也没有熟人给我打招呼这是他的文章，那就reject了，科科。</p> <p>古时，这种审稿意见量还不大，一个脑子正常且负责任的AC还能分辨文章的质量，给一个客观合理的判断。而如今，随着大量中国学术机构炮制的流水线博士与垃圾论文逐渐充斥学术圈，这类审稿意见的量逐渐增大，甚至于50%以上的意见都是这种Not even not valuable。更可怕的是，脑子正常且负责任的AC的量也在变少，操着一口Chinglish满口胡说八道的AC量在急剧增加。</p> <p>数年前，ICLR大胆的采用了openreview的方法来提高双盲的体验，然而open review但不open reviewer的实践，更加的让Chinglish老哥们肆无忌惮。open了劳资都无所畏惧，冲马桶(cmt)那就更加只配吃翔。</p> <p>一方水土养一方人，此方水土注定养活不了GPT级别的创新，只能一言以蔽之罢。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_1.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_2.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_3.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_3.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_4.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年03月20日 01:13 · 广东]]></summary></entry><entry><title type="html">Chinese Academic Institutions Are Polluting the International Academic Community with Low-Quality Peer Reviews (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/review-en/" rel="alternate" type="text/html" title="Chinese Academic Institutions Are Polluting the International Academic Community with Low-Quality Peer Reviews (DeepSeek Translated Version)"/><published>2023-03-20T01:13:00+00:00</published><updated>2023-03-20T01:13:00+00:00</updated><id>https://minogame.github.io/blog/2023/review-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/review-en/"><![CDATA[<p>Using the simplest four-quadrant model, peer review comments can be categorized into four types:</p> <p>Positive-valuable, Negative-valuable, Positive-worthless, Negative-worthless.</p> <p>Generally speaking, depending on the quality of the paper and the academic community, the ratio of positive/negative and valuable/worthless may vary slightly. However, one thing remains constant: the vast majority of negative and worthless review comments are written in pure Chinglish.</p> <p>These types of reviews convey nothing but the following messages:</p> <p>I didn’t read your paper carefully—I just skimmed it last-minute before the review deadline.</p> <p>I didn’t really understand what you were saying, but your Table 1 results seem unimpressive.</p> <p>I’m not great at English, nor am I proficient with a keyboard, so three sentences should suffice.</p> <p>I don’t recognize any names as potential friends who might have submitted this, so let’s just reject it. Heh.</p> <p>In the past, the volume of such reviews was still manageable. A sane and responsible Area Chair (AC) could discern the paper’s actual quality and make an objective judgment. But now, as Chinese academic institutions mass-produce assembly-line PhDs and low-quality papers that flood the academic world, the number of such reviews has skyrocketed. More than 50% of the feedback now falls into the “Not even not valuable” category. Even worse, the number of rational and responsible ACs is dwindling, while those spouting nonsense in Chinglish is rapidly increasing.</p> <p>A few years ago, ICLR boldly adopted open peer review to improve the double-blind review experience. However, the practice of making reviews open—but not revealing reviewers—has only emboldened the Chinglish-speaking reviewers. If they don’t care even when their identities could be exposed, then anonymous conference management tools (CMT) are nothing more than a cesspool to them.</p> <p>Different environments breed different people. This particular environment is simply incapable of fostering GPT-level innovation. Enough said.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_1.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_2.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_3.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_3.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230320/640_4.webp" sizes="95vw"/> <img src="/assets/img/20230320/640_4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · March 20, 2023, 01:13 · Guangdong]]></summary></entry></feed>