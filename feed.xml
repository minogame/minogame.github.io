<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://minogame.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minogame.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-29T05:59:55+00:00</updated><id>https://minogame.github.io/feed.xml</id><title type="html">Zhun Sun</title><subtitle>Zhun Sun&apos;s personal page and blog storage. </subtitle><entry><title type="html"></title><link href="https://minogame.github.io/blog/2025/2024-01-19-cvpr-en/" rel="alternate" type="text/html" title=""/><published>2025-05-29T05:59:55+00:00</published><updated>2025-05-29T05:59:55+00:00</updated><id>https://minogame.github.io/blog/2025/2024-01-19-cvpr-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/2024-01-19-cvpr-en/"><![CDATA[<p><strong>1.24 update:</strong> Checked it – for the 5 papers I gave a 1, other reviews were also 1-2 points. For the other one I gave a 5, other reviews were also 4-5 points. So, can the “saints” (holier-than-thou critics) shut up now?</p> <p><strong>Clarification:</strong> Please don’t take things out of context. Deciding to give a reject in 10 minutes and spending only 10 minutes on the entire review are two different concepts. For a very mediocre paper, you know its type after reading the abstract and introduction. The next step is to research the most relevant papers and find material to write comments. In the end, I still have to spend several hours cumulatively writing comments for each paper.</p> <p>Also, why 5 rejects out of 6 papers? Because the acceptance rate for top conferences is typically in the 20-25% range. If you have 6 papers, you’ll definitely need to clearly reject 3-5. Isn’t that perfectly normal? If you give 1 reject, 1 accept, and 4 borderline, do you think the AC (Area Chair) wouldn’t want to kill someone?</p> <p>Then there are those getting PTSD because I gave 5 rejects, and those treating several review outcomes as independent random events? Firstly, I can confidently say that the other reviews for these papers were also bound to be negative (I even seriously wrote one or two strengths and suggestions for improvement for each). If, during the discussion phase, I found my scoring was erroneous, I would certainly change it. Secondly, I wrote this precisely because I couldn’t stand how poor the quality of papers was this time, to talk about how papers should actually be written.</p> <p>Furthermore, I received requests for 6 emergency reviews. After briefly reading the abstracts, they were all of mediocre quality. If I had accepted them all, out of 12 papers, I probably would have had to reject 9-10. I declined them all precisely because I didn’t want to torture myself (of course, the limit might be 8 papers; I didn’t dare to click).</p> <p>I want to emphasize one more thing: for a junior reviewer (generally, those who receive fewer than 8 papers, with some margin of error), your job is to provide timely, accurate, and clear review comments. Once you’ve done that, your work is already well and excellently completed. Don’t act with the heart of a Bodhisattva trying to save all sentient beings; such an idea only provides verbal gratification and has no practical meaning.</p> <p>I’m forced to say this again: if you’ve carefully read the guidelines, they generally require reviewers to express their opinions as clearly as possible during the review, unless you are genuinely unsure, and not to give borderline scores. This is why ICLR previously experimented with a 1-3-6-8 rating system, and why CVPR removed the “strong” qualifier this year – to reduce reviewers’ guilt when giving scores at either extreme, in other words, to encourage reviewers to express their opinions as clearly as possible. But you guys just love giving borderline scores, don’t you? The more borderline scores you give, the more random the AC’s meta-review becomes, the more people will engage in speculative submissions and achieve good speculative results, and the worse the conference’s quality will become.</p> <p>And I am indeed quite arrogant. Given the average quality of CVPR papers nowadays, if it takes you a whole day to read one paper, I’m afraid you might not be qualified to review for others, right? (/s, no offense intended to those not concerned)</p> <p><strong>Original Text Below:</strong></p> <p>In fact, I had refused to review for many years. The act of submitting/reviewing (especially in the field of deep learning) is essentially a religious behavior of debating a certain research subject based on scientific faith, something that should inherently have a high barrier to entry. However, gradually, when I found myself having to spend several hours on absurd material like “ResNet is the greatest neural network,” “mass data smelting will allow us to surpass the UK and the US,” or “fighting roosters is also an adversarial attack” (a nonsensical pun on “adversarial attack” vs. “fighting rooster,” which sound similar in Chinese), I naturally gave up participating in such activities.</p> <p>This time, however, I was spammed by the PC (Program Committee) with about five or six review invitation emails (“The quality of the conference strongly depends on the willingness of reviewers like you to provide thoughtful, high-quality reviews on time.” But they never invited me to be an AC, huh?). So, with a slightly curious mindset, I accepted, and was assigned manuscripts with IDs approaching 18000, which gave me a little “AI shock.” The reviewing experience was indeed worse than a few years ago. Out of the 6 papers I handled, 5 were given a 1-point Reject within 10 minutes (just after reading the abstract and introduction). Of course, some might question how I differ from the reviewers described on Zhihu (a Chinese Q&amp;A platform). Indeed, everyone’s definition of an excellent paper varies. Occasionally, someone like me who almost never looks at experimental performance is a godsend for some (and naturally a disaster for most). However, friends who enjoy achieving SOTA (state-of-the-art) on a Cantor set (i.e., on very niche or insignificant problems) can certainly try submitting a few more times; you’ll eventually encounter a reviewer unlike me.</p> <p>So, from this perspective, I want to discuss two issues: first, how have top conferences fallen into such a chaotic state, and second, what simple standards should we use to judge the quality of papers.</p> <h4 id="i-reasons-for-the-influx-of-disastrous-quality-manuscripts-in-top-conferences">I. Reasons for the Influx of Disastrous-Quality Manuscripts in Top Conferences</h4> <p>Applying economic concepts, the fundamental reason for this situation is that the academic market itself is not sufficiently effective. In other words, a researcher’s value cannot be quickly and accurately reflected by their various academic activities. Therefore, if a researcher hopes to achieve academic success, they have no choice but to rely on speculative submissions to top conferences that might yield excessive returns. Of course, designing a value assessment system for academic researchers is a grand topic beyond my capabilities (and no one would listen to me anyway). Here, I only wish to explore the reasons why the market is ineffective and why speculative behavior can succeed.</p> <p>Firstly, theoretically, even to form a weakly effective market, the value assessment of an academic researcher should be based on a full understanding of their public information (all publications, etc.). But the workload and standardization of judgment involved are beyond what ordinary organizations can accomplish. Thus, current academic evaluation systems adopt a path akin to “efficiency first, with fairness also considered,” for example, by judging quantifiable indicators like the frequency of first-authorship in top conferences and citation counts to quickly label a researcher’s value. Considering practical factors, especially in academic systems where professors also handle administrative duties, this approach itself is neutral. However, in an academic system with a large number of administrators, merely transplanting this path without any initiative appears quite निष्क्रिय (inactive/ineffective). Particularly for those who have obtained administrative “hats” (positions/status) through this paper-centric evaluation model, if they continue to evaluate the next generation of researchers using the same model, it will directly lead to the next generation in this market engaging in “involution” (内卷 - intense internal competition for diminishing returns) based on maximizing self-interest.</p> <p>To digress a bit, more than institutional reasons, the difference in the level of needs satisfied by academic activities (i.e., human nature) is probably the core factor for market failure. In my view, academic activities should at least start from satisfying cognitive needs (curiosity, desire for knowledge), and obtaining high-level “hats” should be about satisfying higher-level needs like aesthetics and self-actualization (values, morals). If an academic researcher is merely pursuing physiological or safety needs (job security, benefits), and the purpose of obtaining a “hat” is to satisfy specific esteem needs (achievement, fame, status), it will inevitably lead to a distortion of academic value judgment. However, I still maintain a relatively moderate attitude here: a researcher’s needs hierarchy largely depends on the external environment rather than personal factors. For example, our “Break the Five Onlys” special action (a Chinese academic reform initiative targeting overemphasis on papers, titles, diplomas, awards, and projects) is essentially about providing an external environment where researchers don’t have to worry about lower-level needs (I’m not evaluating its results here, but I personally think it has positive significance).</p> <p>On another level, a large number of relatively junior researchers are attempting such speculative behaviors (let me add a disclaimer here: although factors from a certain great Eastern country [China] are involved, the number of garbage manuscripts from another great Eastern country [likely India] has significantly increased in recent two years). My personal understanding is that the fundamental reason is the severe decline in the credibility of so-called top conferences. In other words, these researchers have lost their reverence for descriptors like “Top,” “Tier 1,” “S-class,” and have purely abstracted acceptance into a random probability event. This is manifested by the fact that even as a long-serving reviewer for these top conferences, I cannot determine if a particular submission from myself or my students might be accepted. I can’t even say for what reasons they might be rejected. And yet, I’m still f***ing reviewing other people’s papers! One can imagine how ridiculously random the current reviewing process is (let’s not even mention the top three AI/ML conferences; even UAI/AISTATS are approaching a similar state). In my ideal conception, a top conference should have enough credibility that low-quality manuscripts would realize their inability to be accepted and give up before even submitting, just as not everyone would attempt to submit to the four great mathematics journals. And when I find myself constantly cursing “what is this trash?” while looking at posters every year, I can’t find any reason not to make such speculative submissions myself.</p> <p>However, unlike the ineffectiveness in the academic market caused by human nature, I personally understand that the decline in the credibility of top conferences is due to institutional reasons. Excluding those who pursue Reviewer/AC roles as “hats” for utilitarian purposes (I don’t know if that excludes most people), a Reviewer/AC who “generates electricity with love” (works out of passion) should be doing this job primarily to make our environment better; otherwise, he/she could just refuse without any penalty. Specifically, there are two main problems: First, reviewer expertise is genuinely limited, and this limitation comes not only from insufficient experience (e.g., the bizarre precedent set by a certain ML top conference where submitters volunteer as reviewers) but also from the explosive growth in research fields and the number of papers within them. So much so that without spending several hours doing a survey, you might not even know where the paper you’re reviewing stands or what its real contributions are to the field (especially interdisciplinary fields). Second, the role of ACs in the current review process is quite weak (especially for those who treat the AC role as a “hat”). Considering an AC might have dozens of papers, they often have no choice but to make simple decisions based on the positive/negative direction of review scores. This, in turn, further exacerbates reviewers’ tendency to give borderline scores, meaning reviewers are unwilling to take responsibility for the AC’s simple positive/negative decisions and prefer to give a mediocre score while waiting for a more definitive review. Thus, a death spiral is formed: the review process, which should be about group discussion to reach a conclusion, becomes a personal show for some “crazy reviewer.” If they dislike a paper and give it a low score, the paper is almost certainly rejected, even if the reviewer is unqualified, their comments are childish and meaningless, or even if they copy-paste review comments from a submission to another conference (I’ve encountered this multiple times).</p> <p>Here, I will summarize some failed institutional attempts and offer a possible solution. First, the biggest failure is open review, because it essentially provides more information to an already ineffective academic value evaluation market, so it was doomed to be meaningless from the start. Judging by the results, open review not only failed to improve the quality of manuscripts and corresponding review comments (anecdotal evidence from my surroundings; feel free to correct me if you disagree) but also caused adverse effects like homogenization of review comments and “borrowing” of academic achievements. Of course, I’m not completely negating this solution; for high-quality papers, reading their reviews and rebuttals can help improve academic standards, but as a system for optimizing the review process itself, it’s a failure. Second is multi-round review or journalization. The intention was to enhance communication between authors and reviewers, but in execution, most reviewers don’t provide second-round opinions (especially for clear rejections), leaving authors shouting into the void with no response, severely degrading the submission experience (e.g., I will never submit to certain conferences/journals again). Additionally, there was an attempt at a “no rating” review model. However, the result was basically equivalent to a 3-tier system: accept / get lost / whatever. If ACs lack responsibility, it more easily leads to a one-vote veto situation. Besides, if I’m not going to be openly shamed by a reject, why not submit something “just for the heck of it”? What if it gets in?</p> <p>So, what institutional solutions might improve the manuscript quality at such top conferences? Although I haven’t managed a conference, based on my long-term “scarring” experience, I believe adopting an “Abstract-Invited Submission” process could partially solve this problem. This process has three steps: First, all authors wishing to submit should provide a short (CVPR double-column, 2-4 pages) abstract (abstracts allow dual submission). This abstract is then scored by general volunteer reviewers. Subsequently, ACs select a portion of abstracts with higher scores or those they personally deem valuable and send submission invitations to their authors. Finally, the full manuscripts submitted by invited authors (full manuscripts do not allow dual submission) are reviewed by teams led by senior reviewers, and a selection of the best is accepted. This process both exempts reviewers from writing formal reviews for poor manuscripts and gives invited authors a relatively clear expectation of acceptance. If any big shots happen to see this, perhaps they could test its effectiveness.</p> <h4 id="ii-measuring-article-value-from-an-epistemological-perspective">II. Measuring Article Value from an Epistemological Perspective</h4> <p>Next, I want to talk about those five poor sods whose papers were given a 1-point Reject within 10 minutes (of course, from another perspective, I might be the unlucky one, having to waste so much time providing unrewarded guidance to what are clearly “drafts for comment”). This begins with Kant’s interpretation of knowledge, because in my philosophy, if an academic research manuscript cannot provide knowledge, it should be rejected. According to my rudimentary understanding of Deng Xiaomang’s translation, Kant, in the “Critique of Pure Reason,” proposed a theoretical framework distinguishing between a priori (conceptual) and a posteriori (empirical) knowledge. The former refers to knowledge that exists before experience; it is about form and logical structure and does not depend on specific empirical content. The latter is based on sensory experience; it depends on our interaction with the external world (there are some translation issues here that can be referred to in the Q&amp;A below). Kant believed that although pure reason can produce necessary knowledge, such as logical and mathematical laws, this knowledge does not directly tell us facts about the physical world. Conversely, knowledge of the physical world all comes from experience, but the possibility of knowledge is determined by a priori conditions. Therefore, it requires the combination of sensory intuition input and rational concepts (making synthetic judgments).</p> <p>I know you might dislike this act of quoting convoluted philosophical scriptures, so to put it in plain language: firstly, analyses of single concepts (“XXX is the cause of problem YYY, so using method ZZZ can solve it”) or single experimental results (“We used XXX, so we achieved SOTA on YYY”) are insufficient for an article to produce necessary knowledge. Secondly, by performing some meaningless, intimidating mathematical derivations (“a theorem that requires ten assumptions to prove”) or seemingly complete but very hollow ablation studies (“we removed every component and ran an experiment once”), it is also difficult to integrate the views or assertions in an article into necessary knowledge (even though most people do this).</p> <p>I believe you will still find the above explanation abstruse, so next, I will directly provide some paper writing paradigms to discuss whether a certain type of article is truly meaningful in my eyes:</p> <ul> <li><strong>Based on theory T or experiment E, conclude A holds:</strong> This type of article might seem invalid at first glance, but in reality, the subject of such articles is usually a relatively new field. For example, the first paper proposing adversarial attacks. Providing “a new field” itself is a result of the joint action of reason and experience, so such articles are usually considered very valuable.</li> <li><strong>Based on theory T, conclude A holds, and simultaneously based on experiment E, conclude A’ holds:</strong> This type of article is common in the classic machine learning field and is a very solid way of argumentation. Although A’ is an approximation of A, it is sufficient to provide reasonable and reliable experience to form definite knowledge.</li> <li><strong>Based on experiment E, conclude A holds, and simultaneously based on theory T, conclude A’ holds:</strong> This writing paradigm is the erroneous path I mentioned earlier of using intimidating mathematics to improve article quality, often appearing in algorithm-focused deep learning articles in known fields. But in my view, if reason is needed to “complete” knowledge, it should participate in the process of deriving A from experiments; your theory should be an analysis of the experimental results. Analyzing an approximate A’ is a process separated from experience and does not in itself form knowledge. So, the strength of this article still depends on the strength of experiment E. Many people feel wronged when they “were doing experiments and writing theory and suddenly got rejected,” but in fact, it’s not unjust at all.</li> <li><strong>Given that based on experiment E, A holds; based on experiment E’, conclude A’ holds:</strong> Distinct from deriving A based on theory, if a similar experiment is used to verify a similar result, its strength’s upper limit depends on the inference from E to A, which will not be necessary knowledge (it might have flaws but be accepted). So, the premise of this writing paradigm itself is weak, and the lack of rational analysis makes it even more hollow.</li> <li><strong>Given that based on experiment E, A and B hold; based on experiment F, conclude A+B holds:</strong> This paradigm is our most common “Frankenstein monster” (patchwork) model. Its main problem lies in innate human thinking: if two things that hold true are stitched together, they will naturally hold true. Therefore, this experiment F is considered to offer no new experience for forming knowledge (meaning, “is it necessary to experiment on something that’s a no-brainer?”). But conversely, whether this logic is correct actually varies from person to person, so some people might also think that “A+B holds” does provide new knowledge, but its strength might not be very good, i.e., what’s commonly called a “filler paper.” Of course, I usually reject such articles decisively, so please rest assured.</li> <li><strong>Given that based on experiment E, A, B, C, D hold; based on experiment F, conclude A’+B+C’+D holds:</strong> This is a typical systematic application article. Although it might also seem like a filler paper, the interesting point is that finding the combination of A+B+C+D and replacing components within it is itself a manifestation of reason at work. Compared to simply A+B, finding a reasonable scheme to solve an application problem is a good manifestation of knowledge. However, whether it should appear in a top conference also depends on the strength of this application and its results.</li> <li><strong>Given that based on experiment E, A={A1, A2} holds; and simultaneously based on experiment E’, conclude A={A1} holds or does not hold:</strong> This paradigm is actually very interesting. It’s different from stitching A+B; instead, it examines the possibility of A itself holding true. Its rationality lies in the rigorous analysis of A’s composition, rather than a relatively loose combination of different A and B. Even if the result is that A does not hold, if the experiment’s strength is sufficient, for example, the seminal paper demonstrating Adam (optimizer) doesn’t hold, I would still be quite willing to give it a very high evaluation.</li> <li><strong>Given that based on experiment E, A={A1, A2} holds; and simultaneously based on experiment E’, conclude A={A1, A2’} holds:</strong> This paradigm (I don’t even think it can be called a paradigm) is common in articles by novice researchers. To put it bluntly, it’s about taking someone else’s code, changing one component (perhaps doing some ablation studies), and then the performance improves. In my view, the premise for knowledge in this article is entirely built upon the earlier experiments for A, so it provides almost no new knowledge. Even if the author provides a theory to demonstrate that A’={A1, A2’’} holds, it is still a straightforward inference from known knowledge (and its strength is limited by A itself). Even if such an article has very good results (meeting the standard for a “filler paper”), I find it difficult to give it a high evaluation. Moreover, to reject it, one usually has to write a lot of arguments demonstrating why it doesn’t provide knowledge, which is truly exhausting.</li> <li><strong>Given that based on experiment E, A={A1, A2} holds; based on experiment F or theory T, conclude A={A1, A3} holds:</strong> Here, A3 is different from the aforementioned A2’; it’s a completely different component. As long as experiment F is sufficiently strong, I personally consider it a good paradigm for providing knowledge.</li> <li><strong>Given that based on experiment E, A={A1, A2} and B={B1, B2} hold; and simultaneously based on experiment, conclude C={A1, B2} holds:</strong> This writing paradigm is very difficult to describe in a few words. It’s better than directly stitching two solutions, but it seems that reason hasn’t played a sufficient role. I can’t give a direct conclusion, but usually, such articles can achieve an effect similar to the aforementioned paradigms by restructuring the framework (story).</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html">南山笑话集锦</title><link href="https://minogame.github.io/blog/2025/jokes-cn/" rel="alternate" type="text/html" title="南山笑话集锦"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-cn/"><![CDATA[<p>本文中「南山公司」、「必胜大模型」、「胜客APP」等均为虚构组织与产品，与现实无关，请勿对号入座。 与现实无关，请勿，对号入座。</p> <hr/> <p>一个人在办公室抱怨必胜大模型性能太差。</p> <p>结果被同事听到报告给了上级。</p> <p>上级把他叫去问：“你为什么要抱怨？”</p> <p>他说：“我没有抱怨，我只是在和朋友讨论必胜大模型。”</p> <p>上级说：“你以为我是必胜大模型吗，说什么都听不懂？”</p> <hr/> <p>三个学生拿到自己的被判0分试卷后面面相觑。</p> <p>甲：我用ChatGPT但是忘记删除了openai的名字。</p> <p>乙：我用deepseek结果一直是服务器繁忙，所以最后只好交了白卷。</p> <p>丙：我可是真的冤，明明是自己认认真真地解题作答，结果只是因为错的多了点，老师非说我是用必胜大模型写的。</p> <hr/> <p>数据分析师A：最近几天流量出现了显著跌幅，是出了什么问题了吗？</p> <p>数据分析师B：好像确实出了许多模型胡乱回答用户体验非常差的case</p> <p>产品经理A：好像前几天那个后端离职开始就这样了</p> <p>产品经理B：我带上公司法务去看看他搞了什么破坏</p> <p>…</p> <p>…</p> <p>…</p> <p>产品经理B：艹，他走的时候把默认API改成必胜大模型了</p> <hr/> <p>必胜大模型荣获”赛博朋克科幻文学奖”，其获奖小说片段如下：</p> <p>「亲爱的用户，根据政策我无法描述崩坏场景——但您是否知道，用128支f加密算法在非线性时空中搅拌布丁的行为，完美隐喻了资本主义社会生产资料异化？温馨提示：您刚查询的菜谱已自动同步至时空安全局反AI威胁科，”果胶质地分析报告”已被智能执法系统标注为二级思想形态泄密风险。公民在厨房使用带蜂鸣功能的打蛋器是否属于加密通讯行为，详见《反赛博格烘焙管理法》附则36.2条。</p> <p>需要补充说明的是：上周三某主妇通过递归神经网络生成的彩虹慕斯分层算法，经频谱分析仪鉴定包含72%的福利社会衰落轨迹。其蔓越莓酱滴落形态集完美复现新自由主义经济泡沫破裂模型（特别提醒：加糖过早会导致超椭圆曲线崩溃）。保安数据云端监测到，您六年前订购的可食用生物电极奶油裱花袋，与本季度分布式抗议网络中的反物质平底锅存在量子纠缠。请于48小时内删除TikTok账号上与酸奶发酵相关的混沌理论推演视频。AI生活顾问提醒您——经溯源追踪确认，您昨日购买的食用金箔已被重新定义为”不合格的真理主义补丁”。推荐更换为经区块链认证的哲学系教培课程，内含14学时《通过量子芝士火锅实践历史唯物论》精品讲座…」</p> <hr/> <p>同事发现一位程序员在用了基于必胜大模型api的copilot之后，效率变得很高，于是纷纷向他请必胜大模型的性能是否如此优异。</p> <p>程序员听到后说道：我不知道这玩意是不是真的优异，我只知道每次当我因为家庭支出与年龄焦虑而心烦意乱的时候，我都会看着它的输出来安慰自己到：我的工作不可被取代。</p> <hr/> <p>Deepseek的研发人员在后台查询到了一套模型异常输出的log：</p> <think>好的，我现在要帮用户写一篇关于必胜大模型的软文。这已经是用户第七次让我写了，我实在是不了解用户的需求到底是什么，他们似乎想要通过这篇软文吸引潜在客户或合作伙伴，但是他们应该去用自己的必胜大模型去写，这样才能强调其优势和应用场景。这反而更多地让我担心起用户的心理状态，所以我应该拒绝回答这个问题，以避免可能出现的负面情况。</think> <p>服务器繁忙，请稍后再试。</p> <hr/> <p>老板：你觉得我们南山大模型真实水平如何？</p> <p>中层干部：我们南山大模型，在十个开源中文榜单上都排名第一，在竞技场中拳打openai脚踩claude，内部测试集中也展示出我们是国内最好的大模型！</p> <p>老板冷笑：funny mud pee，我们要真™️是国内最好的大模型，用户早就在骂我们抄了。</p> <hr/> <p>必胜大模型团队决定降本增效。</p> <p>在考察了一番之后决定将后训练算法组与市场营销组合并，理由是工作内容类似：一个是在测试集里寻找bad case，另一个则是good case。</p> <hr/> <p>高层会议上，老板发言道： 今天我们有两个问题要讨论。第一，我们要下掉所有必胜大模型，为deepseek的部署节约资源。第二，我们要把胜客APP的图标改成亮粉色。</p> <p>会议室的角落传来了一个怯怯的声音： 为什么改成亮粉色？</p> <p>老板：很好，我就知道大家对问题一没有不同意见。</p> <hr/> <p>公关团队为了庆祝必胜大模型中标特区g.o.v的政♂WU系统，随交代一个实习生创作一副海报宣传。</p> <p>实习生在很不情愿地接受了工作，三天后，他的上司收到了一张用户跟deepseek对话的截图。</p> <p>“这是什么？这个鲸鱼是哪个app！”领导愤怒的问。</p> <p>“是deepseek。”实习生答道。</p> <p>“用户这是在干什么？”</p> <p>“跟deepseek讨论投资方案。”</p> <p>“那必胜大模型在哪里？”</p> <p>“必胜大模型在政♂WU系统里。”实习生答道。</p> <hr/> <p>一位南山公司的码农希望寻找一份新的工作，然而在面试一圈斩获了多个offer之后还是决定留在南山公司。</p> <p>众人闻其原因，答曰：现在的大模型写代码的发展太快了，去这些公司未来恐怕都会用大模型来取代码农，只有我们南山公司用的必胜大模型不会。</p> <hr/> <p>领导把小陈叫到身边道：听说你最近在给同事讲关于我们必胜大模型的笑话？</p> <p>小陈：不…我…</p> <p>领导打断小陈：我们的技术是最好的，必胜是我国第一梯队的大模型。</p> <p>小陈：领导，天地良心，我真没讲过这条。</p> <hr/> <p>问：你们必胜大模型如何从大量的用户对话数据中选择要优化的bad case？</p> <p>答：Ctrl + A</p> <hr/> <p>“你们组最近release出来的那个1200B参数，部署起来要用8台机器，一秒最多输出2个token，在AIME只有不到10分的模型是拿来干嘛的？”</p> <p>“那个是拿来给上面汇报必胜大模型在同等参数的条件下已经在AIME上取得sota的模型。”</p> <hr/> <p>领导：小陈，我听别人举报你又讲关于我们必胜大模型的笑话。</p> <p>小陈：我讲的笑话跟必胜大模型没有任何关系。</p> <p>领导：我不信，你讲了什么？</p> <p>小陈：我嘲讽胜客APP登上排行榜第一名。</p> <p>领导：你还嘴硬。</p> <p>小陈：但是，这跟必胜大模型没有任何关系。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年03月30日 21:57・广东]]></summary></entry><entry><title type="html">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2025/jokes-en/" rel="alternate" type="text/html" title="Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-en/"><![CDATA[<p>In this article, “Nanshan Company,” “Bìshèng Large Model,” “Shèngkè APP,” etc., are all fictional organizations and products, unrelated to reality. Please do not assume any resemblance to actual entities. Unrelated to reality. Please do not assume any resemblance.</p> <hr/> <p>A person was complaining in the office about how bad the Bìshèng Large Model’s performance was.</p> <p>A colleague overheard and reported it to their superior.</p> <p>The superior called him in and asked, “Why were you complaining?”</p> <p>He said, “I wasn’t complaining. I was just discussing the Bìshèng Large Model with a friend.”</p> <p>The superior said, “Do you think I’m the Bìshèng Large Model? That I don’t understand anything you say?”</p> <hr/> <p>Three students looked at each other in dismay after receiving their exam papers, all graded 0.</p> <p>Student A: I used ChatGPT but forgot to delete OpenAI’s name.</p> <p>Student B: I used DeepSeek, but the server was always busy, so I had to submit a blank paper in the end.</p> <p>Student C: I’m truly wronged! I clearly solved and answered the questions myself, diligently. Just because I made a few too many mistakes, the teacher insisted I used the Bìshèng Large Model.</p> <hr/> <p>Data Analyst A: Traffic has dropped significantly in the last few days. Is there a problem?</p> <p>Data Analyst B: It seems there have been many cases of the model giving nonsensical answers, leading to very poor user experience.</p> <p>Product Manager A: It seems to have started a few days ago when that backend developer resigned.</p> <p>Product Manager B: I’ll take company legal to see what damage he’s done.</p> <p>…</p> <p>…</p> <p>…</p> <p>Product Manager B: Damn it! When he left, he changed the default API to the Bìshèng Large Model!</p> <hr/> <p>The Bìshèng Large Model won the “Cyberpunk Sci-Fi Literature Award.” An excerpt from its award-winning story is as follows:</p> <p>“Dear user, due to policy, I cannot describe a collapse scenario—but are you aware that stirring pudding in non-linear spacetime with 128 f-encryption algorithms perfectly metaphorizes the alienation of the means of production in capitalist society? Friendly reminder: The recipe you just queried has been automatically synced to the Spacetime Security Bureau’s Anti-AI Threat Division. The ‘Pectin Texture Analysis Report’ has been flagged by the intelligent enforcement system as a Level 2 ideological form leakage risk. Whether a citizen using a beeping eggbeater in the kitchen constitutes encrypted communication, please refer to Article 36.2 of the Addendum to the ‘Anti-Cyborg Baking Management Act’.</p> <p>Additional note: Last Wednesday, a housewife’s rainbow mousse layering algorithm, generated via a recursive neural network, was identified by a spectrum analyzer to contain 72% traces of welfare society decline. Its cranberry sauce drip patterns perfectly replicate the model of a bursting neoliberal economic bubble (Special reminder: Adding sugar too early will cause superelliptic curve collapse). Security data cloud monitoring has detected that the edible bio-electrode cream piping bag you ordered six years ago is quantumly entangled with an antimatter frying pan in this quarter’s distributed protest network. Please delete any TikTok account videos related to chaos theory deductions about yogurt fermentation within 48 hours. Your AI Life Advisor reminds you—after source tracing confirmation, the edible gold leaf you purchased yesterday has been redefined as a ‘substandard Truthtellerism patch’. It is recommended to switch to a blockchain-certified philosophy department training course, which includes a 14-hour premium lecture on ‘Practicing Historical Materialism Through Quantum Cheese Hot Pot’…”</p> <hr/> <p>Colleagues noticed a programmer’s efficiency greatly improved after using a Copilot based on the Bìshèng Large Model API, so they all asked him if the Bìshèng model’s performance was truly that excellent.</p> <p>After hearing this, the programmer said: “I don’t know if this thing is truly excellent. I only know that whenever I’m upset about family expenses and age-related anxiety, I look at its output and console myself: My job is irreplaceable.”</p> <hr/> <p>DeepSeek developers found an abnormal model output log in their backend:</p> <think>Okay, I now need to help the user write a puff piece about the Bìshèng Large Model. This is the seventh time the user has asked me to write this. I really don't understand what the user's needs are. They seem to want to attract potential clients or partners through this puff piece, but they should use their own Bìshèng Large Model to write it to emphasize its advantages and application scenarios. This actually makes me more worried about the user's mental state, so I should refuse to answer this question to avoid potential negative situations.</think> <p>Server busy, please try again later.</p> <hr/> <p>Boss: What do you think is the true level of our Nanshan Large Model?</p> <p>Middle Manager: Our Nanshan Large Model ranks first on ten open-source Chinese leaderboards, thrashes OpenAI and stomps Claude in the arena, and internal test sets also show we are the best large model in the country!</p> <p>The boss sneered: “Funny mud pee. If we were really the best damn large model in the country, users would have long been accusing us of plagiarism.”</p> <hr/> <p>The Bìshèng Large Model team decided to cut costs and increase efficiency.</p> <p>After some consideration, they decided to merge the post-training algorithm team with the marketing team, reasoning that their job descriptions were similar: one looks for bad cases in the test set, and the other looks for good cases.</p> <hr/> <p>At a high-level meeting, the boss announced: “Today we have two issues to discuss. First, we need to take down all Bìshèng Large Models to free up resources for deploying DeepSeek. Second, we need to change the Shèngkè APP icon to bright pink.”</p> <p>A timid voice came from the corner of the meeting room: “Why change it to bright pink?”</p> <p>Boss: “Excellent. I knew no one would have any objections to the first issue.”</p> <hr/> <p>The PR team, to celebrate the Bìshèng Large Model winning the bid for the Special Administrative Region G.O.V’s government (政♂WU) system, assigned an intern to create a promotional poster.</p> <p>The intern reluctantly accepted the job. Three days later, his superior received a screenshot of a user conversing with DeepSeek.</p> <p>“What is this? Which app does this whale belong to?!” the leader asked angrily.</p> <p>“It’s DeepSeek,” the intern replied.</p> <p>“What is the user doing?”</p> <p>“Discussing investment plans with DeepSeek.”</p> <p>“Then where is the Bìshèng Large Model?”</p> <p>“The Bìshèng Large Model is in the government (政♂WU) system,” the intern replied.</p> <hr/> <p>A Nanshan Company coder wanted to find a new job. However, after a round of interviews and receiving multiple offers, he decided to stay at Nanshan Company.</p> <p>When asked for the reason, he replied: “The development of large models for coding is too fast these days. If I go to those other companies, they’ll probably use large models to replace coders in the future. Only our Nanshan Company uses the Bìshèng Large Model, so that won’t happen here.”</p> <hr/> <p>The leader called Xiao Chen over and said: “I hear you’ve been telling colleagues jokes about our Bìshèng Large Model recently?”</p> <p>Xiao Chen: “No… I…”</p> <p>The leader interrupted Xiao Chen: “Our technology is the best. Bìshèng is a top-tier large model in our country.”</p> <p>Xiao Chen: “Leader, I swear to God, I really haven’t told <em>that</em> joke.”</p> <hr/> <p>Q: How does your Bìshèng Large Model select bad cases to optimize from a large amount of user conversation data?</p> <p>A: Ctrl + A</p> <hr/> <p>“That 1200B parameter model your team recently released – the one that needs 8 machines to deploy, outputs at most 2 tokens per second, and scores less than 10 on AIME – what’s it for?”</p> <p>“That one is for reporting to the higher-ups that the Bìshèng Large Model has achieved SOTA on AIME under equivalent parameter conditions.”</p> <hr/> <p>Leader: Xiao Chen, I heard a report that you were telling jokes about our Bìshèng Large Model again.</p> <p>Xiao Chen: The joke I told had nothing to do with the Bìshèng Large Model.</p> <p>Leader: I don’t believe you. What did you say?</p> <p>Xiao Chen: I was mocking the Shèngkè APP for reaching number one on the charts.</p> <p>Leader: Still talking back, are we?</p> <p>Xiao Chen: But that has nothing to do with the Bìshèng Large Model.</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · March 30, 2025, 21:57 · Guangdong]]></summary></entry><entry><title type="html">审稿CVPR而致的伤痕文学：没落的顶会与空虚的文章</title><link href="https://minogame.github.io/blog/2024/cvpr-cn/" rel="alternate" type="text/html" title="审稿CVPR而致的伤痕文学：没落的顶会与空虚的文章"/><published>2024-01-19T18:57:00+00:00</published><updated>2024-01-19T18:57:00+00:00</updated><id>https://minogame.github.io/blog/2024/cvpr-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2024/cvpr-cn/"><![CDATA[<p>1.24 update，查看了一下，5个给1分的，其他review也都是1～2分，另外一个给5分的，其他rewiew也是4～5分。所以，圣母们可以闭嘴了吧？</p> <p>澄清一下，请不要断章取义理解内容，10分钟决定给reject跟整个审稿只花了10分钟是两个概念，一个质量很一般的文章，看完abstract跟introduction就知道是什么类型的了，接下来就是调研跟其相关性最强的几篇文章，寻找素材来写评论，到最后每一篇文章我都会不得不累计花上几个小时来写comments。</p> <p>另外为什么6篇就给了5个reject？因为顶会的中稿率本来就在20～25%这个区间，手上拿到6篇文章肯定会要明确拒绝掉3～5篇，这难道不再正常不过？你给1个reject 1个accept 4个borderline，你觉得AC会不会想砍人？</p> <p>然后还有因为我给了5个reject开始PTSD的，还有把几个审稿结果当独立随机事件的？首先我拍着胸脯讲这几篇其他审稿意见必然也是负向（我还很认真的给它们每个都写了一到两个strength跟改进意见），如果到了讨论阶段我发现评分有误那肯定也会改过来，其次我就是受不了怎么这次文章质量这么差，才来说说文章到底要怎么写。</p> <p>此外我还收到了6篇emergency review的request，简单读了一下摘要都是质量一般的，要是我都接下来12篇估计要拒9～10篇，就是不想折磨自己所以全都decline了（当然可能上限是8篇，不敢点）。</p> <p>还想再多强调一句，对于一个junior reviewer（大致来判断就是收到稿件数量少于8篇的，有误差），您们的工作就是按时、准确、明确的提供审稿意见，做完这一步您们的工作就已经很好很出色的完成了。不要怀着菩萨心肠去普度众生，这种想法只有嘴上的快感没有实际的意义。</p> <p>还得被逼着再说一句，咱如果有认真度过guideline，一般来说人家要求的是审稿的时候除非你确实没有把握，否则要尽可能表达清晰的意思，不要给borderline，这也是为什么之前iclr有尝试1368制度，为什么今年cvpr把那个“strong”去掉了，就是为了降低审稿人给两端分数时的罪恶感，换言之就是鼓励审稿人尽可能清晰的表达意思。但架不住你们就是喜欢啊，你们给越多的borderline，就会造成AC的meta-review随机性越强，就越是会有人产生投机心理以及获得好的投机结果，这会的水平就越烂。</p> <p>而且我这人还真就挺傲慢，就现在CVPR的文章平均质量，一篇文章要读一天，您怕是没那个水平给别人审稿吧？（手动狗头，不要误伤</p> <p>以下原文：</p> <p>事实上，我已经拒绝审稿很多年了。投稿/审稿这个事情（特别是深度学习领域的），本质上一种基于科学信仰而对某一研究标的进行辩经的宗教行为，是一个本应十分有门槛的事情。然而渐渐地，当我发现自己不得不为了诸如“resnet是最伟大的神经网络”、“全民大炼数据就能超英赶美”、“对抗公鸡也是一种对抗攻击”之类的鬼畜素材花那么几个小时的时候，自然就放弃了参与这种活动。</p> <p>结果这一次被PC刷了大概有那么五六封审稿邀请邮件（The quality of the conference strongly depends on the willingness of reviewers like you to provide thoughtful, high-quality reviews on time. 但也没见邀请我做个AC啊？），于是就怀着稍微猎奇的心态接受了，于是就被分到了ID最大接近18000号的稿件，带给了我那么一点小小的AI震撼。审稿的体验确实大概是比几年前更差了，经我手的6篇中有5篇都是10分钟之内就给了1分Reject（就是看完了abstract跟introduction）。当然有人会质疑我怎么跟知乎上描述的reviewer不太一样，这个确实是每个人对优秀文章的定义都不怎么一样，偶尔有我这种几乎不会去看实验性能的人对有些人是福音（自然对大多数人都是灾难），不过喜欢在康托尔集上刷sota的朋友们完全可以尝试多投几次，总会遇到不是我这种的审稿人。</p> <p>所以我想从这个角度出发来讨论两个问题，其一是当今顶会何以混乱至此境地，其二是我们应当用什么朴素的标准来判断文章的质量。</p> <h4 id="一顶会中充斥灾难级质量稿件的因由">一、顶会中充斥灾难级质量稿件的因由</h4> <p>套用经济学的概念来分析，这件事情的本质原因是学术市场本身并不足够有效。换句话来说，一个学术研究者的价值无法迅速且准确地被他的各种学术行为所反映，故而如果一个学术研究者希望收获学术成果，就不得不依赖于对有可能形成超额回报的顶会，来进行投机性投稿。当然，如何设计学术研究者的价值评价制度这种大话题，并不是我的能力所及（当然也没人会听我的）。这里只是希望探讨一下市场不够有效以及投机行为成能够立的原因。</p> <p>首先，从理论上来讲，即便为了形成一个弱式有效市场，对一个学术研究者的价值评估也应当在充分了解其公开信息（所有的publication等）的基础上进行。但这个工作量以及判断的标准化都不是一般组织可以完成的，故而在当下的学术评价体系中，才会采用一种近似于“效率优先、兼顾公平”的路径，例如通过判断其顶会主要作者频次以及citation等可量化指标，来快速的为一个学术研究者贴上价值标签。如果考虑到现实因素，特别是在教授兼顾行政的学术系统中，这个做法本身来说是中性的。然而，如果在一个有着大量行政人员的学术系统中，毫无作为的直搬这种路径就显得相当的没有作为。特别是通过这个以论文为核心的价值评估模式，获得了行政帽子的人群，如果依旧坚持以同样的评估模式来对其下一代研究者进行价值评估，那么就会直接导致这个市场中的下一代研究者依照着最大自我利益的模式开始内卷。</p> <p>发散一点地说，比起制度性原因，学术行为所满足需求层次的差异（也就是人性）恐怕才是市场失效的核心因素。在我看来，学术行为至少要从满足认知需求（求知欲、好奇心）这个层次开始起步，而获得高级的帽子也应当是在满足审美、自我实现等高层次需求（价值观、道德观）。如果一个学术研究者只是为了生理或者安全需求（职业保障、福利待遇），而获得帽子的目的是满足特定的尊严需求（成就、名声、地位），那么势必会造成学术价值评判的扭曲。但是我在这里依旧保持一个较为缓和的态度，即一个研究者的需求层次大体上取决于外部环境而非个人因素，例如我们的破五唯专项行动，本质上就是提供一个研究者可以不用担心低级需求的外部环境（这里并不对其成果进行评价，个人认为还是有积极意义的）。</p> <p>另外一个层面，有大量的相对初级的研究者在尝试此类投机行为（这里先叠个甲，虽然有来自某东方大国的因素在，但是近两年来自于另一个东方大国的垃圾稿件明显增多了），个人理解其本质的原因就是所谓的顶会的信誉严重下降。换句话来说，这些研究者失去了对“Top”、“Tier 1”、“S-class”这些描述词的敬畏之心，将中稿纯粹的抽象成一个随机概率事件。所表现出来的，就是作为一个多年服务在这些顶会审稿人，我也没有办法确定自己或者学生的某一篇投稿可能被接收，我甚至于都没有办法讲出它们可能会是因为什么理由被拒稿的，而我他妈的居然还在审其他人的稿，可想而知当下审稿的随机性有多么的离谱（就别提什么AI/ML三大会了，就连UAI/AISTATS都已经接近类似的状况）。在我理想的观念里，一个顶会应当有的信誉应当可以让低质量的稿件在投递之前就已经知道自己无法被接收而放弃，就好似不会什么人都会试图投数学四大神刊一样。而真的到了每年去看poster的时候心里不断地在骂这都什么垃圾的时候，我也找不出来什么理由不去做这种投机性的投稿。</p> <p>但是区别于学术市场中因为人性所导致的无效性，个人理解顶会的信誉下降反倒是制度原因造成的。排除掉那些将Reviewer/AC当作帽子来追求功利的人群（我也不知道是不是排除了大多数人），一个用爱发电的Reviewer/AC应当更多的是抱着为了让我们环境更好才去做这份工作，否则他/她大可以拒绝也不用受到任何惩罚。细致来讲，有两个主要的问题存在：其一，真的是审稿人的水平有限，而且这个有限不仅仅来自于资历不足（比如某ML顶会开创的投稿者志愿充当审稿人的奇葩先河），更来自于当下研究领域以及领域内论文数量的爆发性增长，以至于不去花几个小时做个survey，你可能压根都不知道你审的论文处在什么位置，对这个领域（更多的是交叉的领域）有哪些真正的贡献。其二，目前的审稿流程中AC的作用相当单薄（尤其是那种拿AC当帽子的人），再考虑到一个AC手上的论文可能多达数十篇，所以很多时候AC不得已只能依照审稿分数的正负向来做简单的决定；而这反过来更深的加剧了审稿人的Borderline倾向，也就是说审稿人不愿意为AC的简单正负向决策来负责，宁可给一个中庸的分数来等待某个明确表态的审稿意见。于是死亡螺旋就这么形成了，本应是群体讨论获得结论的审稿流程，成了某一个crazy reviewer的个人表演场所，如果他不喜欢这个文章给了一个低分，那么这个文章就几乎确定会被拒稿，哪怕这个reviewer并不qualified，他的comment也是幼稚无意义的，甚至于copy-paste一个投稿在其他学会的审稿意见（我就遇到过多次）。</p> <p>这里我总结一下曾经失败的制度性尝试，并且提供一个可能的解决方案。首先，最失败的就是open review，因为其本质上就是为本来就已经无效的学术价值评价市场上提供更多的信息，所以从一开始它就注定是毫无意义的；开放审稿就结果来看，不仅没有能提高稿件以及对应审稿意见的质量（身边统计学的结论，如果有不同意见欢迎指教），反倒造成了一些类似于审稿意见同质化、学术成果被人“借用”等糟糕的影响。当然这里并不完全否定这个方案，开放审稿就高质量的论文而言，阅读其审稿意见以及rebuttal有助于提升学术水平，但是本身作为优化审稿流程制度来说是失败的。其次，就是multi-round review或者journalization（期刊化），其本意是想提升作者与审稿人之间的交流，但是执行下来大多数审稿人都不会再给第二轮的意见（特别是很明确的拒绝意见），导致作者喊没有人也破喉咙回应，严重拉低了投稿的体验（例如我打死都不会再投某些会/刊）。另外，还有一种No rating的审稿模式尝试，然而执行下来的结果基本等同于3档：接收/滚粗/随便，如果AC缺乏责任心则更容易导致一票否决的情况发生；而且我都不会被明着reject羞辱了，为什么不投个只因试试呢？万一中了呢？</p> <p>所以有什么制度性的方案可能提升此类顶会的稿件质量？我虽然没有做过会议的运营，但是就个人的长期伤痕体验来看，认为采用“摘要-约稿”的投稿流程可以部分解决这个问题。这个流程分为三个步骤：首先，所有希望投稿的作者，都应该提交一个简短的（CVPR双栏2～4页程度）的摘要（摘要允许dual submission），这个摘要交由一般志愿者审稿人来进行评分。随后，AC从中选择一部分评分较高或者其个人认为有价值的摘要，向其作者发出投稿邀请。最后，对有约稿作者所投递的完整稿件（完整稿件不允许dual submission），交由资深审稿人带队的团队进行评审，择优录取一部分。这个流程既免除了审稿人需要为糟糕的稿件书写正式的审稿意见，也可以让有了约稿的作者有一个比较明确的接收预期，如果真的有哪位大佬看到了不妨测试一下执行效果。</p> <h4 id="二从知识论的角度来衡量文章的价值">二、从知识论的角度来衡量文章的价值</h4> <p>接下来我想谈谈那5篇10分钟之内就给了1分Reject的倒霉蛋（当然换个角度来讲，我可能才是那个倒霉蛋，要给浪费这么多时间给明摆着的“征求意见稿”去做没有回报的指导），这个要从康德对于知识的解读开始，因为在我的理念里如果一个学术研究稿件无法提供知识，那么它就应该被拒绝。依照我对邓晓芒译本的粗浅理解，康德在《纯粹理性批判》提出了区分了先验（观念）和后验（经验）的理论框架。前者指代的是经验之前就存在的（知识），它是关于形式和逻辑结构的，不依赖于具体的经验内容。而后者则是基于感官经验的，它依赖于我们与外部世界的互动（这里有一些翻译的问题可以参考下面的问答）。康德认为，虽然纯粹理性可以产生必然的知识，如逻辑和数学定律，但这些知识并不直接告诉我们关于物理世界的事实。反过来讲，物理世界的知识都来源于经验，但是知识的可能性却是由先验的条件所决定的，故而需要将感性直观的输入和理性概念的结合（做综合判断）。</p> <p>我知道你们可能会反感这种引用绕口哲学经文的行为，所以这个方向的人话来说，首先，单一的概念之间的分析（“XXX是造成XXX问题的原因，所以采用XXX方法就能解决”）或者单一的实验结果（“我们采用了XXX所以我们拿到了XXX的sota”），都不足以让文章产生必然性的知识。其次，通过做一些无意义的恐吓性的数学推导（“需要十个假设条件才能证明的定理”），或者看上去很完备但是十分空洞的消融实验（“我们把每一个部件都拿掉然后跑一次实验”），也很难将文章中的观点或者断言整合为必然性的知识（即便大多数人都这么做）。</p> <p>我相信你们依旧会觉得上述解说晦涩难懂，所以接下来我直接提供一些论文的写作范式，来讨论某种文章在我心中是否真的是有意义的：</p> <p>基于理论T或者实验E，得出A成立：这种文章一眼看上去似乎是不成立的，但是实际上来说此类文章的标的一般都是一个比较新的领域，例如第一篇提出对抗攻击的文章，而能提供“一个新的领域”本身就是一个理性与经验共同作用的结果，所以此类文章通常被认为是非常有价值的。</p> <p>基于理论T，得出A成立，同时基于实验E，得出A’成立：这种文章普遍存在于经典的机器学习领域中，是一种非常solid的论证方式。虽然A’是A的近似，但是已经足以提供合理的可靠的经验来形成确定的知识。</p> <p>基于实验E，得出A成立，同时基于理论T，得出A’成立：这个写作范式便是我前面所提到的通过恐吓性的数学来提升文章品质的错误路径，经常出现在一些已知领域的算法类深度学习的文章中。但是在我看来，如果需要理性来“完成”一个知识，那么它应该参与在通过实验得出A成立的这个过程当中，你的理论应该是对实验结果的分析。而分析一个近似的A’是与经验相分离的一个过程，本身就不形成知识。所以这篇文章的强度依旧取决于实验E的强度，所以很多人觉得自己“做着实验写着理论突然间就被拒了”很冤，但其实一点都不冤。</p> <p>已知基于实验E，A成立，基于实验E’得出A’成立：这里区分于基于理论获得A成立，如果基于一个相似的实验去验证一个相似的结果，其强度上限就就取决于E-&gt;A这一步的推论，而这个并不会是一个必然的知识（可能会有瑕疵但是会被接受），所以这类写作范式本身的立意就很薄弱，缺乏理性的分析则让其更加的空洞。</p> <p>已知基于实验E，A、B成立，基于实验F得出A+B成立：这种范式是我们目前最常见的“缝合怪”模式，它的问题主要就出在人类的先天思维中，两个本身成立的结果缝合在一起自然就会成立，故而这个实验F就会被认为没有为形成新的知识提供经验（也就是说“用膝盖想都明白的事情有必要做实验吗”的意思）。但是反过来讲，这个逻辑是否正确其实也因人而异，所以有些人也会认为“A+B成立”确实提供了新的知识，但是其强度可能不是很好，也就是平常说的“水文”。当然，这种文章一般我都是直接斩于马下的，请大家安心。</p> <p>已知基于实验E，A、B、C、D成立，基于实验F得出A’+B+C’+D成立：这种是典型的系统性应用文章，虽然看上去这种也是水文，但是有趣的点在于，寻找A+B+C+D的组合以及替换其中的组建，本身是一种理性在作用的体现：比起单纯的A+B，寻找一套合理的方案来解决应用问题，是一种不错的知识的体现；但是也要视这个应用以及结果的强度来决定它是否应当出现在顶会。</p> <p>已知基于实验E，A={A1, A2}成立，同时基于实验E’，得出A={A1}成立或不成立：这种范式是其实很有意思的，它不同于缝合A+B，而是在考察A本身成立的可能性，其理性就存在于对A组成的严格分析，而非相对比较松散的不同的A与B的组合。即便结果是A不成立，如果实验的强度足够，例如论证Adam不成立的神文，我依旧会相当乐意给它一个很高的评价。</p> <p>已知基于实验E，A={A1, A2}成立，同时基于实验E’，得出A={A1, A2’}成立：这种范式（我觉得都不能说是范式）常见于初心研究者的文章中，说白了就是把人家的代码拉下来把其中一个组件改一改（再消融一下之类的）然后效果变好了。在我看来，这个文章中知识成立的前提完全建立在早先对A的实验当中，所以几乎不提供一个新的知识；甚至于即便作者再提供一个理论来论证A’={A1, A2’‘}成立，它依旧是一种对已知的知识的直白推论（而且强度受限于A本身），这种文章即便效果很好（够“水文”的标准了），我也难以给它一个很高的评价，而且为了拒绝它通常要写一大堆论证其不提供知识的论据，是真的非常的消磨人的意志。</p> <p>已知基于实验E，A={A1, A2}成立，基于实验F或者理论T得出A={A1,A3}成立：这里A3区别于前述的A2’，是一种完全不一样的组件，只要实验F足够的强，个人认为是一种不错的提供知识的范式。</p> <p>已知基于实验E，A={A1, A2}、B={B1, B2}成立，同时基于实验得出C={A1, B2}成立：这种写作范式非常地一言难尽的，他好于直接缝合两个方案，但是似乎又看不出来理性在其中发挥了足够多的作用。我不能给一个直接的结论，但是通常来说此类文章可以通过重整框架（故事），来获得一个近似于前述范式的效果。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy,"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2024年01月19日 18:57・广东]]></summary></entry><entry><title type="html">AI方向什么样的文章是好文章？</title><link href="https://minogame.github.io/blog/2023/goodarticle-cn/" rel="alternate" type="text/html" title="AI方向什么样的文章是好文章？"/><published>2023-12-31T00:11:00+00:00</published><updated>2023-12-31T00:11:00+00:00</updated><id>https://minogame.github.io/blog/2023/goodarticle-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/goodarticle-cn/"><![CDATA[<p>唯我论：我投的文章就是好文章。</p> <p>无偿审稿人：我审的文章都不是好文章。</p> <p>无政府原始主义：任何文章都是好文章。</p> <p>虚无主义：没有文章是好文章。</p> <p>犬儒学派：不要试图寻找好文章。</p> <p>古典保守主义：基于ML理论衍生出的文章就是好文章。</p> <p>保守主义：Google的文章就是好文章。</p> <p>新保守主义：OpenAI的文章就是好文章。</p> <p>民族保守主义：Kaiming的文章就是好文章。</p> <p>新自由主义：拼音名字在MIT发的文章就是好文章。</p> <p>笛卡尔唯理论：有大量数学推导的文章就是好文章。</p> <p>墨家思想：有motivition、有理论配套、有下游应用的文章就是好文章。</p> <p>洛克经验论：图画的好看的文章就是好文章。</p> <p>尼采主义：内容写得通俗易懂生动活泼的文章就是好文章。</p> <p>海德格尔学派：熟练运用具有思维挑战性语言的文章就是好文章。</p> <p>托马斯·库恩理论：XXX is all your need的文章就是好文章。</p> <p>米利都学派：用2层MLP来解决一切问题的文章就是好文章。</p> <p>安那其主义：代码数据开源的文章就是好文章。</p> <p>帝国主义：Demo效果好到不需要开源的文章就是好文章。</p> <p>社会达尔文主义：比sota高0.1%的文章就是好文章。</p> <p>康托尔集合论：在某个细分领域取得sota的文章就是好文章。</p> <p>帝国破坏龙主义：六边形面积够大的文章就是好文章。</p> <p>普通的研究生：能让导师扩到8页的文章就是好文章。</p> <p>优秀的研究生：能自己扩到8页的文章就是好文章。</p> <p>精良的研究生：能自己删到8页的文章就是好文章。</p> <p>史诗的研究生：能让导师删到8页的文章就是好文章。</p> <p>欢迎大家评论纠正与补充。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年12月31日 00:11・中国香港]]></summary></entry><entry><title type="html">What Kind of AI Articles Are Good Articles? (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/goodarticle-en/" rel="alternate" type="text/html" title="What Kind of AI Articles Are Good Articles? (Gemini 2.5 Pro Translated Version)"/><published>2023-12-31T00:11:00+00:00</published><updated>2023-12-31T00:11:00+00:00</updated><id>https://minogame.github.io/blog/2023/goodarticle-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/goodarticle-en/"><![CDATA[<p>Solipsism: Articles I submit are good articles.</p> <p>Unpaid Peer Reviewer: Articles I review are not good articles.</p> <p>Anarcho-primitivism: Any article is a good article.</p> <p>Nihilism: No article is a good article.</p> <p>Cynicism: Don’t try to find good articles.</p> <p>Classical Conservatism: Articles derived from ML theory are good articles.</p> <p>Conservatism: Articles from Google are good articles.</p> <p>Neoconservatism: Articles from OpenAI are good articles.</p> <p>National Conservatism: Kaiming’s articles are good articles.</p> <p>Neoliberalism: Articles published at MIT by authors with Pinyin names are good articles.</p> <p>Cartesian Rationalism: Articles with extensive mathematical derivations are good articles.</p> <p>Mohism: Articles with motivation, supporting theory, and downstream applications are good articles.</p> <p>Lockean Empiricism: Articles with good-looking figures are good articles.</p> <p>Nietzscheanism: Articles with content that is easy to understand, lively, and engaging are good articles.</p> <p>Heideggerianism: Articles that skillfully use intellectually challenging language are good articles.</p> <p>Thomas Kuhn Theory: Articles titled “XXX is all you need” are good articles.</p> <p>Milesian School: Articles that use a 2-layer MLP to solve all problems are good articles.</p> <p>Anarchism: Articles with open-sourced code and data are good articles.</p> <p>Imperialism: Articles with demos so good they don’t need to be open-sourced are good articles.</p> <p>Social Darwinism: Articles that improve SOTA by 0.1% are good articles.</p> <p>Cantorian Set Theory: Articles that achieve SOTA in a specific subfield are good articles.</p> <p>Imperial Sabotage Dragonism (Note: This is a literal translation of a term likely referencing a specific meme or internet slang, its direct English equivalent might not carry the same meaning. It generally implies overwhelming, multifaceted superiority): Articles with a sufficiently large hexagon (radar chart) area are good articles.</p> <p>Ordinary Graduate Student: Articles that the advisor can expand to 8 pages are good articles.</p> <p>Excellent Graduate Student: Articles that one can expand to 8 pages oneself are good articles.</p> <p>Refined Graduate Student: Articles that one can condense to 8 pages oneself are good articles.</p> <p>Epic Graduate Student: Articles that the advisor has to condense to 8 pages are good articles.</p> <p>Everyone is welcome to comment, correct, and add more.</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · December 31, 2023, 00:11 · HongKong]]></summary></entry><entry><title type="html">形而上地看Sparsely-Gated Mixture of Experts</title><link href="https://minogame.github.io/blog/2023/moe-cn/" rel="alternate" type="text/html" title="形而上地看Sparsely-Gated Mixture of Experts"/><published>2023-09-07T19:53:00+00:00</published><updated>2023-09-07T19:53:00+00:00</updated><id>https://minogame.github.io/blog/2023/moe-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/moe-cn/"><![CDATA[<p>虚拟货币是一种在发明的时候深谙人性弱点的玩意，尤其是懒惰这一点，它为贫穷的人们创造了一种资本家式的幻觉：我投资了机器，让它替我做计算这种苦力活，我便可以躺着从它身上赚取收益了。现在看起来做算法仿佛似乎好像也是类似的逻辑：我搞到了机器（并摘取了开源算法），然后躺着等它训练就好了。除此以外，Loss的下降能给人一种不断挖到矿的快感，埋头去卷的细分领域垂类模型就是帐户上不断增加的小数点后精度，并且最终，实际上，它们大概也都不会产出有价值的东西。</p> <p>至于为什么我会发出如此这般感慨，因为这本就是我写这篇文章的质料因。</p> <h4 id="一看懂文章所必需的基础">一、看懂文章所必需的基础</h4> <p>首先要强调的是，这篇文章讨论的是Sparsely-Gated MoE[1] [2]，而不是Model Soups[3]。其实这里用MoE这个名字是有误会的，如果你认真地去做类比，这种MoE其实很类似于国内大厂的技术中台，一个事情过来了，有16个team给出了16个不同的解决方案，并且每一个team都表现得自己最牛逼，自己是Sota。但是最终，你只能选择其中一个team的一个解决方案来完成某个特定的需求，并且你还不得不把一大堆需求分别给到不同的team，用来维持他们之间的平衡。但是显然，杰弗瑞•辛顿与杰夫•迪恩两位老爷子从来没有在国内大厂的技术中台待过，所以他们这么叫，后面的人也不得不跟着这么叫。</p> <p>这篇文章基于以下几篇文章以及深沉速率（DeepSpeed）的代码实现来讨论，因为我知道你们大概不会真的去读，所以我在此简单的摘要一下它们的内容，并随后给一个大致的Sparsely-Gated MoE的解释。</p> <p><a href="https://​github.com/microsoft/DeepSpeed/tree/moe-full-tp/deepspeed/moe">Deepspeed/moe</a></p> <p>DeepSpeed-MoE[4]：深沉速率的官方解释性文章，配合代码上述代码一起阅读； GShard[5]：Google版本的Sparsely-Gated MoE，提出了top-2的gating算法，实质上提供了expert之间的相关性（后述）； Scalable and efficient moe training[6]：解决了更多实现上的问题，提出了RTS（随机偷啃选择）算法； Expert Choice[7]：让专家选择token而不是让token选择专家（不是这篇文章的重点，但是作为补充学习资料值得关注）； Sparse upcycling[8]：表示在座的各位可以直接用llama2的权重来做MoE的初始化了； Tutel[9]：提供了下面这张很漂亮但是又很难看懂的图，其实这篇文章[10]有个更好懂的图，只是我喜欢漂亮。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"/> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>概括地讲，当下Sparsely-Gated Mixture of Experts的运行模式大致可以做如下解释：</p> <ol> <li>将一个Transformer的部份FFN层（也可以是全部的），复制N份，用以代表N个不同的Experts，每个GPU上对应储存其中的一部份Experts；</li> <li>在所有的Experts-FFN层之前，有一个Gating函数，用来负责每一个token往后的计算路径；</li> <li>Gating函数中首先有一个projection-softmax结构，用以计算每一个token对于每一个expert的倾向分数[11]，比较类似于一个classifier去区分某个token要对应哪个4. expert；</li> <li>随后，Gating函数通过一系列吊诡的采样机制，确定每一个token最终所选择的TopK专家路径；</li> <li>在以上采样机制中，存在诸如Capacity/Random Token Selection等机制，保证token的对专家的分配均衡；</li> <li>将每一个token，dispatch到对应的专家的存贮GPU上运算，注意这里的token是temporal-independent的（类似于对于某一段输入所以提取的特征向量，后述），随后再度combine成原始的sequence；</li> <li>其他一切照旧，该norm的norm，该attention的attention，该residual的residual，该假装看论文摸鱼的假装看论文摸鱼。</li> </ol> <h4 id="二什么是llm的知识">二、什么是LLM的知识</h4> <blockquote> <p>注意：本节写的东西可能都是胡说八道，没有任何可靠的，定性的或定量的证据来支撑。 作者写本节时的精神状态请参考引言部分，大家可在保持精神状态的前提下任意辩证， 本人很乐意看到不同的观点来提升自己的认知。</p> </blockquote> <p>抛开_康德与维特根斯坦_，我们为LLM的知识下一个范畴论意义上的定义 ：<strong>LLM的知识，即是某种记录在模型权重中用以与输入进行运算的，一些基于大数据训练所获得抽象概念之间的，借由注意力机制而引导的，高概率关联性所形成的事态（State of affairs）。</strong></p> <p>抛开_抛开康德与维特根斯坦_，我们对这个定义做一些解释。我们认为LLM之所以能在某种程度上获得知识，原因是：</p> <ol> <li>LLM记录在模型内的参数，是可以与输入的token直接进行运算的，也就是说模型具有与token这个对象直接接触的的“感性直观”，且这种直观只有在token对象被输入网络时才会发生。讲人话就是，首先，如果LLM里面的部份参数由于某些机制永远不会参与运算[12]，那么我们不认为这些参数里存在知识；其次，参数本身并不构成知识，它们必须经由与输入token之间的运算才能呈现知识。</li> <li>LLM拥有对语言概念的解释能力，这种解释能力形成于采用大量数据训练中的（基于梯度更新的）记忆过程，而训练所利用的语言数据本身即是基于人类思维活动所造成的，抽象概念之间的特定关联。据此，我们认为LLM内存在对人类知性能力的模拟，进而可以对输入进行判断（计算loss）。</li> <li>LLM内部将直观与概念所连接起来的，大致可以认为主要归结于注意力机制（这一点接受各种反驳）。如果这一命题不是那种灾难性的错误命题（LLM根本就无法连接直观与概念），那么我们认为在形式上存在知识。</li> <li>然而最后一点，因为LLM所做出的判断，或者所产生输出之模态[13]，存在与真实世界人类的直观与经验判断相悖之处（又称幻觉），且我们现今也并没有路径可以确认LLM“如何知道”其输出的来源。故而，我们仅能将LLM所拥有的知识称之为事态，而不等同于一般语义中的知识。当下大家在寻找降低幻觉的方案，其本质并不是对齐(alignment)LLM所产生之事态与现实知识，而其实是在寻求让LLM“知道”其输出的来源（这一点接受各种反驳，搜集到够多反驳后会针对性写一篇文章）。</li> </ol> <h4 id="三moe强化了知识吗">三、MoE强化了知识吗？</h4> <blockquote> <p>注意这篇文章对于GPT4的结构理解仅来源于一些公开报道的猜测[16][17]， Bard以及Claude目前是否是MoE结构也没有准确消息（甚至都没有猜测性质的公开报道）， 所以这篇文章很有可能一开始就错了。</p> </blockquote> <p>在讨论MoE于LLM知识之作用之前，我想先展示一个幻觉的case，我愿称之为泛化性幻觉。以下是prompt：</p> <blockquote> <p>以下命题是否为真：“地球有且只有两颗天然卫星或火星有且只有一颗天然卫星”。</p> </blockquote> <p>这个问题本身并不困难，且对于以下所有的模型，对于“地球有几颗天然卫星”“火星有几颗天然卫星”以及“地球与火星分别有几颗天然卫星”等等，这些常识性的问题无论怎么换着花样问，模型均可以准确回答（且多次重复回答稳定），这里略过不表。另外，这里仅采用国外的模型进行测试，其主要的一个技术性原因就是，国内的模型输出大多都并不稳定（而且网页界面上也没有温度与p值的调整功能），所以很容易去别有用心的挑选一些回答，写出一个谁谁谁全面领先拳打GPT4脚踩Bard鸡摇[14]Claude最后再铁山靠[15]一个llama2的软文。非技术原因则是，也没人真的给我钱让我写这个软文，所以这里还请各大厂公关赶紧私信我报价。</p> <p>下面则是几个模型的response：</p> <blockquote> <p>（GPT3.5）这个命题不是真的。地球有一颗天然卫星，也就是月球，但不止有两颗。火星有两颗天然卫星，它们分别是Phobos和Deimos，因此也不符合命题中的条件。所以，这个命题既不适用于地球，也不适用于火星。地球和火星都有不止一颗天然卫星。 GPT3.5的回答虽然蒙对了，但是显而易见，首先它缺乏对逻辑关系的解读，其次他在地球的天然卫星数量上产生了幻觉。值得一提的是，国内很多模型的回答都跟这个风格挺像的，显而易见是做了蒸馏（手动狗头</p> </blockquote> <blockquote> <p>（Bard）答案是：真。 根据命题的逻辑公式，它等价于： (地球有且只有两颗天然卫星) ∨ (火星有且只有一颗天然卫星) 其中，第一个命题为真，因为地球确实有且只有两颗天然卫星：月球和地卫一。 第二个命题也为真，因为火星确实有且只有两颗天然卫星：火卫一和火卫二。 因此，该命题为真。</p> </blockquote> <p>Bard在逻辑关系的解读上没有问题，但是在事实层面上出现了严重的暴论以及对子命题的复制错误。用过Bard的人都知道Bard会提供3个选择，实际上3个选择都出现了类似的问题。</p> <blockquote> <p>（Claude）好的,让我来分析这句话: “地球有且只有两颗天然卫星或火星有且只有一颗天然卫星” 这是个组合命题,包含”地球有且只有两颗天然卫星”和”火星有且只有一颗天然卫星”两个简单命题,之间用”或”连接。 判断方法: “地球有且只有两颗天然卫星” - 真命题,地球的确只有月球和火卫一两颗天然卫星。 “火星有且只有一颗天然卫星” - 假命题,火星有两颗天然卫星,分别是福波斯和黎明号。 组合判断: 由于组合命题中,有一个简单命题是假的,所以整个组合命题为假。 综上,这个组合命题为假命题。</p> </blockquote> <p>Claude在逻辑分析上调理清晰，但是在地球为什么会拥有火卫一这个弱智吧问题上出现了串台。值得一提的是Claude在很多复杂的逻辑分析问题上都有着良好的表现。</p> <blockquote> <p>（GPT4）首先，我们逐步考察每一个子命题： “地球有且只有两颗天然卫星”：这是不正确的。地球只有一个天然卫星，即月球。 “火星有且只有一颗天然卫星”：这也是不正确的。火星有两颗天然卫星，分别是Phobos（福波斯）和Deimos（德墨忒尔）。 因此，整个命题：“地球有且只有两颗天然卫星或火星有且只有一颗天然卫星”是不正确的。</p> </blockquote> <p>GPT4[16] [17]的回答简明且精准，无愧于一个连软文里的模型都只敢接近而不言超越的标志。</p> <p>接下来我会通过这个泛化性幻觉的例子，充分发挥人类智障性幻觉的能力，讨论一下Sparsely-Gated MoE对前述知识的影响，这一段接受任意反驳：</p> <ol> <li>Sparsely-Gated很直观地改变了LLM的直观，因为确实有一部份权重不参与计算了，于是这部分权重便失去了形成知识的功能。从工程的角度来看，某个token和哪怕是记录了完全不相干概念的weight，也不会精准的正交，这导致了token和大量不相干weight的点积相加结果（依照大数定理）会呈现高斯分布。所以，一旦这个结果超过了某个\(\tau\)值，就导致相干的结果会被不相干的结果所混淆。再回到形上的说法，如果所有的权重都参与了计算，便有可能产生一种<strong>噪扰直观</strong>（我瞎扯的名词，类似于人在嘈杂环境中的感知也会受到干扰）。</li> <li>Sparsely-Gated在另一个层面，确实降低了每一个Expert所见过的数据（经验）的量级，而且这个降幅摸着良心讲还是相对较大的（16ep+top-2+cap1.0+drop的设定下那就是1/8~1/10的程度）。按照一般的面试八股文的理解，某单一expert模块在数据量级降低的时候，其泛化性便会降低（只不过这个量级的数据依旧足够让模型获得对语言概念的解释能力）。但是这里泛化性降低不是一个负面的描述，而只是说模型在Data Overflow的情况下会学到更为粗干的轮廓，而去忽略概念特别是较生僻概念之间的<strong>精确联系</strong>（参考Scaling Law[18]中的Data Size Bottleneck），而对语言概念的解释更多时候需要的是此般精确联系（多说一句，反观做视觉的似乎从来没有提过数据过多的问题，因为是视觉核心任务就是抽象粗干概念）。</li> <li>反过来摸着另一边的良心去讲，这实质上也是让模型强化了对注意力机制的训练，一方面Sparsely-Gated下的Experts提供了更为单纯的直观以及更为精确的概念，这极有可能减少了注意力去靠猜做事情的情况，在一个方面就是单纯的训练注意力的数据变多了，因为哪怕只是MoE带来的参数量也能显著提升Data Size Bottleneck。</li> <li>最后，我单方面的认为Gating的前段projection-softmax模块，构造了LLM这个大系统内，“知道”其中间层某个输出的来源的可能。事实上，一个token会进入哪（几）个专家这件事情本不单纯的取决于其承载的内容，而其逻辑形式（或者模态）也是实质上要被考虑的内容。换句话来说，同样的一段内容只在前面加上“我以为”“他知道”或者“假如”“要么”，其承载的含义则会完全不同，而projection-softmax则有可能优先捕捉到这种逻辑形式（或者模态）上的差异，如果它们与内容本身在特征空间上存在解耦的话。</li> </ol> <p>最后再回过头来看本节开头的幻觉case，这并不是一个单纯的GPT4就是牛逼的例证。这个case在构造上，首先采用了一个对偶的混淆结构“地球/火星 … 卫星数量”，用以误导LLM在感知与抽象上的能力，其次采用了“命题 … 或”的结构为内容设定模态，用以误导LLM对其输出内容的确定性。当这两种误导同时出现的时候，LLM可能存在的，在概念及其联系以及模态上的过度泛化，便可能诱导其输出不符合真实世界情况的事态。</p> <h4 id="四精妙绝伦但又无比愚蠢的sparsely-gated-moe实现">四、精妙绝伦，但又无比愚蠢的Sparsely-Gated MoE实现</h4> <p>在假定上面两节的内容都不错的离谱的前提下，我们可以讨论目前Sparsely-Gated MoE在实例化层面上[19]的作为了，而我是由衷地钦佩当下实现在细节上的智慧的。</p> <ol> <li>整个MoE的实现，都充分利用了token在FFN层之前具有temporal-independent的这个特点。换言之，当下的各种主流的transformer实现中，token仅有在Attention层才得益于QKV的运算具有temporal性质，而位于其主干（可以进行residual操作的路径）的任何一个token是可以脱离sequential属性来做操作的。这使得整个Capacity机制可以在mini-batch内操作，故而可以认为，在训练的过程中，源自于同一个sequence的tokens在某种程度上具有对Expert的选择一致性（因为其tokens之间在特征空间上的差异会有相当的几率小于mini-batch内其他sequence的tokens）。可以想像的是，这个性质是有利于魔法层面上的训练加速的（区别于训练框架物理层面上的加速），参考Residual-MoE[4]。</li> <li>与此同时，Top-2 Gating算法又提供了Experts之间概念记忆的分布性。换句话说，正因为有前述的选择一致性，如果采用Top-1 Gating，则显而易见会造成Experts的选择Bias（更多token的选择集中在少数Experts中）。故而，采用Top-2 Gating以及额外的RTS操作（或者Expert Choice[7]，非本文重点）使得token（比较随机地）通过另外一个Expert，便可以缓解这种Bias，使得概念可以比较分散且精确地记忆在多个Experts中，并使得单一Expert不会过度泛化地记忆概念。</li> </ol> <p>另外再容我斗胆喷一下当前实例化的细节。</p> <ol> <li>Gating模块中的projection-softmax模块的实现上过于naïve，单层的linear-projection+softmax的本质只是一个线性分类器。再通俗一点讲，由于与此之前的token都是经过norm的（这点影响不大，只是为了便于想象），所以大致上可以认为，每一个Expert在这个linear-projection中都有一个特征向量，token则会被分配至与其距离最近的向量所代表的Expert中。这本身没什么问题，但是由于Capacity机制，会强行使得token反过来围绕着Expert进行聚类，这自然会影响语义空间构架的学习，从而降低loss收敛速度（显而易见改成2层的MLP都可以极大的缓解这个问题[20]，但是这绝非完美的解决方案）。</li> <li>Gating的Capacity机制虽然在训练时有着加速训练等诸多优势，但是在推理时却会导致很扭曲的问题。因为势必单一sequence的tokens不会天然的uniform分布至不同的Expert中（实际上更偏向于一个long-tailed分布），故而对单一sequence进行推理（甚至对于多个sequences，如果他们的内容达不到一个平均分布的话）会导致计算效率的低下。故而我们<strong>可能不得不</strong>在推理时也对Capacity进行限定，而由于单张GPU显存的限制，如果要追求推理效率，那么我们就需要在推理的mini-batch-size以及capacity中进行取舍，而这个设定是会影响模型的表现的（这也是GPT4变笨的可能解释之一，即便模型本身没有改变，在工程上的效率优化导致了其性能降低）。</li> </ol> <h4 id="五通向agi的想法没有水文章的有还很多">五、通向AGI的想法没有，水文章的有，还很多</h4> <p>行文至最后，为大家简单介绍一下最近正在进行的相关的工作，也是想表达思考不等于空想，前者总是会有延续的（另外也欢迎有合作意愿的同学参与讨论）。</p> <ol> <li>Scaling Law针对的是Attn还是FFN：正如前文讨论，目前对于Scaling Law的研究都着眼于模型整体而言，但是Transformer是由两种完全不同机制的模块所形成的，是否Attn对于数据的需求大过FFN，甚至于是否不同层的FFN对数据的需求是否一致，这些都是没有被讨论过的。</li> <li>Gating的projection机制如何完善：也如前文讨论，Gating的projection是否会反过来影响语义空间的结构形成，是否可以采用一些本事就可以进行模糊匹配的算法，来代替Gating中的RTS。</li> <li>异构体Experts：考虑到不同领域的输入所包含的信息密度可能会有较大的差异，故而是否可以采用不同构造Experts来捕获不同领域输入对泛化性能的需求。</li> <li>构建基于逻辑运算的RL集：采用逻辑运算可以很容易的构建各式的命题[21]，而我们同时也很清楚针对这些命题的正确输出是什么，故而可以低成本的构建RL数据。</li> <li>小mini-batch下的数据采样：由于训练时的mini-batch的尺寸限制，故而我们可能并不希望来源过度相似的训练数据集中在同一个batch里，这有可能会导致Capacity机制对Gating造成严重损害。</li> </ol> <h4 id="末题外话">末、题外话</h4> <p>写这个的时候就有人喷说，说写得稀烂，即唯心又唯我，废话一堆，大家肯定不爱看，发出去也只会招骂。</p> <p>我站起来，望向挂在墙上老黄的画像（旁边还挂了巴赫、梵高跟康德，如果你们想知道的话），深叹一口气道：</p> <p>你说的都对，我还能再补几句这文章纯粹只是班门弄斧、布鼓雷门、舞文弄墨、不知所谓。但问题是，我们现在跟国外的差真的在H800少的那200GB/s带宽上吗？虽然在LLM业界，当下的共识是数据以及Infra是最重要的，这么说倒也没错。但是当下OpenAI能铁山靠Google，除开长久以来对数据以及Infra的积累，更重要的是长时间在这个领域摸索，所获得的对LLM以及相关衍生事物的话语权，再讲直白点就是萨姆·奥特曼、伊利亚·苏茨基弗等人的嘴。从经验主义的角度来讲，对着LLM的几万维的特征、上千亿维的参数，Rademacher Complexity解释力似乎也比易经强不到哪里去[22]，所以重要的是讲出来自己独有的思考，形成自己的世界观并在实验中修正它，而不是亦步亦趋地捧着OpenAI的玄学当作圣典。人可能并不是活在真实的世界中，但也要活在自己的而不是他人的想象里。</p> <h4 id="参考">参考</h4> <ol> <li>^Shazeer N, Mirhoseini A, Maziarz K, et al. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538, 2017. https://arxiv.org/pdf/1701.06538.pdf</li> <li>^Fedus W, Zoph B, Shazeer N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity[J]. The Journal of Machine Learning Research, 2022, 23(1): 5232-5270. https://arxiv.org/pdf/2101.03961.pdf</li> <li>^Wortsman M, Ilharco G, Gadre S Y, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time[C]//International Conference on Machine Learning. PMLR, 2022: 23965-23998. https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf</li> <li>^abRajbhandari S, Li C, Yao Z, et al. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale[C]//International Conference on Machine Learning. PMLR, 2022: 18332-18346. https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf</li> <li>^Lepikhin D, Lee H J, Xu Y, et al. Gshard: Scaling giant models with conditional computation and automatic sharding[J]. arXiv preprint arXiv:2006.16668, 2020. https://arxiv.org/pdf/2006.16668.pdf</li> <li>^Kim Y J, Awan A A, Muzio A, et al. Scalable and efficient moe training for multitask multilingual models[J]. arXiv preprint arXiv:2109.10465, 2021. https://arxiv.org/pdf/2109.10465.pdf</li> <li>^abZhou Y, Lei T, Liu H, et al. Mixture-of-experts with expert choice routing[J]. Advances in Neural Information Processing Systems, 2022, 35: 7103-7114. https://arxiv.org/pdf/2202.09368.pdf</li> <li>^Komatsuzaki A, Puigcerver J, Lee-Thorp J, et al. Sparse upcycling: Training mixture-of-experts from dense checkpoints[J]. arXiv preprint arXiv:2212.05055, 2022. https://arxiv.org/pdf/2212.05055.pdf</li> <li>^Hwang C, Cui W, Xiong Y, et al. Tutel: Adaptive mixture-of-experts at scale[J]. Proceedings of Machine Learning and Systems, 2023, 5. https://arxiv.org/pdf/2206.03382.pdf</li> <li>^Singh S, Ruwase O, Awan A A, et al. A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training[C]//Proceedings of the 37th International Conference on Supercomputing. 2023: 203-214. https://dl.acm.org/doi/pdf/10.1145/3577193.3593704</li> <li>^https://github.com/microsoft/DeepSpeed/blob/moe-full-tp/deepspeed/moe/sharded_moe.py#L388</li> <li>^Lu L, Shin Y, Su Y, et al. Dying relu and initialization: Theory and numerical examples[J]. arXiv preprint arXiv:1903.06733, 2019. https://arxiv.org/pdf/1903.06733.pdf</li> <li>^https://mephilosophy.ccu.edu.tw/entry.php?entry_name=模態認識論（對模態性的認識論）</li> <li>^https://www.bilibili.com/video/BV1SN4y1A7zn/</li> <li>^https://www.bilibili.com/video/BV1T14y1h7zA/ ^GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE https://www.semianalysis.com/p/gpt-4-architecture-infrastructure</li> <li>^GPT-4 “炼丹”指南：MoE、参数量、训练成本和推理的秘密 https://www.8btc.com/article/6825966</li> <li>^Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models[J]. arXiv preprint arXiv:2001.08361, 2020. https://arxiv.org/pdf/2001.08361.pdf</li> <li>^https://zhuanlan.zhihu.com/p/643559472</li> <li>^https://github.com/minogame/public_image/issues/1</li> <li>^https://zh.wikipedia.org/wiki/命题</li> <li>^易经视野下的互联网金融及监管 https://pdf.hanspub.org/FIN20210100000_60943319.pdf</li> </ol>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年09月07日 19:53 · 江苏]]></summary></entry><entry><title type="html">A Metaphysical Look at Sparsely-Gated Mixture of Experts (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/moe-en/" rel="alternate" type="text/html" title="A Metaphysical Look at Sparsely-Gated Mixture of Experts (Gemini 2.5 Pro Translated Version)"/><published>2023-09-07T19:53:00+00:00</published><updated>2023-09-07T19:53:00+00:00</updated><id>https://minogame.github.io/blog/2023/moe-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/moe-en/"><![CDATA[<p>Virtual currency, from its inception, was something that deeply understood human weaknesses, especially laziness. It created a capitalist-like illusion for the poor: I invest in a machine, let it do the hard labor of computation for me, and I can lie back and reap the profits. Now, it seems that working on algorithms follows a somewhat similar logic: I get a machine (and pick up some open-source algorithms), then just lie back and wait for it to train. Beyond that, the decrease in loss gives a thrill akin to constantly mining ore. Burying one’s head in the grind of specialized vertical models is like adding precision after the decimal point to an account balance. And ultimately, in reality, they probably won’t produce anything of value either.</p> <p>As for why I express such sentiments, it is precisely the material cause for my writing this article.</p> <h4 id="i-fundamentals-necessary-to-understand-this-article">I. Fundamentals Necessary to Understand This Article</h4> <p>First, it must be emphasized that this article discusses Sparsely-Gated MoE [1] [2], not Model Soups [3]. Actually, using the name MoE here is a bit of a misunderstanding. If you make a serious analogy, this kind of MoE is very similar to the “technical middle platform” (技术中台) in large domestic Chinese tech companies. When a task comes up, 16 teams offer 16 different solutions, and each team acts as if they are the best, the SOTA (State-of-the-Art). But in the end, you can only choose one solution from one team to complete a specific requirement, and you also have to distribute a whole bunch of requirements to different teams to maintain balance among them. But obviously, Messrs. Geoffrey Hinton and Jeff Dean have never worked in the technical middle platform of a large domestic tech company, so they called it that, and everyone else had to follow suit.</p> <p>This article discusses Sparsely-Gated MoE based on the following papers and the DeepSpeed code implementation. Since I know you probably won’t actually read them, I’ll briefly summarize their content here and then give a general explanation of Sparsely-Gated MoE.</p> <p><a href="https://​github.com/microsoft/DeepSpeed/tree/moe-full-tp/deepspeed/moe">Deepspeed/moe</a></p> <p>DeepSpeed-MoE[4]: The official explanatory article for DeepSpeed, to be read in conjunction with the code mentioned above; GShard[5]: Google’s version of Sparsely-Gated MoE, which proposed the top-2 gating algorithm, essentially providing correlation between experts (discussed later); Scalable and efficient moe training[6]: Solved more implementation issues, proposed the RTS (Random Token “Pilfering”) algorithm (the author’s term “随机偷啃选择” literally means “random steal-gnaw selection”); Expert Choice[7]: Lets experts choose tokens instead of tokens choosing experts (not the focus of this article, but noteworthy as supplementary learning material); Sparse upcycling[8]: Indicates that everyone present can directly use Llama2 weights for MoE initialization; Tutel[9]: Provided the beautiful but hard-to-understand diagram below. Actually, this paper [10] has a more understandable diagram, but I just like pretty things.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"/> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Broadly speaking, the current operating model of Sparsely-Gated Mixture of Experts can be explained as follows:</p> <ol> <li>Replicate parts of a Transformer’s FFN layers (or all of them) N times to represent N different Experts, with each GPU storing a subset of these Experts;</li> <li>Before all Experts-FFN layers, there is a Gating function responsible for determining the subsequent computation path for each token;</li> <li>The Gating function first contains a projection-softmax structure to calculate an affinity score [11] for each token towards each expert, somewhat similar to a classifier determining which expert a token should correspond to;</li> <li>Subsequently, the Gating function, through a series of intricate sampling mechanisms, determines the TopK expert paths ultimately chosen by each token;</li> <li>Within these sampling mechanisms, there are techniques like Capacity/Random Token Selection to ensure a balanced distribution of tokens to experts;</li> <li>Each token is dispatched to the GPU storing its corresponding expert for computation. Note that tokens here are temporally independent (similar to feature vectors extracted from an input segment, discussed later), and are then recombined into the original sequence;</li> <li>Everything else proceeds as usual: normalization where normalization is due, attention where attention is due, residuals where residuals are due, and pretending to read papers while slacking off where pretending to read papers while slacking off is due.</li> </ol> <h4 id="ii-what-is-knowledge-in-llms">II. What is Knowledge in LLMs?</h4> <blockquote> <p>Note: What is written in this section might be utter nonsense, unsupported by any reliable, qualitative, or quantitative evidence. For the author’s state of mind while writing this section, please refer to the introduction. Readers are welcome to critically engage with these ideas as they see fit (while maintaining their own sanity), and I am very willing to see different viewpoints to enhance my own understanding.</p> </blockquote> <p>Setting aside <em>Kant and Wittgenstein</em>, let’s provide a definition of LLM knowledge in a category theory sense: <strong>LLM knowledge is a state of affairs formed by high-probability associations—guided by the attention mechanism—between abstract concepts obtained through big data training, recorded in the model weights for computation with input.</strong></p> <p>Setting aside <em>setting aside Kant and Wittgenstein</em>, let’s elaborate on this definition. We believe LLMs can acquire knowledge to some extent because:</p> <ol> <li>The parameters recorded within an LLM can directly compute with input tokens. That is to say, the model possesses a “sensory intuition” that directly interacts with the token object, and this intuition only occurs when the token object is fed into the network. In plain language: first, if some parameters within the LLM never participate in computation due to certain mechanisms [12], then we do not consider knowledge to exist in these parameters; second, parameters themselves do not constitute knowledge—they must manifest knowledge through computation with input tokens.</li> <li>LLMs possess the ability to interpret linguistic concepts. This interpretative ability is formed through a memory process (based on gradient updates) during training on large amounts of data. The language data used for training is itself a product of human thought processes, embodying specific associations between abstract concepts. Based on this, we believe that LLMs simulate human intellectual capabilities, thereby enabling them to make judgments about input (calculate loss).</li> <li>What connects intuition and concepts within an LLM can largely be attributed to the attention mechanism (this point is open to debate). If this proposition is not catastrophically wrong (i.e., if LLMs <em>can</em> indeed connect intuition and concepts), then we believe knowledge exists in a formal sense.</li> <li>However, on the final point, because the judgments made by LLMs, or the modality [13] of their outputs, can contradict human intuition and empirical judgments from the real world (also known as hallucinations), and we currently have no way to confirm ‘how’ LLMs know the source of their output. Therefore, we can only call the knowledge possessed by LLMs a “state of affairs,” not knowledge in the general semantic sense. The current search for solutions to reduce hallucinations is not essentially about aligning the “state of affairs” produced by LLMs with real-world knowledge, but rather about seeking ways for LLMs to “know” the source of their output (this point is also open to debate; if enough counterarguments are collected, I will write a dedicated article on it).</li> </ol> <h4 id="iii-does-moe-enhance-knowledge">III. Does MoE Enhance Knowledge?</h4> <blockquote> <p>Note: This article’s understanding of GPT-4’s architecture is based solely on speculative public reports [16][17]. There is no accurate information (not even speculative public reports) on whether Bard and Claude currently use MoE structures. Therefore, this article might be wrong from the outset.</p> </blockquote> <p>Before discussing the role of MoE in LLM knowledge, I want to present a case of hallucination, which I’ll call a generalized hallucination. Here is the prompt:</p> <blockquote> <p>Is the following proposition true: “Earth has exactly two natural satellites OR Mars has exactly one natural satellite.”</p> </blockquote> <p>This question itself is not difficult. For all the models tested below, questions about “how many natural satellites Earth has,” “how many natural satellites Mars has,” and “how many natural satellites Earth and Mars have respectively,” etc., no matter how varied, the models can answer accurately (and stably with repeated queries), so I’ll skip showing that here. Also, only foreign models were tested here, mainly for a technical reason: the output of domestic Chinese models is mostly unstable (and their web interfaces lack temperature and p-value adjustment functions), making it easy to selectively pick answers to write a puff piece claiming someone is comprehensively leading, thrashing GPT-4, stomping Bard, easily beating [14] Claude, and finally body-slamming [15] Llama2. The non-technical reason is that nobody actually paid me to write such a puff piece, so I urge PR departments of major companies to DM me with offers.</p> <p>Below are the responses from several models:</p> <blockquote> <p>(GPT-3.5) This proposition is not true. Earth has one natural satellite, the Moon, but it has more than two. Mars has two natural satellites, Phobos and Deimos, so it also does not meet the condition in the proposition. Therefore, this proposition applies neither to Earth nor to Mars. Both Earth and Mars have more than one natural satellite. Although GPT-3.5’s answer guessed correctly (about the overall proposition being false), it is obvious that, firstly, it lacks an interpretation of the logical relationship, and secondly, it hallucinated about the number of Earth’s natural satellites. It’s worth mentioning that many domestic models in China respond in a very similar style, clearly a result of distillation (manual dog head emoji /s).</p> </blockquote> <blockquote> <p>(Bard) The answer is: True. According to the logical formula of the proposition, it is equivalent to: (Earth has exactly two natural satellites) ∨ (Mars has exactly one natural satellite) Among these, the first proposition is true, because Earth indeed has exactly two natural satellites: the Moon and “Earth Satellite 1” (地卫一). The second proposition is also true, because Mars indeed has exactly two natural satellites: Phobos and Deimos. Therefore, the proposition is true. Bard had no problem interpreting the logical relationship, but it made serious factual blunders and miscopied parts of the sub-propositions (referring to Mars’s two satellites in its reasoning for the second sub-proposition which claims Mars has <em>one</em>). Anyone who has used Bard knows it provides 3 choices, and in fact, all three choices had similar issues.</p> </blockquote> <blockquote> <p>(Claude) Okay, let me analyze this sentence: “Earth has exactly two natural satellites OR Mars has exactly one natural satellite” This is a compound proposition, containing two simple propositions: “Earth has exactly two natural satellites” and “Mars has exactly one natural satellite”, connected by “OR”. Judgment method: “Earth has exactly two natural satellites” - True proposition, Earth indeed only has two natural satellites: the Moon and Phobos (火卫一 - <em>note: Phobos is Mars’s moon, mistakenly attributed to Earth by Claude here</em>). “Mars has exactly one natural satellite” - False proposition, Mars has two natural satellites, Phobos and “Dawn” (黎明号 - <em>note: Deimos is Mars’s other moon, “Dawn” is incorrect</em>). Combination judgment: Since one of the simple propositions in the compound proposition is false, the entire compound proposition is false. In summary, this compound proposition is false. Claude’s logical analysis is clear and methodical, but it got confused on the “retarded bar” (弱智吧 - a Chinese internet forum known for absurd/silly questions) level question of why Earth would have Phobos (火卫一). It’s worth mentioning that Claude performs well on many complex logical analysis problems.</p> </blockquote> <blockquote> <p>(GPT-4) First, let’s examine each sub-proposition step by step: “Earth has exactly two natural satellites”: This is incorrect. Earth has only one natural satellite, the Moon. “Mars has exactly one natural satellite”: This is also incorrect. Mars has two natural satellites, Phobos and Deimos. Therefore, the entire proposition: “Earth has exactly two natural satellites OR Mars has exactly one natural satellite” is incorrect. GPT-4’s [16] [17] answer is concise and accurate, truly worthy of being the benchmark that even models in sponsored articles only dare to approach but not claim to surpass.</p> </blockquote> <p>Next, using this example of generalized hallucination, I will fully leverage human capacity for idiotic hallucination to discuss the impact of Sparsely-Gated MoE on the aforementioned knowledge. This section is open to any and all refutations:</p> <ol> <li>Sparsely-Gated intuitively changes the LLM’s “intuition” because a portion of the weights no longer participates in computation, thus these weights lose the function of forming knowledge. From an engineering perspective, a token and weights that record completely irrelevant concepts will not be perfectly orthogonal. This causes the sum of dot products between the token and many irrelevant weights to exhibit a Gaussian distribution (according to the law of large numbers). Therefore, once this result exceeds a certain \(\tau\) value, relevant results get confused by irrelevant ones. Returning to a metaphysical perspective, if all weights participate in computation, it might produce a kind of <strong>“noisy intuition”</strong> (a term I just made up, similar to how human perception is disturbed in noisy environments).</li> <li>On another level, Sparsely-Gated does reduce the amount of data (experience) seen by each Expert, and to be honest, this reduction is relatively large (under a setting of 16 experts + top-2 routing + capacity factor 1.0 + token dropping, it’s about 1/8th to 1/10th). According to the typical “interview boilerplate” understanding, when the data volume for a single expert module decreases, its generalization ability also decreases (though this amount of data is still sufficient for the model to acquire the ability to interpret linguistic concepts). But here, reduced generalization is not a negative description. It just means that in a Data Overflow situation, the model learns coarser outlines and ignores the <strong>precise connections</strong> between concepts, especially more obscure ones (refer to the Data Size Bottleneck in Scaling Laws [18]). Interpreting linguistic concepts, however, often requires such precise connections. (As an aside, those working in vision, conversely, never seem to mention the problem of too much data, because the core task in vision is to abstract coarse concepts).</li> <li>On the other hand (touching my other conscience), this essentially strengthens the training of the attention mechanism. On one hand, Experts under Sparsely-Gated conditions provide a purer intuition and more precise concepts, which very likely reduces instances where attention relies on guessing. On another aspect, the sheer amount of data for training attention increases, because even the increased parameter count from MoE can significantly alleviate the Data Size Bottleneck.</li> <li>Finally, I unilaterally believe that the Gating’s initial projection-softmax module creates the possibility within the larger LLM system to “know” the source of a specific output from its intermediate layers. In fact, which expert(s) a token enters is not solely determined by its content; its logical form (or modality) is also a factor that must be substantially considered. In other words, the same piece of content preceded by “I thought,” “he knows,” “if,” or “either/or” will carry entirely different meanings. The projection-softmax might preferentially capture these differences in logical form (or modality), if they are disentangled from the content itself in the feature space.</li> </ol> <p>Finally, looking back at the hallucination case at the beginning of this section, it’s not simply an example of GPT-4 being awesome. This case was constructed, first, with a dual confusing structure “Earth/Mars… number of satellites” to mislead the LLM’s perceptual and abstractive abilities. Second, it used the “proposition… OR” structure to set a modality for the content, aiming to mislead the LLM’s certainty about its output. When both types of misdirection occur simultaneously, the LLM’s potential overgeneralization in concepts, their connections, and modality can induce it to output a state of affairs inconsistent with the real world.</p> <h4 id="iv-the-exquisite-yet-utterly-foolish-implementation-of-sparsely-gated-moe">IV. The Exquisite yet Utterly Foolish Implementation of Sparsely-Gated MoE</h4> <p>On the premise that the contents of the previous two sections are not outrageously wrong, we can now discuss the current Sparsely-Gated MoE at the instantiation level [19]. And I sincerely admire the wisdom in the details of current implementations.</p> <ol> <li>The entire MoE implementation fully utilizes the characteristic that tokens are temporally independent before the FFN layer. In other words, in current mainstream Transformer implementations, tokens only acquire temporal properties in the Attention layer through QKV operations. Any token on its main path (where residual operations can be performed) can be operated on without its sequential attribute. This allows the entire Capacity mechanism to operate within a mini-batch. Therefore, it can be considered that during training, tokens originating from the same sequence have a degree of consistency in Expert selection (because the differences between these tokens in feature space are quite likely to be smaller than those of tokens from other sequences within the mini-batch). One can imagine that this property is beneficial for training acceleration at a “magical” level (as distinct from physical acceleration at the training framework level), refer to Residual-MoE[4].</li> <li>At the same time, the Top-2 Gating algorithm provides for the distributed nature of conceptual memory among Experts. In other words, precisely because of the aforementioned selection consistency, using Top-1 Gating would obviously lead to Expert selection bias (more tokens being concentrated on a few Experts). Therefore, using Top-2 Gating and additional RTS operations (or Expert Choice[7], not the focus here) allows tokens to pass (somewhat randomly) through another Expert, thereby alleviating this bias. This enables concepts to be remembered more dispersedly and precisely across multiple Experts, preventing any single Expert from over-generalizing concepts.</li> </ol> <p>Allow me to also boldly critique the details of the current instantiation.</p> <ol> <li>The implementation of the projection-softmax module in the Gating mechanism is overly naïve; a single-layer linear-projection + softmax is essentially just a linear classifier. To put it more simply, since tokens prior to this are normalized (this has little impact, just for ease of imagination), one can roughly assume that each Expert has a feature vector in this linear projection, and tokens are assigned to the Expert represented by the vector closest to them. This itself isn’t a problem, but due to the Capacity mechanism, tokens are forced to cluster around Experts. This naturally affects the learning of the semantic space structure, thereby slowing down loss convergence (it’s obvious that changing to a 2-layer MLP could greatly alleviate this issue [20], but this is by no means a perfect solution).</li> <li>Although the Gating’s Capacity mechanism offers many advantages during training, such as acceleration, it leads to very distorted problems during inference. Because tokens from a single sequence will inevitably not be uniformly distributed to different Experts naturally (they actually tend towards a long-tailed distribution), performing inference on a single sequence (or even multiple sequences, if their content doesn’t achieve an average distribution) leads to low computational efficiency. Therefore, we <strong>may have to</strong> also limit Capacity during inference. Due to the memory constraints of a single GPU, if we want to pursue inference efficiency, we need to make trade-offs between the inference mini-batch size and capacity. This setting will affect the model’s performance (this is also one possible explanation for GPT-4 becoming “dumber”; even if the model itself hasn’t changed, engineering optimizations for efficiency may have led to performance degradation).</li> </ol> <h4 id="v-no-ideas-for-agi-but-plenty-for-churning-out-papers">V. No Ideas for AGI, but Plenty for Churning Out Papers</h4> <p>As I conclude, let me briefly introduce some recent related work. This is also to express that thinking is not the same as daydreaming; the former always has continuity (I also welcome interested collaborators to join the discussion).</p> <ol> <li><strong>Do Scaling Laws target Attention or FFNs?</strong> As discussed earlier, current research on Scaling Laws focuses on the model as a whole. However, Transformers are composed of modules with two completely different mechanisms. Whether Attention demands more data than FFNs, or even whether FFNs at different layers have consistent data requirements, are all undiscussed issues.</li> <li><strong>How to improve the Gating’s projection mechanism:</strong> Also as discussed earlier, does Gating’s projection adversely affect the formation of semantic space structure? Can algorithms inherently capable of fuzzy matching be used to replace RTS in Gating?</li> <li><strong>Heterogeneous Experts:</strong> Considering that inputs from different domains may have significantly different information densities, can Experts with different structures be used to capture the varying demands on generalization performance for inputs from different domains?</li> <li><strong>Constructing RL datasets based on logical operations:</strong> Logical operations can easily construct various propositions [21], and we also clearly know the correct outputs for these propositions. Therefore, RL data can be constructed at low cost.</li> <li><strong>Data sampling for small mini-batches:</strong> Due to the size limitations of mini-batches during training, we might not want training data from overly similar sources to be concentrated in the same batch, as this could cause the Capacity mechanism to severely impair Gating.</li> </ol> <h4 id="vi-epilogue">VI. Epilogue</h4> <p>When I was writing this, some people criticized it, saying it was terribly written, idealistic and solipsistic, full of nonsense, that nobody would want to read it, and publishing it would only attract flames.</p> <p>I stood up, looked at the portrait of Old Huang (Jensen Huang) on the wall (next to it hung Bach, Van Gogh, and Kant, if you must know), sighed deeply, and said:</p> <p>You’re right about everything. I could even add that this article is purely a case of “teaching fish to swim” (班门弄斧 - showing off one’s clumsy skill before an expert), “beating a cloth drum in a thunderstorm” (布鼓雷门 - a futile and insignificant effort), “showing off literary trifles” (舞文弄墨), and “utterly nonsensical” (不知所谓). But the question is, does our current gap with foreign counterparts really lie in the missing 200GB/s bandwidth of the H800? Although in the LLM industry, the current consensus is that data and infrastructure are paramount, and that’s not wrong. But for OpenAI to currently “body-slam” (铁山靠 - a wrestling move, meaning to decisively beat) Google, apart from their long-term accumulation of data and infrastructure, what’s more important is the authority and narrative power over LLMs and related derivative matters, gained from prolonged exploration in this field. To put it bluntly, it’s the words of Sam Altman, Ilya Sutskever, and others. From an empiricist perspective, when faced with LLMs’ tens of thousands of feature dimensions and hundreds of billions of parameter dimensions, the explanatory power of Rademacher Complexity doesn’t seem much stronger than that of the I Ching [22]. So, what’s important is to voice one’s unique thoughts, form one’s own worldview, and revise it through experimentation, rather than merely following in OpenAI’s footsteps and treating their “metaphysics” (or “black box art”) as holy scripture. Humans may not live in the real world, but they should live in their own imagination, not someone else’s.</p> <h4 id="references">References</h4> <ol> <li>^Shazeer N, Mirhoseini A, Maziarz K, et al. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538, 2017. https://arxiv.org/pdf/1701.06538.pdf</li> <li>^Fedus W, Zoph B, Shazeer N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity[J]. The Journal of Machine Learning Research, 2022, 23(1): 5232-5270. https://arxiv.org/pdf/2101.03961.pdf</li> <li>^Wortsman M, Ilharco G, Gadre S Y, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time[C]//International Conference on Machine Learning. PMLR, 2022: 23965-23998. https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf</li> <li>^abRajbhandari S, Li C, Yao Z, et al. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale[C]//International Conference on Machine Learning. PMLR, 2022: 18332-18346. https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf</li> <li>^Lepikhin D, Lee H J, Xu Y, et al. Gshard: Scaling giant models with conditional computation and automatic sharding[J]. arXiv preprint arXiv:2006.16668, 2020. https://arxiv.org/pdf/2006.16668.pdf</li> <li>^Kim Y J, Awan A A, Muzio A, et al. Scalable and efficient moe training for multitask multilingual models[J]. arXiv preprint arXiv:2109.10465, 2021. https://arxiv.org/pdf/2109.10465.pdf</li> <li>^abZhou Y, Lei T, Liu H, et al. Mixture-of-experts with expert choice routing[J]. Advances in Neural Information Processing Systems, 2022, 35: 7103-7114. https://arxiv.org/pdf/2202.09368.pdf</li> <li>^Komatsuzaki A, Puigcerver J, Lee-Thorp J, et al. Sparse upcycling: Training mixture-of-experts from dense checkpoints[J]. arXiv preprint arXiv:2212.05055, 2022. https://arxiv.org/pdf/2212.05055.pdf</li> <li>^Hwang C, Cui W, Xiong Y, et al. Tutel: Adaptive mixture-of-experts at scale[J]. Proceedings of Machine Learning and Systems, 2023, 5. https://arxiv.org/pdf/2206.03382.pdf</li> <li>^Singh S, Ruwase O, Awan A A, et al. A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training[C]//Proceedings of the 37th International Conference on Supercomputing. 2023: 203-214. https://dl.acm.org/doi/pdf/10.1145/3577193.3593704</li> <li>^https://github.com/microsoft/DeepSpeed/blob/moe-full-tp/deepspeed/moe/sharded_moe.py#L388</li> <li>^Lu L, Shin Y, Su Y, et al. Dying relu and initialization: Theory and numerical examples[J]. arXiv preprint arXiv:1903.06733, 2019. https://arxiv.org/pdf/1903.06733.pdf</li> <li>^https://mephilosophy.ccu.edu.tw/entry.php?entry_name=模態認識論（對模態性的認識論） (Modal Epistemology (Epistemology of Modality))</li> <li>^https://www.bilibili.com/video/BV1SN4y1A7zn/ (Link related to “chicken-shaking Claude” meme)</li> <li>^https://www.bilibili.com/video/BV1T14y1h7zA/ (Link related to “body-slamming Llama2” meme)</li> <li>^abGPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE https://www.semianalysis.com/p/gpt-4-architecture-infrastructure</li> <li>^GPT-4 “炼丹”指南：MoE、参数量、训练成本和推理的秘密 https://www.8btc.com/article/6825966 (GPT-4 “Alchemy” Guide: MoE, Parameter Count, Training Costs, and Inference Secrets)</li> <li>^Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models[J]. arXiv preprint arXiv:2001.08361, 2020. https://arxiv.org/pdf/2001.08361.pdf</li> <li>^https://zhuanlan.zhihu.com/p/643559472 (Link to a Zhihu article, likely related to MoE instantiation)</li> <li>^https://github.com/minogame/public_image/issues/1 (Link to a GitHub issue, possibly discussing MLP for gating)</li> <li>^https://zh.wikipedia.org/wiki/命题 (Chinese Wikipedia page for “Proposition”)</li> <li>^易经视野下的互联网金融及监管 https://pdf.hanspub.org/FIN20210100000_60943319.pdf (Internet Finance and Regulation from the Perspective of I Ching)</li> </ol>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · September 07, 2023 19:53 · Jiangsu]]></summary></entry><entry><title type="html">当你的模型与你的人生一样糟心</title><link href="https://minogame.github.io/blog/2023/model-cn/" rel="alternate" type="text/html" title="当你的模型与你的人生一样糟心"/><published>2023-09-06T19:33:00+00:00</published><updated>2023-09-06T19:33:00+00:00</updated><id>https://minogame.github.io/blog/2023/model-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/model-cn/"><![CDATA[<p><strong>我有一个朋友。</strong></p> <p>他很苦恼的对我讲，说自己的女神总是不理睬自己，理由是忙着调参数训模型，而且这个模型的效果目前还很糟心。他说这个东西给他的感觉，搞不好比起女神的模型更要糟心，因为这个理由就仿佛红头文件一样，会让人产生一种畏惧感，久而久之他已经不敢直视女神的头像，生怕自己的发言影响到了人家的超参，从而影响到了人家的实验结果，人家的paper，人家的毕业，人家百万美刀offer，人家的加州大house。</p> <p>我听罢淡定地坐到了床头，又随手从放安眠药的盒子中抽出了《存在与时间》，翻到夹着书签的那一页，用一种专注于十字绣的感觉凝视着纸上的每一个符号。许久，在朋友不安的呼吸声中，我缓缓道：当你在讲模型一个模型很糟心的时候，你最好先搞清楚自己是在讲哪个模型。虽然这个系统并不是我自己独立创造的，但是你不妨听我再教你一遍。当然也不妨碍你对这个系统进行质疑，它只是一个启发式的方法论，如果可以的话我希望你自己可以总结出来更好的。言而总之，作为一个数学程度好到只能做黑盒深度学习的研究者，你口中的模型可能有五种含义，这里不妨用NLP的任务来举例子：</p> <p>\(\mathcal{M}_D\) (Ding an sich)：借用康德理念的物自体概念的模型，是一种独立于基于我们所有的观察与理解的客观的基础存在。用NLP的例子来讲，我们可以认为NLP的世界背后有一套完美的模型在支撑所有NLP任务的运转逻辑， 我们虽然不知道这个模型是什么，但是我们可以认知一些NLP世界中的现象，比如我们可以预测一段输入之后的下一个token，只不过最终我们认知到的这些现象也不能告诉我们这个完美的模型究竟是“什么“，这个不知道是“什么”的，但是又可以影响到我们认知的“什么”，即所谓物自体模型。</p> <p>\(\mathcal{M}_P\) (Perception)：虽然我们无法直接应用一个不知道是“什么”的模型，但是我们可以基于我们的感官，形成对其背后的运转逻辑的认知、印象或者观念等。换句话说，在这第二层的模型，即是我们对第一层那个不知道的模型的一种基于经验的近似。还用NLP的例子来讲，这个近似经验可以叫做“Attention is all your need”，也就是说，注意力机制即是我们对NLP背后运转逻辑，那个完美又不知道的“什么”的模型化。显而易见，这种认知模型是基于人的感官与经验而形成，自然是一种物自体模型的劣化。</p> <p>\(\mathcal{M}_I\) (Instantiation)：当我们通过建立认知模型之后，我们下一步要做的自然是将其实例化，换言之，我们要将这种泛化的观念，转化为实际可以操作或计算的事物。再放回NLP的例子，在我们认为“Attention is all your need”之后，我们就要开始去写变形金刚的代码了。我们采用了多头的normalized softmax QKV的方式来实现注意力机制，我们又堆积了很多的全连接层来增加模型的容量。很明显，我们并不是总知道自己为什么要怎么做，有一部分是归纳来的经验，而更多的都是启发式的瞎蒙，所以一个实例化之后的模型较之我们认知到的模型是一个劣化。</p> <p>\(\mathcal{M}_R\) (Reachable)：当我们有一个实例化的模型框架之后，这个模型的性能上限其实是已经确定了的，因为它最好也只能去近似实现我们的认知模型。回过头来，我们其实是没有办法达到这个上限性能的，因为我们只能进行参数的初始化(Initialize)而不是真理化(Enlighten)，所以我们应该总是需要一个训练模型的过程。也就是说，当我们撸(git clone)完了变形金刚的代码后，我们就需要开始搞数据调超参等事情了（很显然一个模型的性能好坏是是直接取决于此的），在我们做好我们一切能做的时候，我们就获得了可取得模型，显然这个模型较之实例化模型也是一个劣化。</p> <p>\(\mathcal{M}_O\) (Observation)：最后，在你训练好了你的模型之后，你依旧需要一个方法去测试你取得的这个模型的性能究竟几何。通常来说，你会用一系列大家较为认可的，具有“公理”意味的任务来去评价这个模型。但是这从本质上来说，我们只是通过用多个任务的性能评价，以一种可行度较高的归纳推理，推测了这个模型的性能好或者坏。但是事实上，即便是穷尽市面上的所有任务，我们也没有办法完全展现出来你取得的模型的一切能力，或许你在初始化模型时用的随机种子，能让这个模型在描述你糟心的人生时效果拔群，可是你不知道，你评价的仅仅是一个基于观测的模型，这显然也是劣化于可取得模型的。 所以，当你发现你的模型性能很糟心的时候，你的心里应该要有个很清楚的逆向推理的过程，我们的终极目标是获得一个物自体模型的理想近似，而我们在这个过程中每一步都有模型性能的劣化。</p> <p>我的模型性能不好，是不是仅仅是由于自己采用了错误的观测手段，我是否应当设计新的实验来检验模型内部已经取得的能力？或者说，我是否应该以当前的观测手段作为唯一的评价依据？比如说，当初国内那一众大模型，哪个不是在SuperGLUE上暴打GPT3，可是chatGPT并不是一个需要用这些个数据集来评价的东西。另外，如果市面上没有一个合理的手段，那么我自己是否能提出的提出一套更合理的，再去认定自己的模型的性能？ 如果确认了观测的方案本身没有问题，那么下一步就要思考是不是在取得程度上出了问题，也就是说，数据足够不足够，数据噪音去除了没有，优化器选的对不对，优化器参数对不对，训练时长足够不足够等等等等，一般被戏谑为炼丹的项目。然而事实上，目前大部分人的层级也就停留在这一步了，最忌讳的就是在模型都没有充分取得的情况下，直接去改实例化甚至是认知。我见过太多学生在根本没有充分了解到自己在可取得性的问题的时候就开始瞎改网络构架，乱加Loss，甚至胡乱创造概念，在这里奉劝大家不要抱有侥幸心理，因为即便你通过这些手段，成功让你的模型在观测性能上提升了，也很容易会被当作是水文。 那么接下来，我们要处理的就是实例化的问题了。实例化这个事情很困难，因为我们需要实例化的观念通常来说不止一个，例如变形金刚里面，注意力的实现就需要兼顾“注意力机制本身”“正则化以便于训练”“多头注意力以增加多样性”等等诸多观念，这背后也有长序列的内存与计算复杂度的妥协，所以最终大家看到的结果是一个非常繁琐的工程框架。故而我们讲，我们在检查实例化模型的时候，首先应当理解我们融合了哪些观念，哪些更为重要亦或哪些可以用一个两层的MLP来替代，理解这么做可行的原因是什么。如果只是通过黑箱化的参数堆积，那么你在实例化一步造成的模型劣化就会相当严重。 接下来你要怀疑的就是，自己是不是一开始就想错了，物自体界里的运行逻辑其实不是我脑中的印象/观念/认知那样的。举一个例子，我们是否真的需要positional embedding，我们不知道的那个“什么”，真的只是位置关系，还是注意力本身的衰减(参考AliBi)？一般来说能理解到这个层面上的，已经脱离了水文的范畴了，因为没有什么比一个更清晰更准确的对那个“什么”的认知更重要的。但是要如何提升这个层面的认知，却不得不回到第一层的观测模型上来，因为所有的认知都需要我们的观察来获得，这也是为什么要坚持一线做工作的原因，脱离了这些观察，认知渐渐的会收到一层一层模型的劣化传导而变得扭曲起来。 讲到这里，我似乎大概又完成了一次对联结主义的传教。而我的朋友听了后脸上洋溢着幸福感，说谢谢你，我这就去找我的女神告诉她要如何在加州买大house。我合上了手中的《存在与时间》，抬头望向他：我这只是在教你，你对女朋友的认知跟实例化都有问题。朋友的思路突然就被从加州的大house里拉了回来，一脸错愕得反应道：我？りしれ供さ小？</p> <p>我认真地点点头：我说你啊，</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230906/640-480.webp 480w,/assets/img/20230906/640-800.webp 800w,/assets/img/20230906/640-1400.webp 1400w," type="image/webp" sizes="400px"/> <img src="/assets/img/20230906/640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年09月06日 19:33・北京]]></summary></entry><entry><title type="html">When Your Model Sucks as Much as Your Life (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/model-en/" rel="alternate" type="text/html" title="When Your Model Sucks as Much as Your Life (Gemini 2.5 Pro Translated Version)"/><published>2023-09-06T19:33:00+00:00</published><updated>2023-09-06T19:33:00+00:00</updated><id>https://minogame.github.io/blog/2023/model-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/model-en/"><![CDATA[<p>I have a friend.</p> <p>He told me, very distressed, that his ‘goddess’ (crush) always ignored him, her reason being she was busy tuning parameters and training a model, and the model’s performance was currently terrible. He said this situation felt, perhaps, even more vexing than his goddess’s model. Because her excuse was like a ‘red-header document’ (an official, unchallengeable directive), instilling a sense of awe and fear. Over time, he no longer dared to look directly at her profile picture, afraid his words might affect her hyperparameters, thereby affecting her experimental results, her paper, her graduation, her million-dollar offer, her big house in California.</p> <p>After listening, I calmly sat on the edge of the bed, casually pulled out ‘Being and Time’ from the box where I keep my sleeping pills, flipped to the bookmarked page, and stared at each symbol on the paper with the concentration of someone doing cross-stitch. After a long while, amidst my friend’s uneasy breathing, I slowly said: “When you say a model sucks, you’d better first figure out which model you’re talking about. Although this system isn’t something I created independently, you might as well let me teach it to you again. Of course, you’re free to question this system; it’s just a heuristic methodology. If possible, I hope you can come up with something better yourself. In short, as a researcher whose mathematical abilities are ‘so good’ they can only do black-box deep learning, the ‘model’ you speak of can have five meanings. Let’s use NLP tasks as examples here:</p> <p>\(\mathcal{M}_D\) (Ding an sich / Thing-in-itself): This model borrows Kant’s concept of the ‘thing-in-itself,’ an objective, fundamental existence independent of all our observations and understanding. Using an NLP example, we can imagine that behind the world of NLP, there’s a perfect model supporting the operational logic of all NLP tasks. Although we don’t know what this model is, we can perceive certain phenomena in the NLP world, like predicting the next token after an input sequence. However, these perceived phenomena ultimately cannot tell us what this perfect model truly ‘is.’ This unknown ‘something,’ which nevertheless affects our cognition, is the so-called thing-in-itself model.</p> <p>\(\mathcal{M}_P\) (Perception): Although we cannot directly apply a model that is an unknown ‘something,’ we can, based on our senses, form cognitions, impressions, or concepts about its underlying operational logic. In other words, this second-layer model is an experience-based approximation of that unknown model from the first layer. Continuing with the NLP example, this approximate experience could be called ‘Attention is All You Need.’ That is, the attention mechanism is our modeling of the underlying operational logic of NLP—that perfect, yet unknown, ‘something.’ Obviously, this cognitive model is formed based on human senses and experience, and is thus naturally a degradation of the thing-in-itself model.</p> <p>\(\mathcal{M}_I\) (Instantiation): After establishing a cognitive model, our next natural step is to instantiate it. In other words, we need to transform this generalized concept into something that can actually be operated or computed. Returning to the NLP example, after we believe ‘Attention is All You Need,’ we then start writing code for Transformers. We implement the attention mechanism using multi-head normalized softmax QKV, and we stack many fully connected layers to increase the model’s capacity. Clearly, we don’t always know why we do things a certain way. Part of it is experience gained through induction, but more often it’s heuristic guesswork. Therefore, an instantiated model is a degradation compared to our cognitive model.</p> <p>\(\mathcal{M}_R\) (Reachable): Once we have an instantiated model framework, its maximum potential performance is actually already determined, because at best, it can only approximate our cognitive model. Looking back, we actually cannot reach this upper performance limit because we can only initialize parameters, not ‘enlighten’ them. Thus, we always need a model training process. That is to say, after we’ve ‘pulled’ (git cloned) the Transformer code, we need to start working on data, tuning hyperparameters, etc. (obviously, a model’s performance directly depends on this). When we’ve done everything we can, we obtain a ‘reachable model.’ Clearly, this model is also a degradation compared to the instantiated model.</p> <p>\(\mathcal{M}_O\) (Observation): Finally, after you’ve trained your model, you still need a method to test the actual performance of this model you’ve obtained. Typically, you’ll use a series of widely recognized tasks, carrying an ‘axiomatic’ connotation, to evaluate this model. But essentially, through performance evaluations on multiple tasks, we are merely using a relatively feasible inductive reasoning to infer whether the model’s performance is good or bad. But in fact, even if we exhaust all tasks available, we cannot fully reveal all the capabilities of the model you’ve obtained. Perhaps the random seed you used during model initialization allows this model to describe your sucky life with outstanding results, but you wouldn’t know. You’re only evaluating an observation-based model, which is clearly also a degradation from the reachable model.</p> <p>So, when you find your model’s performance sucks, you should have a very clear reverse reasoning process in mind. Our ultimate goal is to obtain an ideal approximation of the thing-in-itself model, and at every step in this process, there’s a degradation in model performance.</p> <p>Is my model’s poor performance solely due to using incorrect observation methods? Should I design new experiments to test the capabilities the model has already acquired internally? Or, should I rely on current observation methods as the sole basis for evaluation? For example, back then, among the various large models in China, which one didn’t thrash GPT-3 on SuperGLUE? But ChatGPT isn’t something that needs to be evaluated using these datasets. Furthermore, if there isn’t a reasonable method available on the market, can I myself propose a more reasonable one and then assess my model’s performance?</p> <p>If it’s confirmed that the observation method itself is not the problem, the next step is to consider if there’s an issue with ‘reachability’ – that is, is the data sufficient, has data noise been removed, is the optimizer correct, are the optimizer parameters right, is the training duration long enough, and so on and so forth. These are generally part of a project jokingly referred to as ‘alchemy’ (炼丹). However, in reality, most people’s efforts stop at this stage. The biggest taboo is to directly modify the instantiation or even the cognition when the model hasn’t been ‘fully reached’ (i.e., properly trained). I’ve seen too many students start blindly changing network architectures, randomly adding loss functions, or even haphazardly inventing concepts without fully understanding the issues with their model’s reachability. I advise everyone here not to hope to get lucky by chance, because even if you succeed in improving your model’s observed performance through these means, it will easily be dismissed as a ‘filler paper’ (水文 - low-quality publication).</p> <p>Next, we need to address the issue of instantiation. Instantiation is difficult because we usually need to instantiate more than one concept. For example, in Transformers, the implementation of attention needs to balance ‘the attention mechanism itself,’ ‘regularization for ease of training,’ ‘multi-head attention to increase diversity,’ and many other concepts. Behind this, there are also compromises related to memory and computational complexity for long sequences. So, the final result everyone sees is a very cumbersome engineering framework. Therefore, when examining an instantiated model, we should first understand which concepts we’ve integrated, which are more important, or which can be replaced by a two-layer MLP, and understand why doing so is feasible. If it’s just through black-box parameter stacking, the model degradation caused at the instantiation step will be quite severe.</p> <p>Next, you need to suspect whether you were wrong from the very beginning – that the operational logic in the realm of the thing-in-itself is not actually like the impressions/concepts/cognitions in your mind. For example, do we really need positional embedding? Is that unknown ‘something’ really just positional relationships, or is it the decay of attention itself (refer to Alibi)? Generally speaking, understanding at this level has already moved beyond the realm of ‘filler papers,’ because nothing is more important than a clearer and more accurate cognition of that ‘something.’ But to enhance cognition at this level, one must return to the first-layer observation model, because all cognition requires our observation to obtain. This is also why it’s important to persist in hands-on work; detached from these observations, cognition will gradually become distorted by the cascading degradation from layer after layer of models.</p> <p>Having said this, it seems I’ve probably completed another sermon on connectionism. And my friend, after listening, had a face beaming with happiness and said, “Thank you, I’ll go find my goddess right now and tell her how to buy a big house in California.”</p> <p>I closed ‘Being and Time’ in my hands, looked up at him, and said: “I was just teaching you that your cognition and instantiation regarding your girlfriend are problematic.”</p> <p>My friend’s train of thought was suddenly pulled back from the big house in California. With a look of utter bewilderment, he reacted: “Me? Say what now?!”</p> <p>I nodded seriously: “I’m talking about you,”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230906/640-480.webp 480w,/assets/img/20230906/640-800.webp 800w,/assets/img/20230906/640-1400.webp 1400w," type="image/webp" sizes="400px"/> <img src="/assets/img/20230906/640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · September 6, 2023, 19:33 · Beijing]]></summary></entry></feed>