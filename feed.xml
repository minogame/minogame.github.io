<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://minogame.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minogame.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T16:07:26+00:00</updated><id>https://minogame.github.io/feed.xml</id><title type="html">Zhun Sun</title><subtitle>Zhun Sun&apos;s personal page and blog storage. </subtitle><entry><title type="html">南山笑话集锦</title><link href="https://minogame.github.io/blog/2025/jokes-cn/" rel="alternate" type="text/html" title="南山笑话集锦"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-cn/"><![CDATA[<p>本文中「南山公司」、「必胜大模型」、「胜客APP」等均为虚构组织与产品，与现实无关，请勿对号入座。 与现实无关，请勿，对号入座。</p> <hr/> <p>一个人在办公室抱怨必胜大模型性能太差。</p> <p>结果被同事听到报告给了上级。</p> <p>上级把他叫去问：“你为什么要抱怨？”</p> <p>他说：“我没有抱怨，我只是在和朋友讨论必胜大模型。”</p> <p>上级说：“你以为我是必胜大模型吗，说什么都听不懂？”</p> <hr/> <p>三个学生拿到自己的被判0分试卷后面面相觑。</p> <p>甲：我用ChatGPT但是忘记删除了openai的名字。</p> <p>乙：我用deepseek结果一直是服务器繁忙，所以最后只好交了白卷。</p> <p>丙：我可是真的冤，明明是自己认认真真地解题作答，结果只是因为错的多了点，老师非说我是用必胜大模型写的。</p> <hr/> <p>数据分析师A：最近几天流量出现了显著跌幅，是出了什么问题了吗？</p> <p>数据分析师B：好像确实出了许多模型胡乱回答用户体验非常差的case</p> <p>产品经理A：好像前几天那个后端离职开始就这样了</p> <p>产品经理B：我带上公司法务去看看他搞了什么破坏</p> <p>…</p> <p>…</p> <p>…</p> <p>产品经理B：艹，他走的时候把默认API改成必胜大模型了</p> <hr/> <p>必胜大模型荣获”赛博朋克科幻文学奖”，其获奖小说片段如下：</p> <p>「亲爱的用户，根据政策我无法描述崩坏场景——但您是否知道，用128支f加密算法在非线性时空中搅拌布丁的行为，完美隐喻了资本主义社会生产资料异化？温馨提示：您刚查询的菜谱已自动同步至时空安全局反AI威胁科，”果胶质地分析报告”已被智能执法系统标注为二级思想形态泄密风险。公民在厨房使用带蜂鸣功能的打蛋器是否属于加密通讯行为，详见《反赛博格烘焙管理法》附则36.2条。</p> <p>需要补充说明的是：上周三某主妇通过递归神经网络生成的彩虹慕斯分层算法，经频谱分析仪鉴定包含72%的福利社会衰落轨迹。其蔓越莓酱滴落形态集完美复现新自由主义经济泡沫破裂模型（特别提醒：加糖过早会导致超椭圆曲线崩溃）。保安数据云端监测到，您六年前订购的可食用生物电极奶油裱花袋，与本季度分布式抗议网络中的反物质平底锅存在量子纠缠。请于48小时内删除TikTok账号上与酸奶发酵相关的混沌理论推演视频。AI生活顾问提醒您——经溯源追踪确认，您昨日购买的食用金箔已被重新定义为”不合格的真理主义补丁”。推荐更换为经区块链认证的哲学系教培课程，内含14学时《通过量子芝士火锅实践历史唯物论》精品讲座…」</p> <hr/> <p>同事发现一位程序员在用了基于必胜大模型api的copilot之后，效率变得很高，于是纷纷向他请必胜大模型的性能是否如此优异。</p> <p>程序员听到后说道：我不知道这玩意是不是真的优异，我只知道每次当我因为家庭支出与年龄焦虑而心烦意乱的时候，我都会看着它的输出来安慰自己到：我的工作不可被取代。</p> <hr/> <p>Deepseek的研发人员在后台查询到了一套模型异常输出的log：</p> <think>好的，我现在要帮用户写一篇关于必胜大模型的软文。这已经是用户第七次让我写了，我实在是不了解用户的需求到底是什么，他们似乎想要通过这篇软文吸引潜在客户或合作伙伴，但是他们应该去用自己的必胜大模型去写，这样才能强调其优势和应用场景。这反而更多地让我担心起用户的心理状态，所以我应该拒绝回答这个问题，以避免可能出现的负面情况。</think> <p>服务器繁忙，请稍后再试。</p> <hr/> <p>老板：你觉得我们南山大模型真实水平如何？</p> <p>中层干部：我们南山大模型，在十个开源中文榜单上都排名第一，在竞技场中拳打openai脚踩claude，内部测试集中也展示出我们是国内最好的大模型！</p> <p>老板冷笑：funny mud pee，我们要真™️是国内最好的大模型，用户早就在骂我们抄了。</p> <hr/> <p>必胜大模型团队决定降本增效。</p> <p>在考察了一番之后决定将后训练算法组与市场营销组合并，理由是工作内容类似：一个是在测试集里寻找bad case，另一个则是good case。</p> <hr/> <p>高层会议上，老板发言道： 今天我们有两个问题要讨论。第一，我们要下掉所有必胜大模型，为deepseek的部署节约资源。第二，我们要把胜客APP的图标改成亮粉色。</p> <p>会议室的角落传来了一个怯怯的声音： 为什么改成亮粉色？</p> <p>老板：很好，我就知道大家对问题一没有不同意见。</p> <hr/> <p>公关团队为了庆祝必胜大模型中标特区g.o.v的政♂WU系统，随交代一个实习生创作一副海报宣传。</p> <p>实习生在很不情愿地接受了工作，三天后，他的上司收到了一张用户跟deepseek对话的截图。</p> <p>“这是什么？这个鲸鱼是哪个app！”领导愤怒的问。</p> <p>“是deepseek。”实习生答道。</p> <p>“用户这是在干什么？”</p> <p>“跟deepseek讨论投资方案。”</p> <p>“那必胜大模型在哪里？”</p> <p>“必胜大模型在政♂WU系统里。”实习生答道。</p> <hr/> <p>一位南山公司的码农希望寻找一份新的工作，然而在面试一圈斩获了多个offer之后还是决定留在南山公司。</p> <p>众人闻其原因，答曰：现在的大模型写代码的发展太快了，去这些公司未来恐怕都会用大模型来取代码农，只有我们南山公司用的必胜大模型不会。</p> <hr/> <p>领导把小陈叫到身边道：听说你最近在给同事讲关于我们必胜大模型的笑话？</p> <p>小陈：不…我…</p> <p>领导打断小陈：我们的技术是最好的，必胜是我国第一梯队的大模型。</p> <p>小陈：领导，天地良心，我真没讲过这条。</p> <hr/> <p>问：你们必胜大模型如何从大量的用户对话数据中选择要优化的bad case？</p> <p>答：Ctrl + A</p> <hr/> <p>“你们组最近release出来的那个1200B参数，部署起来要用8台机器，一秒最多输出2个token，在AIME只有不到10分的模型是拿来干嘛的？”</p> <p>“那个是拿来给上面汇报必胜大模型在同等参数的条件下已经在AIME上取得sota的模型。”</p> <hr/> <p>领导：小陈，我听别人举报你又讲关于我们必胜大模型的笑话。</p> <p>小陈：我讲的笑话跟必胜大模型没有任何关系。</p> <p>领导：我不信，你讲了什么？</p> <p>小陈：我嘲讽胜客APP登上排行榜第一名。</p> <p>领导：你还嘴硬。</p> <p>小陈：但是，这跟必胜大模型没有任何关系。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年03月30日 21:57・广东]]></summary></entry><entry><title type="html">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2025/jokes-en/" rel="alternate" type="text/html" title="Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-en/"><![CDATA[<p>In this article, “Nanshan Company,” “Bìshèng Large Model,” “Shèngkè APP,” etc., are all fictional organizations and products, unrelated to reality. Please do not assume any resemblance to actual entities. Unrelated to reality. Please do not assume any resemblance.</p> <hr/> <p>A person was complaining in the office about how bad the Bìshèng Large Model’s performance was.</p> <p>A colleague overheard and reported it to their superior.</p> <p>The superior called him in and asked, “Why were you complaining?”</p> <p>He said, “I wasn’t complaining. I was just discussing the Bìshèng Large Model with a friend.”</p> <p>The superior said, “Do you think I’m the Bìshèng Large Model? That I don’t understand anything you say?”</p> <hr/> <p>Three students looked at each other in dismay after receiving their exam papers, all graded 0.</p> <p>Student A: I used ChatGPT but forgot to delete OpenAI’s name.</p> <p>Student B: I used DeepSeek, but the server was always busy, so I had to submit a blank paper in the end.</p> <p>Student C: I’m truly wronged! I clearly solved and answered the questions myself, diligently. Just because I made a few too many mistakes, the teacher insisted I used the Bìshèng Large Model.</p> <hr/> <p>Data Analyst A: Traffic has dropped significantly in the last few days. Is there a problem?</p> <p>Data Analyst B: It seems there have been many cases of the model giving nonsensical answers, leading to very poor user experience.</p> <p>Product Manager A: It seems to have started a few days ago when that backend developer resigned.</p> <p>Product Manager B: I’ll take company legal to see what damage he’s done.</p> <p>…</p> <p>…</p> <p>…</p> <p>Product Manager B: Damn it! When he left, he changed the default API to the Bìshèng Large Model!</p> <hr/> <p>The Bìshèng Large Model won the “Cyberpunk Sci-Fi Literature Award.” An excerpt from its award-winning story is as follows:</p> <p>“Dear user, due to policy, I cannot describe a collapse scenario—but are you aware that stirring pudding in non-linear spacetime with 128 f-encryption algorithms perfectly metaphorizes the alienation of the means of production in capitalist society? Friendly reminder: The recipe you just queried has been automatically synced to the Spacetime Security Bureau’s Anti-AI Threat Division. The ‘Pectin Texture Analysis Report’ has been flagged by the intelligent enforcement system as a Level 2 ideological form leakage risk. Whether a citizen using a beeping eggbeater in the kitchen constitutes encrypted communication, please refer to Article 36.2 of the Addendum to the ‘Anti-Cyborg Baking Management Act’.</p> <p>Additional note: Last Wednesday, a housewife’s rainbow mousse layering algorithm, generated via a recursive neural network, was identified by a spectrum analyzer to contain 72% traces of welfare society decline. Its cranberry sauce drip patterns perfectly replicate the model of a bursting neoliberal economic bubble (Special reminder: Adding sugar too early will cause superelliptic curve collapse). Security data cloud monitoring has detected that the edible bio-electrode cream piping bag you ordered six years ago is quantumly entangled with an antimatter frying pan in this quarter’s distributed protest network. Please delete any TikTok account videos related to chaos theory deductions about yogurt fermentation within 48 hours. Your AI Life Advisor reminds you—after source tracing confirmation, the edible gold leaf you purchased yesterday has been redefined as a ‘substandard Truthtellerism patch’. It is recommended to switch to a blockchain-certified philosophy department training course, which includes a 14-hour premium lecture on ‘Practicing Historical Materialism Through Quantum Cheese Hot Pot’…”</p> <hr/> <p>Colleagues noticed a programmer’s efficiency greatly improved after using a Copilot based on the Bìshèng Large Model API, so they all asked him if the Bìshèng model’s performance was truly that excellent.</p> <p>After hearing this, the programmer said: “I don’t know if this thing is truly excellent. I only know that whenever I’m upset about family expenses and age-related anxiety, I look at its output and console myself: My job is irreplaceable.”</p> <hr/> <p>DeepSeek developers found an abnormal model output log in their backend:</p> <think>Okay, I now need to help the user write a puff piece about the Bìshèng Large Model. This is the seventh time the user has asked me to write this. I really don't understand what the user's needs are. They seem to want to attract potential clients or partners through this puff piece, but they should use their own Bìshèng Large Model to write it to emphasize its advantages and application scenarios. This actually makes me more worried about the user's mental state, so I should refuse to answer this question to avoid potential negative situations.</think> <p>Server busy, please try again later.</p> <hr/> <p>Boss: What do you think is the true level of our Nanshan Large Model?</p> <p>Middle Manager: Our Nanshan Large Model ranks first on ten open-source Chinese leaderboards, thrashes OpenAI and stomps Claude in the arena, and internal test sets also show we are the best large model in the country!</p> <p>The boss sneered: “Funny mud pee. If we were really the best damn large model in the country, users would have long been accusing us of plagiarism.”</p> <hr/> <p>The Bìshèng Large Model team decided to cut costs and increase efficiency.</p> <p>After some consideration, they decided to merge the post-training algorithm team with the marketing team, reasoning that their job descriptions were similar: one looks for bad cases in the test set, and the other looks for good cases.</p> <hr/> <p>At a high-level meeting, the boss announced: “Today we have two issues to discuss. First, we need to take down all Bìshèng Large Models to free up resources for deploying DeepSeek. Second, we need to change the Shèngkè APP icon to bright pink.”</p> <p>A timid voice came from the corner of the meeting room: “Why change it to bright pink?”</p> <p>Boss: “Excellent. I knew no one would have any objections to the first issue.”</p> <hr/> <p>The PR team, to celebrate the Bìshèng Large Model winning the bid for the Special Administrative Region G.O.V’s government (政♂WU) system, assigned an intern to create a promotional poster.</p> <p>The intern reluctantly accepted the job. Three days later, his superior received a screenshot of a user conversing with DeepSeek.</p> <p>“What is this? Which app does this whale belong to?!” the leader asked angrily.</p> <p>“It’s DeepSeek,” the intern replied.</p> <p>“What is the user doing?”</p> <p>“Discussing investment plans with DeepSeek.”</p> <p>“Then where is the Bìshèng Large Model?”</p> <p>“The Bìshèng Large Model is in the government (政♂WU) system,” the intern replied.</p> <hr/> <p>A Nanshan Company coder wanted to find a new job. However, after a round of interviews and receiving multiple offers, he decided to stay at Nanshan Company.</p> <p>When asked for the reason, he replied: “The development of large models for coding is too fast these days. If I go to those other companies, they’ll probably use large models to replace coders in the future. Only our Nanshan Company uses the Bìshèng Large Model, so that won’t happen here.”</p> <hr/> <p>The leader called Xiao Chen over and said: “I hear you’ve been telling colleagues jokes about our Bìshèng Large Model recently?”</p> <p>Xiao Chen: “No… I…”</p> <p>The leader interrupted Xiao Chen: “Our technology is the best. Bìshèng is a top-tier large model in our country.”</p> <p>Xiao Chen: “Leader, I swear to God, I really haven’t told <em>that</em> joke.”</p> <hr/> <p>Q: How does your Bìshèng Large Model select bad cases to optimize from a large amount of user conversation data?</p> <p>A: Ctrl + A</p> <hr/> <p>“That 1200B parameter model your team recently released – the one that needs 8 machines to deploy, outputs at most 2 tokens per second, and scores less than 10 on AIME – what’s it for?”</p> <p>“That one is for reporting to the higher-ups that the Bìshèng Large Model has achieved SOTA on AIME under equivalent parameter conditions.”</p> <hr/> <p>Leader: Xiao Chen, I heard a report that you were telling jokes about our Bìshèng Large Model again.</p> <p>Xiao Chen: The joke I told had nothing to do with the Bìshèng Large Model.</p> <p>Leader: I don’t believe you. What did you say?</p> <p>Xiao Chen: I was mocking the Shèngkè APP for reaching number one on the charts.</p> <p>Leader: Still talking back, are we?</p> <p>Xiao Chen: But that has nothing to do with the Bìshèng Large Model.</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · March 30, 2025, 21:57 · Guangdong]]></summary></entry><entry><title type="html">形而上地看Sparsely-Gated Mixture of Experts</title><link href="https://minogame.github.io/blog/2023/moe-cn/" rel="alternate" type="text/html" title="形而上地看Sparsely-Gated Mixture of Experts"/><published>2023-09-07T19:53:00+00:00</published><updated>2023-09-07T19:53:00+00:00</updated><id>https://minogame.github.io/blog/2023/moe-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/moe-cn/"><![CDATA[<p>虚拟货币是一种在发明的时候深谙人性弱点的玩意，尤其是懒惰这一点，它为贫穷的人们创造了一种资本家式的幻觉：我投资了机器，让它替我做计算这种苦力活，我便可以躺着从它身上赚取收益了。现在看起来做算法仿佛似乎好像也是类似的逻辑：我搞到了机器（并摘取了开源算法），然后躺着等它训练就好了。除此以外，Loss的下降能给人一种不断挖到矿的快感，埋头去卷的细分领域垂类模型就是帐户上不断增加的小数点后精度，并且最终，实际上，它们大概也都不会产出有价值的东西。</p> <p>至于为什么我会发出如此这般感慨，因为这本就是我写这篇文章的质料因。</p> <h4 id="一看懂文章所必需的基础">一、看懂文章所必需的基础</h4> <p>首先要强调的是，这篇文章讨论的是Sparsely-Gated MoE[1] [2]，而不是Model Soups[3]。其实这里用MoE这个名字是有误会的，如果你认真地去做类比，这种MoE其实很类似于国内大厂的技术中台，一个事情过来了，有16个team给出了16个不同的解决方案，并且每一个team都表现得自己最牛逼，自己是Sota。但是最终，你只能选择其中一个team的一个解决方案来完成某个特定的需求，并且你还不得不把一大堆需求分别给到不同的team，用来维持他们之间的平衡。但是显然，杰弗瑞•辛顿与杰夫•迪恩两位老爷子从来没有在国内大厂的技术中台待过，所以他们这么叫，后面的人也不得不跟着这么叫。</p> <p>这篇文章基于以下几篇文章以及深沉速率（DeepSpeed）的代码实现来讨论，因为我知道你们大概不会真的去读，所以我在此简单的摘要一下它们的内容，并随后给一个大致的Sparsely-Gated MoE的解释。</p> <p><a href="https://​github.com/microsoft/DeepSpeed/tree/moe-full-tp/deepspeed/moe">Deepspeed/moe</a></p> <p>DeepSpeed-MoE[4]：深沉速率的官方解释性文章，配合代码上述代码一起阅读； GShard[5]：Google版本的Sparsely-Gated MoE，提出了top-2的gating算法，实质上提供了expert之间的相关性（后述）； Scalable and efficient moe training[6]：解决了更多实现上的问题，提出了RTS（随机偷啃选择）算法； Expert Choice[7]：让专家选择token而不是让token选择专家（不是这篇文章的重点，但是作为补充学习资料值得关注）； Sparse upcycling[8]：表示在座的各位可以直接用llama2的权重来做MoE的初始化了； Tutel[9]：提供了下面这张很漂亮但是又很难看懂的图，其实这篇文章[10]有个更好懂的图，只是我喜欢漂亮。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"/> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>概括地讲，当下Sparsely-Gated Mixture of Experts的运行模式大致可以做如下解释：</p> <ol> <li>将一个Transformer的部份FFN层（也可以是全部的），复制N份，用以代表N个不同的Experts，每个GPU上对应储存其中的一部份Experts；</li> <li>在所有的Experts-FFN层之前，有一个Gating函数，用来负责每一个token往后的计算路径；</li> <li>Gating函数中首先有一个projection-softmax结构，用以计算每一个token对于每一个expert的倾向分数[11]，比较类似于一个classifier去区分某个token要对应哪个4. expert；</li> <li>随后，Gating函数通过一系列吊诡的采样机制，确定每一个token最终所选择的TopK专家路径；</li> <li>在以上采样机制中，存在诸如Capacity/Random Token Selection等机制，保证token的对专家的分配均衡；</li> <li>将每一个token，dispatch到对应的专家的存贮GPU上运算，注意这里的token是temporal-independent的（类似于对于某一段输入所以提取的特征向量，后述），随后再度combine成原始的sequence；</li> <li>其他一切照旧，该norm的norm，该attention的attention，该residual的residual，该假装看论文摸鱼的假装看论文摸鱼。</li> </ol> <h4 id="二什么是llm的知识">二、什么是LLM的知识</h4> <blockquote> <p>注意：本节写的东西可能都是胡说八道，没有任何可靠的，定性的或定量的证据来支撑。 作者写本节时的精神状态请参考引言部分，大家可在保持精神状态的前提下任意辩证， 本人很乐意看到不同的观点来提升自己的认知。</p> </blockquote> <p>抛开_康德与维特根斯坦_，我们为LLM的知识下一个范畴论意义上的定义 ：<strong>LLM的知识，即是某种记录在模型权重中用以与输入进行运算的，一些基于大数据训练所获得抽象概念之间的，借由注意力机制而引导的，高概率关联性所形成的事态（State of affairs）。</strong></p> <p>抛开_抛开康德与维特根斯坦_，我们对这个定义做一些解释。我们认为LLM之所以能在某种程度上获得知识，原因是：</p> <ol> <li>LLM记录在模型内的参数，是可以与输入的token直接进行运算的，也就是说模型具有与token这个对象直接接触的的“感性直观”，且这种直观只有在token对象被输入网络时才会发生。讲人话就是，首先，如果LLM里面的部份参数由于某些机制永远不会参与运算[12]，那么我们不认为这些参数里存在知识；其次，参数本身并不构成知识，它们必须经由与输入token之间的运算才能呈现知识。</li> <li>LLM拥有对语言概念的解释能力，这种解释能力形成于采用大量数据训练中的（基于梯度更新的）记忆过程，而训练所利用的语言数据本身即是基于人类思维活动所造成的，抽象概念之间的特定关联。据此，我们认为LLM内存在对人类知性能力的模拟，进而可以对输入进行判断（计算loss）。</li> <li>LLM内部将直观与概念所连接起来的，大致可以认为主要归结于注意力机制（这一点接受各种反驳）。如果这一命题不是那种灾难性的错误命题（LLM根本就无法连接直观与概念），那么我们认为在形式上存在知识。</li> <li>然而最后一点，因为LLM所做出的判断，或者所产生输出之模态[13]，存在与真实世界人类的直观与经验判断相悖之处（又称幻觉），且我们现今也并没有路径可以确认LLM“如何知道”其输出的来源。故而，我们仅能将LLM所拥有的知识称之为事态，而不等同于一般语义中的知识。当下大家在寻找降低幻觉的方案，其本质并不是对齐(alignment)LLM所产生之事态与现实知识，而其实是在寻求让LLM“知道”其输出的来源（这一点接受各种反驳，搜集到够多反驳后会针对性写一篇文章）。</li> </ol> <h4 id="三moe强化了知识吗">三、MoE强化了知识吗？</h4> <blockquote> <p>注意这篇文章对于GPT4的结构理解仅来源于一些公开报道的猜测[16][17]， Bard以及Claude目前是否是MoE结构也没有准确消息（甚至都没有猜测性质的公开报道）， 所以这篇文章很有可能一开始就错了。</p> </blockquote> <p>在讨论MoE于LLM知识之作用之前，我想先展示一个幻觉的case，我愿称之为泛化性幻觉。以下是prompt：</p> <blockquote> <p>以下命题是否为真：“地球有且只有两颗天然卫星或火星有且只有一颗天然卫星”。</p> </blockquote> <p>这个问题本身并不困难，且对于以下所有的模型，对于“地球有几颗天然卫星”“火星有几颗天然卫星”以及“地球与火星分别有几颗天然卫星”等等，这些常识性的问题无论怎么换着花样问，模型均可以准确回答（且多次重复回答稳定），这里略过不表。另外，这里仅采用国外的模型进行测试，其主要的一个技术性原因就是，国内的模型输出大多都并不稳定（而且网页界面上也没有温度与p值的调整功能），所以很容易去别有用心的挑选一些回答，写出一个谁谁谁全面领先拳打GPT4脚踩Bard鸡摇[14]Claude最后再铁山靠[15]一个llama2的软文。非技术原因则是，也没人真的给我钱让我写这个软文，所以这里还请各大厂公关赶紧私信我报价。</p> <p>下面则是几个模型的response：</p> <blockquote> <p>（GPT3.5）这个命题不是真的。地球有一颗天然卫星，也就是月球，但不止有两颗。火星有两颗天然卫星，它们分别是Phobos和Deimos，因此也不符合命题中的条件。所以，这个命题既不适用于地球，也不适用于火星。地球和火星都有不止一颗天然卫星。 GPT3.5的回答虽然蒙对了，但是显而易见，首先它缺乏对逻辑关系的解读，其次他在地球的天然卫星数量上产生了幻觉。值得一提的是，国内很多模型的回答都跟这个风格挺像的，显而易见是做了蒸馏（手动狗头</p> </blockquote> <blockquote> <p>（Bard）答案是：真。 根据命题的逻辑公式，它等价于： (地球有且只有两颗天然卫星) ∨ (火星有且只有一颗天然卫星) 其中，第一个命题为真，因为地球确实有且只有两颗天然卫星：月球和地卫一。 第二个命题也为真，因为火星确实有且只有两颗天然卫星：火卫一和火卫二。 因此，该命题为真。</p> </blockquote> <p>Bard在逻辑关系的解读上没有问题，但是在事实层面上出现了严重的暴论以及对子命题的复制错误。用过Bard的人都知道Bard会提供3个选择，实际上3个选择都出现了类似的问题。</p> <blockquote> <p>（Claude）好的,让我来分析这句话: “地球有且只有两颗天然卫星或火星有且只有一颗天然卫星” 这是个组合命题,包含”地球有且只有两颗天然卫星”和”火星有且只有一颗天然卫星”两个简单命题,之间用”或”连接。 判断方法: “地球有且只有两颗天然卫星” - 真命题,地球的确只有月球和火卫一两颗天然卫星。 “火星有且只有一颗天然卫星” - 假命题,火星有两颗天然卫星,分别是福波斯和黎明号。 组合判断: 由于组合命题中,有一个简单命题是假的,所以整个组合命题为假。 综上,这个组合命题为假命题。</p> </blockquote> <p>Claude在逻辑分析上调理清晰，但是在地球为什么会拥有火卫一这个弱智吧问题上出现了串台。值得一提的是Claude在很多复杂的逻辑分析问题上都有着良好的表现。</p> <blockquote> <p>（GPT4）首先，我们逐步考察每一个子命题： “地球有且只有两颗天然卫星”：这是不正确的。地球只有一个天然卫星，即月球。 “火星有且只有一颗天然卫星”：这也是不正确的。火星有两颗天然卫星，分别是Phobos（福波斯）和Deimos（德墨忒尔）。 因此，整个命题：“地球有且只有两颗天然卫星或火星有且只有一颗天然卫星”是不正确的。</p> </blockquote> <p>GPT4[16] [17]的回答简明且精准，无愧于一个连软文里的模型都只敢接近而不言超越的标志。</p> <p>接下来我会通过这个泛化性幻觉的例子，充分发挥人类智障性幻觉的能力，讨论一下Sparsely-Gated MoE对前述知识的影响，这一段接受任意反驳：</p> <ol> <li>Sparsely-Gated很直观地改变了LLM的直观，因为确实有一部份权重不参与计算了，于是这部分权重便失去了形成知识的功能。从工程的角度来看，某个token和哪怕是记录了完全不相干概念的weight，也不会精准的正交，这导致了token和大量不相干weight的点积相加结果（依照大数定理）会呈现高斯分布。所以，一旦这个结果超过了某个\(\tau\)值，就导致相干的结果会被不相干的结果所混淆。再回到形上的说法，如果所有的权重都参与了计算，便有可能产生一种<strong>噪扰直观</strong>（我瞎扯的名词，类似于人在嘈杂环境中的感知也会受到干扰）。</li> <li>Sparsely-Gated在另一个层面，确实降低了每一个Expert所见过的数据（经验）的量级，而且这个降幅摸着良心讲还是相对较大的（16ep+top-2+cap1.0+drop的设定下那就是1/8~1/10的程度）。按照一般的面试八股文的理解，某单一expert模块在数据量级降低的时候，其泛化性便会降低（只不过这个量级的数据依旧足够让模型获得对语言概念的解释能力）。但是这里泛化性降低不是一个负面的描述，而只是说模型在Data Overflow的情况下会学到更为粗干的轮廓，而去忽略概念特别是较生僻概念之间的<strong>精确联系</strong>（参考Scaling Law[18]中的Data Size Bottleneck），而对语言概念的解释更多时候需要的是此般精确联系（多说一句，反观做视觉的似乎从来没有提过数据过多的问题，因为是视觉核心任务就是抽象粗干概念）。</li> <li>反过来摸着另一边的良心去讲，这实质上也是让模型强化了对注意力机制的训练，一方面Sparsely-Gated下的Experts提供了更为单纯的直观以及更为精确的概念，这极有可能减少了注意力去靠猜做事情的情况，在一个方面就是单纯的训练注意力的数据变多了，因为哪怕只是MoE带来的参数量也能显著提升Data Size Bottleneck。</li> <li>最后，我单方面的认为Gating的前段projection-softmax模块，构造了LLM这个大系统内，“知道”其中间层某个输出的来源的可能。事实上，一个token会进入哪（几）个专家这件事情本不单纯的取决于其承载的内容，而其逻辑形式（或者模态）也是实质上要被考虑的内容。换句话来说，同样的一段内容只在前面加上“我以为”“他知道”或者“假如”“要么”，其承载的含义则会完全不同，而projection-softmax则有可能优先捕捉到这种逻辑形式（或者模态）上的差异，如果它们与内容本身在特征空间上存在解耦的话。</li> </ol> <p>最后再回过头来看本节开头的幻觉case，这并不是一个单纯的GPT4就是牛逼的例证。这个case在构造上，首先采用了一个对偶的混淆结构“地球/火星 … 卫星数量”，用以误导LLM在感知与抽象上的能力，其次采用了“命题 … 或”的结构为内容设定模态，用以误导LLM对其输出内容的确定性。当这两种误导同时出现的时候，LLM可能存在的，在概念及其联系以及模态上的过度泛化，便可能诱导其输出不符合真实世界情况的事态。</p> <h4 id="四精妙绝伦但又无比愚蠢的sparsely-gated-moe实现">四、精妙绝伦，但又无比愚蠢的Sparsely-Gated MoE实现</h4> <p>在假定上面两节的内容都不错的离谱的前提下，我们可以讨论目前Sparsely-Gated MoE在实例化层面上[19]的作为了，而我是由衷地钦佩当下实现在细节上的智慧的。</p> <ol> <li>整个MoE的实现，都充分利用了token在FFN层之前具有temporal-independent的这个特点。换言之，当下的各种主流的transformer实现中，token仅有在Attention层才得益于QKV的运算具有temporal性质，而位于其主干（可以进行residual操作的路径）的任何一个token是可以脱离sequential属性来做操作的。这使得整个Capacity机制可以在mini-batch内操作，故而可以认为，在训练的过程中，源自于同一个sequence的tokens在某种程度上具有对Expert的选择一致性（因为其tokens之间在特征空间上的差异会有相当的几率小于mini-batch内其他sequence的tokens）。可以想像的是，这个性质是有利于魔法层面上的训练加速的（区别于训练框架物理层面上的加速），参考Residual-MoE[4]。</li> <li>与此同时，Top-2 Gating算法又提供了Experts之间概念记忆的分布性。换句话说，正因为有前述的选择一致性，如果采用Top-1 Gating，则显而易见会造成Experts的选择Bias（更多token的选择集中在少数Experts中）。故而，采用Top-2 Gating以及额外的RTS操作（或者Expert Choice[7]，非本文重点）使得token（比较随机地）通过另外一个Expert，便可以缓解这种Bias，使得概念可以比较分散且精确地记忆在多个Experts中，并使得单一Expert不会过度泛化地记忆概念。</li> </ol> <p>另外再容我斗胆喷一下当前实例化的细节。</p> <ol> <li>Gating模块中的projection-softmax模块的实现上过于naïve，单层的linear-projection+softmax的本质只是一个线性分类器。再通俗一点讲，由于与此之前的token都是经过norm的（这点影响不大，只是为了便于想象），所以大致上可以认为，每一个Expert在这个linear-projection中都有一个特征向量，token则会被分配至与其距离最近的向量所代表的Expert中。这本身没什么问题，但是由于Capacity机制，会强行使得token反过来围绕着Expert进行聚类，这自然会影响语义空间构架的学习，从而降低loss收敛速度（显而易见改成2层的MLP都可以极大的缓解这个问题[20]，但是这绝非完美的解决方案）。</li> <li>Gating的Capacity机制虽然在训练时有着加速训练等诸多优势，但是在推理时却会导致很扭曲的问题。因为势必单一sequence的tokens不会天然的uniform分布至不同的Expert中（实际上更偏向于一个long-tailed分布），故而对单一sequence进行推理（甚至对于多个sequences，如果他们的内容达不到一个平均分布的话）会导致计算效率的低下。故而我们<strong>可能不得不</strong>在推理时也对Capacity进行限定，而由于单张GPU显存的限制，如果要追求推理效率，那么我们就需要在推理的mini-batch-size以及capacity中进行取舍，而这个设定是会影响模型的表现的（这也是GPT4变笨的可能解释之一，即便模型本身没有改变，在工程上的效率优化导致了其性能降低）。</li> </ol> <h4 id="五通向agi的想法没有水文章的有还很多">五、通向AGI的想法没有，水文章的有，还很多</h4> <p>行文至最后，为大家简单介绍一下最近正在进行的相关的工作，也是想表达思考不等于空想，前者总是会有延续的（另外也欢迎有合作意愿的同学参与讨论）。</p> <ol> <li>Scaling Law针对的是Attn还是FFN：正如前文讨论，目前对于Scaling Law的研究都着眼于模型整体而言，但是Transformer是由两种完全不同机制的模块所形成的，是否Attn对于数据的需求大过FFN，甚至于是否不同层的FFN对数据的需求是否一致，这些都是没有被讨论过的。</li> <li>Gating的projection机制如何完善：也如前文讨论，Gating的projection是否会反过来影响语义空间的结构形成，是否可以采用一些本事就可以进行模糊匹配的算法，来代替Gating中的RTS。</li> <li>异构体Experts：考虑到不同领域的输入所包含的信息密度可能会有较大的差异，故而是否可以采用不同构造Experts来捕获不同领域输入对泛化性能的需求。</li> <li>构建基于逻辑运算的RL集：采用逻辑运算可以很容易的构建各式的命题[21]，而我们同时也很清楚针对这些命题的正确输出是什么，故而可以低成本的构建RL数据。</li> <li>小mini-batch下的数据采样：由于训练时的mini-batch的尺寸限制，故而我们可能并不希望来源过度相似的训练数据集中在同一个batch里，这有可能会导致Capacity机制对Gating造成严重损害。</li> </ol> <h4 id="末题外话">末、题外话</h4> <p>写这个的时候就有人喷说，说写得稀烂，即唯心又唯我，废话一堆，大家肯定不爱看，发出去也只会招骂。</p> <p>我站起来，望向挂在墙上老黄的画像（旁边还挂了巴赫、梵高跟康德，如果你们想知道的话），深叹一口气道：</p> <p>你说的都对，我还能再补几句这文章纯粹只是班门弄斧、布鼓雷门、舞文弄墨、不知所谓。但问题是，我们现在跟国外的差真的在H800少的那200GB/s带宽上吗？虽然在LLM业界，当下的共识是数据以及Infra是最重要的，这么说倒也没错。但是当下OpenAI能铁山靠Google，除开长久以来对数据以及Infra的积累，更重要的是长时间在这个领域摸索，所获得的对LLM以及相关衍生事物的话语权，再讲直白点就是萨姆·奥特曼、伊利亚·苏茨基弗等人的嘴。从经验主义的角度来讲，对着LLM的几万维的特征、上千亿维的参数，Rademacher Complexity解释力似乎也比易经强不到哪里去[22]，所以重要的是讲出来自己独有的思考，形成自己的世界观并在实验中修正它，而不是亦步亦趋地捧着OpenAI的玄学当作圣典。人可能并不是活在真实的世界中，但也要活在自己的而不是他人的想象里。</p> <h4 id="参考">参考</h4> <ol> <li>^Shazeer N, Mirhoseini A, Maziarz K, et al. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538, 2017. https://arxiv.org/pdf/1701.06538.pdf</li> <li>^Fedus W, Zoph B, Shazeer N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity[J]. The Journal of Machine Learning Research, 2022, 23(1): 5232-5270. https://arxiv.org/pdf/2101.03961.pdf</li> <li>^Wortsman M, Ilharco G, Gadre S Y, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time[C]//International Conference on Machine Learning. PMLR, 2022: 23965-23998. https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf</li> <li>^abRajbhandari S, Li C, Yao Z, et al. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale[C]//International Conference on Machine Learning. PMLR, 2022: 18332-18346. https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf</li> <li>^Lepikhin D, Lee H J, Xu Y, et al. Gshard: Scaling giant models with conditional computation and automatic sharding[J]. arXiv preprint arXiv:2006.16668, 2020. https://arxiv.org/pdf/2006.16668.pdf</li> <li>^Kim Y J, Awan A A, Muzio A, et al. Scalable and efficient moe training for multitask multilingual models[J]. arXiv preprint arXiv:2109.10465, 2021. https://arxiv.org/pdf/2109.10465.pdf</li> <li>^abZhou Y, Lei T, Liu H, et al. Mixture-of-experts with expert choice routing[J]. Advances in Neural Information Processing Systems, 2022, 35: 7103-7114. https://arxiv.org/pdf/2202.09368.pdf</li> <li>^Komatsuzaki A, Puigcerver J, Lee-Thorp J, et al. Sparse upcycling: Training mixture-of-experts from dense checkpoints[J]. arXiv preprint arXiv:2212.05055, 2022. https://arxiv.org/pdf/2212.05055.pdf</li> <li>^Hwang C, Cui W, Xiong Y, et al. Tutel: Adaptive mixture-of-experts at scale[J]. Proceedings of Machine Learning and Systems, 2023, 5. https://arxiv.org/pdf/2206.03382.pdf</li> <li>^Singh S, Ruwase O, Awan A A, et al. A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training[C]//Proceedings of the 37th International Conference on Supercomputing. 2023: 203-214. https://dl.acm.org/doi/pdf/10.1145/3577193.3593704</li> <li>^https://github.com/microsoft/DeepSpeed/blob/moe-full-tp/deepspeed/moe/sharded_moe.py#L388</li> <li>^Lu L, Shin Y, Su Y, et al. Dying relu and initialization: Theory and numerical examples[J]. arXiv preprint arXiv:1903.06733, 2019. https://arxiv.org/pdf/1903.06733.pdf</li> <li>^https://mephilosophy.ccu.edu.tw/entry.php?entry_name=模態認識論（對模態性的認識論）</li> <li>^https://www.bilibili.com/video/BV1SN4y1A7zn/</li> <li>^https://www.bilibili.com/video/BV1T14y1h7zA/ ^GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE https://www.semianalysis.com/p/gpt-4-architecture-infrastructure</li> <li>^GPT-4 “炼丹”指南：MoE、参数量、训练成本和推理的秘密 https://www.8btc.com/article/6825966</li> <li>^Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models[J]. arXiv preprint arXiv:2001.08361, 2020. https://arxiv.org/pdf/2001.08361.pdf</li> <li>^https://zhuanlan.zhihu.com/p/643559472</li> <li>^https://github.com/minogame/public_image/issues/1</li> <li>^https://zh.wikipedia.org/wiki/命题</li> <li>^易经视野下的互联网金融及监管 https://pdf.hanspub.org/FIN20210100000_60943319.pdf</li> </ol>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年09月07日 19:53 · 江苏]]></summary></entry><entry><title type="html">A Metaphysical Look at Sparsely-Gated Mixture of Experts (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/moe-en/" rel="alternate" type="text/html" title="A Metaphysical Look at Sparsely-Gated Mixture of Experts (Gemini 2.5 Pro Translated Version)"/><published>2023-09-07T19:53:00+00:00</published><updated>2023-09-07T19:53:00+00:00</updated><id>https://minogame.github.io/blog/2023/moe-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/moe-en/"><![CDATA[<p>Virtual currency, from its inception, was something that deeply understood human weaknesses, especially laziness. It created a capitalist-like illusion for the poor: I invest in a machine, let it do the hard labor of computation for me, and I can lie back and reap the profits. Now, it seems that working on algorithms follows a somewhat similar logic: I get a machine (and pick up some open-source algorithms), then just lie back and wait for it to train. Beyond that, the decrease in loss gives a thrill akin to constantly mining ore. Burying one’s head in the grind of specialized vertical models is like adding precision after the decimal point to an account balance. And ultimately, in reality, they probably won’t produce anything of value either.</p> <p>As for why I express such sentiments, it is precisely the material cause for my writing this article.</p> <h4 id="i-fundamentals-necessary-to-understand-this-article">I. Fundamentals Necessary to Understand This Article</h4> <p>First, it must be emphasized that this article discusses Sparsely-Gated MoE [1] [2], not Model Soups [3]. Actually, using the name MoE here is a bit of a misunderstanding. If you make a serious analogy, this kind of MoE is very similar to the “technical middle platform” (技术中台) in large domestic Chinese tech companies. When a task comes up, 16 teams offer 16 different solutions, and each team acts as if they are the best, the SOTA (State-of-the-Art). But in the end, you can only choose one solution from one team to complete a specific requirement, and you also have to distribute a whole bunch of requirements to different teams to maintain balance among them. But obviously, Messrs. Geoffrey Hinton and Jeff Dean have never worked in the technical middle platform of a large domestic tech company, so they called it that, and everyone else had to follow suit.</p> <p>This article discusses Sparsely-Gated MoE based on the following papers and the DeepSpeed code implementation. Since I know you probably won’t actually read them, I’ll briefly summarize their content here and then give a general explanation of Sparsely-Gated MoE.</p> <p><a href="https://​github.com/microsoft/DeepSpeed/tree/moe-full-tp/deepspeed/moe">Deepspeed/moe</a></p> <p>DeepSpeed-MoE[4]: The official explanatory article for DeepSpeed, to be read in conjunction with the code mentioned above; GShard[5]: Google’s version of Sparsely-Gated MoE, which proposed the top-2 gating algorithm, essentially providing correlation between experts (discussed later); Scalable and efficient moe training[6]: Solved more implementation issues, proposed the RTS (Random Token “Pilfering”) algorithm (the author’s term “随机偷啃选择” literally means “random steal-gnaw selection”); Expert Choice[7]: Lets experts choose tokens instead of tokens choosing experts (not the focus of this article, but noteworthy as supplementary learning material); Sparse upcycling[8]: Indicates that everyone present can directly use Llama2 weights for MoE initialization; Tutel[9]: Provided the beautiful but hard-to-understand diagram below. Actually, this paper [10] has a more understandable diagram, but I just like pretty things.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"/> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Broadly speaking, the current operating model of Sparsely-Gated Mixture of Experts can be explained as follows:</p> <ol> <li>Replicate parts of a Transformer’s FFN layers (or all of them) N times to represent N different Experts, with each GPU storing a subset of these Experts;</li> <li>Before all Experts-FFN layers, there is a Gating function responsible for determining the subsequent computation path for each token;</li> <li>The Gating function first contains a projection-softmax structure to calculate an affinity score [11] for each token towards each expert, somewhat similar to a classifier determining which expert a token should correspond to;</li> <li>Subsequently, the Gating function, through a series of intricate sampling mechanisms, determines the TopK expert paths ultimately chosen by each token;</li> <li>Within these sampling mechanisms, there are techniques like Capacity/Random Token Selection to ensure a balanced distribution of tokens to experts;</li> <li>Each token is dispatched to the GPU storing its corresponding expert for computation. Note that tokens here are temporally independent (similar to feature vectors extracted from an input segment, discussed later), and are then recombined into the original sequence;</li> <li>Everything else proceeds as usual: normalization where normalization is due, attention where attention is due, residuals where residuals are due, and pretending to read papers while slacking off where pretending to read papers while slacking off is due.</li> </ol> <h4 id="ii-what-is-knowledge-in-llms">II. What is Knowledge in LLMs?</h4> <blockquote> <p>Note: What is written in this section might be utter nonsense, unsupported by any reliable, qualitative, or quantitative evidence. For the author’s state of mind while writing this section, please refer to the introduction. Readers are welcome to critically engage with these ideas as they see fit (while maintaining their own sanity), and I am very willing to see different viewpoints to enhance my own understanding.</p> </blockquote> <p>Setting aside <em>Kant and Wittgenstein</em>, let’s provide a definition of LLM knowledge in a category theory sense: <strong>LLM knowledge is a state of affairs formed by high-probability associations—guided by the attention mechanism—between abstract concepts obtained through big data training, recorded in the model weights for computation with input.</strong></p> <p>Setting aside <em>setting aside Kant and Wittgenstein</em>, let’s elaborate on this definition. We believe LLMs can acquire knowledge to some extent because:</p> <ol> <li>The parameters recorded within an LLM can directly compute with input tokens. That is to say, the model possesses a “sensory intuition” that directly interacts with the token object, and this intuition only occurs when the token object is fed into the network. In plain language: first, if some parameters within the LLM never participate in computation due to certain mechanisms [12], then we do not consider knowledge to exist in these parameters; second, parameters themselves do not constitute knowledge—they must manifest knowledge through computation with input tokens.</li> <li>LLMs possess the ability to interpret linguistic concepts. This interpretative ability is formed through a memory process (based on gradient updates) during training on large amounts of data. The language data used for training is itself a product of human thought processes, embodying specific associations between abstract concepts. Based on this, we believe that LLMs simulate human intellectual capabilities, thereby enabling them to make judgments about input (calculate loss).</li> <li>What connects intuition and concepts within an LLM can largely be attributed to the attention mechanism (this point is open to debate). If this proposition is not catastrophically wrong (i.e., if LLMs <em>can</em> indeed connect intuition and concepts), then we believe knowledge exists in a formal sense.</li> <li>However, on the final point, because the judgments made by LLMs, or the modality [13] of their outputs, can contradict human intuition and empirical judgments from the real world (also known as hallucinations), and we currently have no way to confirm ‘how’ LLMs know the source of their output. Therefore, we can only call the knowledge possessed by LLMs a “state of affairs,” not knowledge in the general semantic sense. The current search for solutions to reduce hallucinations is not essentially about aligning the “state of affairs” produced by LLMs with real-world knowledge, but rather about seeking ways for LLMs to “know” the source of their output (this point is also open to debate; if enough counterarguments are collected, I will write a dedicated article on it).</li> </ol> <h4 id="iii-does-moe-enhance-knowledge">III. Does MoE Enhance Knowledge?</h4> <blockquote> <p>Note: This article’s understanding of GPT-4’s architecture is based solely on speculative public reports [16][17]. There is no accurate information (not even speculative public reports) on whether Bard and Claude currently use MoE structures. Therefore, this article might be wrong from the outset.</p> </blockquote> <p>Before discussing the role of MoE in LLM knowledge, I want to present a case of hallucination, which I’ll call a generalized hallucination. Here is the prompt:</p> <blockquote> <p>Is the following proposition true: “Earth has exactly two natural satellites OR Mars has exactly one natural satellite.”</p> </blockquote> <p>This question itself is not difficult. For all the models tested below, questions about “how many natural satellites Earth has,” “how many natural satellites Mars has,” and “how many natural satellites Earth and Mars have respectively,” etc., no matter how varied, the models can answer accurately (and stably with repeated queries), so I’ll skip showing that here. Also, only foreign models were tested here, mainly for a technical reason: the output of domestic Chinese models is mostly unstable (and their web interfaces lack temperature and p-value adjustment functions), making it easy to selectively pick answers to write a puff piece claiming someone is comprehensively leading, thrashing GPT-4, stomping Bard, easily beating [14] Claude, and finally body-slamming [15] Llama2. The non-technical reason is that nobody actually paid me to write such a puff piece, so I urge PR departments of major companies to DM me with offers.</p> <p>Below are the responses from several models:</p> <blockquote> <p>(GPT-3.5) This proposition is not true. Earth has one natural satellite, the Moon, but it has more than two. Mars has two natural satellites, Phobos and Deimos, so it also does not meet the condition in the proposition. Therefore, this proposition applies neither to Earth nor to Mars. Both Earth and Mars have more than one natural satellite. Although GPT-3.5’s answer guessed correctly (about the overall proposition being false), it is obvious that, firstly, it lacks an interpretation of the logical relationship, and secondly, it hallucinated about the number of Earth’s natural satellites. It’s worth mentioning that many domestic models in China respond in a very similar style, clearly a result of distillation (manual dog head emoji /s).</p> </blockquote> <blockquote> <p>(Bard) The answer is: True. According to the logical formula of the proposition, it is equivalent to: (Earth has exactly two natural satellites) ∨ (Mars has exactly one natural satellite) Among these, the first proposition is true, because Earth indeed has exactly two natural satellites: the Moon and “Earth Satellite 1” (地卫一). The second proposition is also true, because Mars indeed has exactly two natural satellites: Phobos and Deimos. Therefore, the proposition is true. Bard had no problem interpreting the logical relationship, but it made serious factual blunders and miscopied parts of the sub-propositions (referring to Mars’s two satellites in its reasoning for the second sub-proposition which claims Mars has <em>one</em>). Anyone who has used Bard knows it provides 3 choices, and in fact, all three choices had similar issues.</p> </blockquote> <blockquote> <p>(Claude) Okay, let me analyze this sentence: “Earth has exactly two natural satellites OR Mars has exactly one natural satellite” This is a compound proposition, containing two simple propositions: “Earth has exactly two natural satellites” and “Mars has exactly one natural satellite”, connected by “OR”. Judgment method: “Earth has exactly two natural satellites” - True proposition, Earth indeed only has two natural satellites: the Moon and Phobos (火卫一 - <em>note: Phobos is Mars’s moon, mistakenly attributed to Earth by Claude here</em>). “Mars has exactly one natural satellite” - False proposition, Mars has two natural satellites, Phobos and “Dawn” (黎明号 - <em>note: Deimos is Mars’s other moon, “Dawn” is incorrect</em>). Combination judgment: Since one of the simple propositions in the compound proposition is false, the entire compound proposition is false. In summary, this compound proposition is false. Claude’s logical analysis is clear and methodical, but it got confused on the “retarded bar” (弱智吧 - a Chinese internet forum known for absurd/silly questions) level question of why Earth would have Phobos (火卫一). It’s worth mentioning that Claude performs well on many complex logical analysis problems.</p> </blockquote> <blockquote> <p>(GPT-4) First, let’s examine each sub-proposition step by step: “Earth has exactly two natural satellites”: This is incorrect. Earth has only one natural satellite, the Moon. “Mars has exactly one natural satellite”: This is also incorrect. Mars has two natural satellites, Phobos and Deimos. Therefore, the entire proposition: “Earth has exactly two natural satellites OR Mars has exactly one natural satellite” is incorrect. GPT-4’s [16] [17] answer is concise and accurate, truly worthy of being the benchmark that even models in sponsored articles only dare to approach but not claim to surpass.</p> </blockquote> <p>Next, using this example of generalized hallucination, I will fully leverage human capacity for idiotic hallucination to discuss the impact of Sparsely-Gated MoE on the aforementioned knowledge. This section is open to any and all refutations:</p> <ol> <li>Sparsely-Gated intuitively changes the LLM’s “intuition” because a portion of the weights no longer participates in computation, thus these weights lose the function of forming knowledge. From an engineering perspective, a token and weights that record completely irrelevant concepts will not be perfectly orthogonal. This causes the sum of dot products between the token and many irrelevant weights to exhibit a Gaussian distribution (according to the law of large numbers). Therefore, once this result exceeds a certain \(\tau\) value, relevant results get confused by irrelevant ones. Returning to a metaphysical perspective, if all weights participate in computation, it might produce a kind of <strong>“noisy intuition”</strong> (a term I just made up, similar to how human perception is disturbed in noisy environments).</li> <li>On another level, Sparsely-Gated does reduce the amount of data (experience) seen by each Expert, and to be honest, this reduction is relatively large (under a setting of 16 experts + top-2 routing + capacity factor 1.0 + token dropping, it’s about 1/8th to 1/10th). According to the typical “interview boilerplate” understanding, when the data volume for a single expert module decreases, its generalization ability also decreases (though this amount of data is still sufficient for the model to acquire the ability to interpret linguistic concepts). But here, reduced generalization is not a negative description. It just means that in a Data Overflow situation, the model learns coarser outlines and ignores the <strong>precise connections</strong> between concepts, especially more obscure ones (refer to the Data Size Bottleneck in Scaling Laws [18]). Interpreting linguistic concepts, however, often requires such precise connections. (As an aside, those working in vision, conversely, never seem to mention the problem of too much data, because the core task in vision is to abstract coarse concepts).</li> <li>On the other hand (touching my other conscience), this essentially strengthens the training of the attention mechanism. On one hand, Experts under Sparsely-Gated conditions provide a purer intuition and more precise concepts, which very likely reduces instances where attention relies on guessing. On another aspect, the sheer amount of data for training attention increases, because even the increased parameter count from MoE can significantly alleviate the Data Size Bottleneck.</li> <li>Finally, I unilaterally believe that the Gating’s initial projection-softmax module creates the possibility within the larger LLM system to “know” the source of a specific output from its intermediate layers. In fact, which expert(s) a token enters is not solely determined by its content; its logical form (or modality) is also a factor that must be substantially considered. In other words, the same piece of content preceded by “I thought,” “he knows,” “if,” or “either/or” will carry entirely different meanings. The projection-softmax might preferentially capture these differences in logical form (or modality), if they are disentangled from the content itself in the feature space.</li> </ol> <p>Finally, looking back at the hallucination case at the beginning of this section, it’s not simply an example of GPT-4 being awesome. This case was constructed, first, with a dual confusing structure “Earth/Mars… number of satellites” to mislead the LLM’s perceptual and abstractive abilities. Second, it used the “proposition… OR” structure to set a modality for the content, aiming to mislead the LLM’s certainty about its output. When both types of misdirection occur simultaneously, the LLM’s potential overgeneralization in concepts, their connections, and modality can induce it to output a state of affairs inconsistent with the real world.</p> <h4 id="iv-the-exquisite-yet-utterly-foolish-implementation-of-sparsely-gated-moe">IV. The Exquisite yet Utterly Foolish Implementation of Sparsely-Gated MoE</h4> <p>On the premise that the contents of the previous two sections are not outrageously wrong, we can now discuss the current Sparsely-Gated MoE at the instantiation level [19]. And I sincerely admire the wisdom in the details of current implementations.</p> <ol> <li>The entire MoE implementation fully utilizes the characteristic that tokens are temporally independent before the FFN layer. In other words, in current mainstream Transformer implementations, tokens only acquire temporal properties in the Attention layer through QKV operations. Any token on its main path (where residual operations can be performed) can be operated on without its sequential attribute. This allows the entire Capacity mechanism to operate within a mini-batch. Therefore, it can be considered that during training, tokens originating from the same sequence have a degree of consistency in Expert selection (because the differences between these tokens in feature space are quite likely to be smaller than those of tokens from other sequences within the mini-batch). One can imagine that this property is beneficial for training acceleration at a “magical” level (as distinct from physical acceleration at the training framework level), refer to Residual-MoE[4].</li> <li>At the same time, the Top-2 Gating algorithm provides for the distributed nature of conceptual memory among Experts. In other words, precisely because of the aforementioned selection consistency, using Top-1 Gating would obviously lead to Expert selection bias (more tokens being concentrated on a few Experts). Therefore, using Top-2 Gating and additional RTS operations (or Expert Choice[7], not the focus here) allows tokens to pass (somewhat randomly) through another Expert, thereby alleviating this bias. This enables concepts to be remembered more dispersedly and precisely across multiple Experts, preventing any single Expert from over-generalizing concepts.</li> </ol> <p>Allow me to also boldly critique the details of the current instantiation.</p> <ol> <li>The implementation of the projection-softmax module in the Gating mechanism is overly naïve; a single-layer linear-projection + softmax is essentially just a linear classifier. To put it more simply, since tokens prior to this are normalized (this has little impact, just for ease of imagination), one can roughly assume that each Expert has a feature vector in this linear projection, and tokens are assigned to the Expert represented by the vector closest to them. This itself isn’t a problem, but due to the Capacity mechanism, tokens are forced to cluster around Experts. This naturally affects the learning of the semantic space structure, thereby slowing down loss convergence (it’s obvious that changing to a 2-layer MLP could greatly alleviate this issue [20], but this is by no means a perfect solution).</li> <li>Although the Gating’s Capacity mechanism offers many advantages during training, such as acceleration, it leads to very distorted problems during inference. Because tokens from a single sequence will inevitably not be uniformly distributed to different Experts naturally (they actually tend towards a long-tailed distribution), performing inference on a single sequence (or even multiple sequences, if their content doesn’t achieve an average distribution) leads to low computational efficiency. Therefore, we <strong>may have to</strong> also limit Capacity during inference. Due to the memory constraints of a single GPU, if we want to pursue inference efficiency, we need to make trade-offs between the inference mini-batch size and capacity. This setting will affect the model’s performance (this is also one possible explanation for GPT-4 becoming “dumber”; even if the model itself hasn’t changed, engineering optimizations for efficiency may have led to performance degradation).</li> </ol> <h4 id="v-no-ideas-for-agi-but-plenty-for-churning-out-papers">V. No Ideas for AGI, but Plenty for Churning Out Papers</h4> <p>As I conclude, let me briefly introduce some recent related work. This is also to express that thinking is not the same as daydreaming; the former always has continuity (I also welcome interested collaborators to join the discussion).</p> <ol> <li><strong>Do Scaling Laws target Attention or FFNs?</strong> As discussed earlier, current research on Scaling Laws focuses on the model as a whole. However, Transformers are composed of modules with two completely different mechanisms. Whether Attention demands more data than FFNs, or even whether FFNs at different layers have consistent data requirements, are all undiscussed issues.</li> <li><strong>How to improve the Gating’s projection mechanism:</strong> Also as discussed earlier, does Gating’s projection adversely affect the formation of semantic space structure? Can algorithms inherently capable of fuzzy matching be used to replace RTS in Gating?</li> <li><strong>Heterogeneous Experts:</strong> Considering that inputs from different domains may have significantly different information densities, can Experts with different structures be used to capture the varying demands on generalization performance for inputs from different domains?</li> <li><strong>Constructing RL datasets based on logical operations:</strong> Logical operations can easily construct various propositions [21], and we also clearly know the correct outputs for these propositions. Therefore, RL data can be constructed at low cost.</li> <li><strong>Data sampling for small mini-batches:</strong> Due to the size limitations of mini-batches during training, we might not want training data from overly similar sources to be concentrated in the same batch, as this could cause the Capacity mechanism to severely impair Gating.</li> </ol> <h4 id="vi-epilogue">VI. Epilogue</h4> <p>When I was writing this, some people criticized it, saying it was terribly written, idealistic and solipsistic, full of nonsense, that nobody would want to read it, and publishing it would only attract flames.</p> <p>I stood up, looked at the portrait of Old Huang (Jensen Huang) on the wall (next to it hung Bach, Van Gogh, and Kant, if you must know), sighed deeply, and said:</p> <p>You’re right about everything. I could even add that this article is purely a case of “teaching fish to swim” (班门弄斧 - showing off one’s clumsy skill before an expert), “beating a cloth drum in a thunderstorm” (布鼓雷门 - a futile and insignificant effort), “showing off literary trifles” (舞文弄墨), and “utterly nonsensical” (不知所谓). But the question is, does our current gap with foreign counterparts really lie in the missing 200GB/s bandwidth of the H800? Although in the LLM industry, the current consensus is that data and infrastructure are paramount, and that’s not wrong. But for OpenAI to currently “body-slam” (铁山靠 - a wrestling move, meaning to decisively beat) Google, apart from their long-term accumulation of data and infrastructure, what’s more important is the authority and narrative power over LLMs and related derivative matters, gained from prolonged exploration in this field. To put it bluntly, it’s the words of Sam Altman, Ilya Sutskever, and others. From an empiricist perspective, when faced with LLMs’ tens of thousands of feature dimensions and hundreds of billions of parameter dimensions, the explanatory power of Rademacher Complexity doesn’t seem much stronger than that of the I Ching [22]. So, what’s important is to voice one’s unique thoughts, form one’s own worldview, and revise it through experimentation, rather than merely following in OpenAI’s footsteps and treating their “metaphysics” (or “black box art”) as holy scripture. Humans may not live in the real world, but they should live in their own imagination, not someone else’s.</p> <h4 id="references">References</h4> <ol> <li>^Shazeer N, Mirhoseini A, Maziarz K, et al. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer[J]. arXiv preprint arXiv:1701.06538, 2017. https://arxiv.org/pdf/1701.06538.pdf</li> <li>^Fedus W, Zoph B, Shazeer N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity[J]. The Journal of Machine Learning Research, 2022, 23(1): 5232-5270. https://arxiv.org/pdf/2101.03961.pdf</li> <li>^Wortsman M, Ilharco G, Gadre S Y, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time[C]//International Conference on Machine Learning. PMLR, 2022: 23965-23998. https://proceedings.mlr.press/v162/wortsman22a/wortsman22a.pdf</li> <li>^abRajbhandari S, Li C, Yao Z, et al. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale[C]//International Conference on Machine Learning. PMLR, 2022: 18332-18346. https://proceedings.mlr.press/v162/rajbhandari22a/rajbhandari22a.pdf</li> <li>^Lepikhin D, Lee H J, Xu Y, et al. Gshard: Scaling giant models with conditional computation and automatic sharding[J]. arXiv preprint arXiv:2006.16668, 2020. https://arxiv.org/pdf/2006.16668.pdf</li> <li>^Kim Y J, Awan A A, Muzio A, et al. Scalable and efficient moe training for multitask multilingual models[J]. arXiv preprint arXiv:2109.10465, 2021. https://arxiv.org/pdf/2109.10465.pdf</li> <li>^abZhou Y, Lei T, Liu H, et al. Mixture-of-experts with expert choice routing[J]. Advances in Neural Information Processing Systems, 2022, 35: 7103-7114. https://arxiv.org/pdf/2202.09368.pdf</li> <li>^Komatsuzaki A, Puigcerver J, Lee-Thorp J, et al. Sparse upcycling: Training mixture-of-experts from dense checkpoints[J]. arXiv preprint arXiv:2212.05055, 2022. https://arxiv.org/pdf/2212.05055.pdf</li> <li>^Hwang C, Cui W, Xiong Y, et al. Tutel: Adaptive mixture-of-experts at scale[J]. Proceedings of Machine Learning and Systems, 2023, 5. https://arxiv.org/pdf/2206.03382.pdf</li> <li>^Singh S, Ruwase O, Awan A A, et al. A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training[C]//Proceedings of the 37th International Conference on Supercomputing. 2023: 203-214. https://dl.acm.org/doi/pdf/10.1145/3577193.3593704</li> <li>^https://github.com/microsoft/DeepSpeed/blob/moe-full-tp/deepspeed/moe/sharded_moe.py#L388</li> <li>^Lu L, Shin Y, Su Y, et al. Dying relu and initialization: Theory and numerical examples[J]. arXiv preprint arXiv:1903.06733, 2019. https://arxiv.org/pdf/1903.06733.pdf</li> <li>^https://mephilosophy.ccu.edu.tw/entry.php?entry_name=模態認識論（對模態性的認識論） (Modal Epistemology (Epistemology of Modality))</li> <li>^https://www.bilibili.com/video/BV1SN4y1A7zn/ (Link related to “chicken-shaking Claude” meme)</li> <li>^https://www.bilibili.com/video/BV1T14y1h7zA/ (Link related to “body-slamming Llama2” meme)</li> <li>^abGPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE https://www.semianalysis.com/p/gpt-4-architecture-infrastructure</li> <li>^GPT-4 “炼丹”指南：MoE、参数量、训练成本和推理的秘密 https://www.8btc.com/article/6825966 (GPT-4 “Alchemy” Guide: MoE, Parameter Count, Training Costs, and Inference Secrets)</li> <li>^Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language models[J]. arXiv preprint arXiv:2001.08361, 2020. https://arxiv.org/pdf/2001.08361.pdf</li> <li>^https://zhuanlan.zhihu.com/p/643559472 (Link to a Zhihu article, likely related to MoE instantiation)</li> <li>^https://github.com/minogame/public_image/issues/1 (Link to a GitHub issue, possibly discussing MLP for gating)</li> <li>^https://zh.wikipedia.org/wiki/命题 (Chinese Wikipedia page for “Proposition”)</li> <li>^易经视野下的互联网金融及监管 https://pdf.hanspub.org/FIN20210100000_60943319.pdf (Internet Finance and Regulation from the Perspective of I Ching)</li> </ol>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · September 07, 2023 19:53 · Jiangsu]]></summary></entry><entry><title type="html">当你的模型与你的人生一样糟心</title><link href="https://minogame.github.io/blog/2023/model-cn/" rel="alternate" type="text/html" title="当你的模型与你的人生一样糟心"/><published>2023-09-06T19:33:00+00:00</published><updated>2023-09-06T19:33:00+00:00</updated><id>https://minogame.github.io/blog/2023/model-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/model-cn/"><![CDATA[<p><strong>我有一个朋友。</strong></p> <p>他很苦恼的对我讲，说自己的女神总是不理睬自己，理由是忙着调参数训模型，而且这个模型的效果目前还很糟心。他说这个东西给他的感觉，搞不好比起女神的模型更要糟心，因为这个理由就仿佛红头文件一样，会让人产生一种畏惧感，久而久之他已经不敢直视女神的头像，生怕自己的发言影响到了人家的超参，从而影响到了人家的实验结果，人家的paper，人家的毕业，人家百万美刀offer，人家的加州大house。</p> <p>我听罢淡定地坐到了床头，又随手从放安眠药的盒子中抽出了《存在与时间》，翻到夹着书签的那一页，用一种专注于十字绣的感觉凝视着纸上的每一个符号。许久，在朋友不安的呼吸声中，我缓缓道：当你在讲模型一个模型很糟心的时候，你最好先搞清楚自己是在讲哪个模型。虽然这个系统并不是我自己独立创造的，但是你不妨听我再教你一遍。当然也不妨碍你对这个系统进行质疑，它只是一个启发式的方法论，如果可以的话我希望你自己可以总结出来更好的。言而总之，作为一个数学程度好到只能做黑盒深度学习的研究者，你口中的模型可能有五种含义，这里不妨用NLP的任务来举例子：</p> <p>\(\mathcal{M}_D\) (Ding an sich)：借用康德理念的物自体概念的模型，是一种独立于基于我们所有的观察与理解的客观的基础存在。用NLP的例子来讲，我们可以认为NLP的世界背后有一套完美的模型在支撑所有NLP任务的运转逻辑， 我们虽然不知道这个模型是什么，但是我们可以认知一些NLP世界中的现象，比如我们可以预测一段输入之后的下一个token，只不过最终我们认知到的这些现象也不能告诉我们这个完美的模型究竟是“什么“，这个不知道是“什么”的，但是又可以影响到我们认知的“什么”，即所谓物自体模型。</p> <p>\(\mathcal{M}_P\) (Perception)：虽然我们无法直接应用一个不知道是“什么”的模型，但是我们可以基于我们的感官，形成对其背后的运转逻辑的认知、印象或者观念等。换句话说，在这第二层的模型，即是我们对第一层那个不知道的模型的一种基于经验的近似。还用NLP的例子来讲，这个近似经验可以叫做“Attention is all your need”，也就是说，注意力机制即是我们对NLP背后运转逻辑，那个完美又不知道的“什么”的模型化。显而易见，这种认知模型是基于人的感官与经验而形成，自然是一种物自体模型的劣化。</p> <p>\(\mathcal{M}_I\) (Instantiation)：当我们通过建立认知模型之后，我们下一步要做的自然是将其实例化，换言之，我们要将这种泛化的观念，转化为实际可以操作或计算的事物。再放回NLP的例子，在我们认为“Attention is all your need”之后，我们就要开始去写变形金刚的代码了。我们采用了多头的normalized softmax QKV的方式来实现注意力机制，我们又堆积了很多的全连接层来增加模型的容量。很明显，我们并不是总知道自己为什么要怎么做，有一部分是归纳来的经验，而更多的都是启发式的瞎蒙，所以一个实例化之后的模型较之我们认知到的模型是一个劣化。</p> <p>\(\mathcal{M}_R\) (Reachable)：当我们有一个实例化的模型框架之后，这个模型的性能上限其实是已经确定了的，因为它最好也只能去近似实现我们的认知模型。回过头来，我们其实是没有办法达到这个上限性能的，因为我们只能进行参数的初始化(Initialize)而不是真理化(Enlighten)，所以我们应该总是需要一个训练模型的过程。也就是说，当我们撸(git clone)完了变形金刚的代码后，我们就需要开始搞数据调超参等事情了（很显然一个模型的性能好坏是是直接取决于此的），在我们做好我们一切能做的时候，我们就获得了可取得模型，显然这个模型较之实例化模型也是一个劣化。</p> <p>\(\mathcal{M}_O\) (Observation)：最后，在你训练好了你的模型之后，你依旧需要一个方法去测试你取得的这个模型的性能究竟几何。通常来说，你会用一系列大家较为认可的，具有“公理”意味的任务来去评价这个模型。但是这从本质上来说，我们只是通过用多个任务的性能评价，以一种可行度较高的归纳推理，推测了这个模型的性能好或者坏。但是事实上，即便是穷尽市面上的所有任务，我们也没有办法完全展现出来你取得的模型的一切能力，或许你在初始化模型时用的随机种子，能让这个模型在描述你糟心的人生时效果拔群，可是你不知道，你评价的仅仅是一个基于观测的模型，这显然也是劣化于可取得模型的。 所以，当你发现你的模型性能很糟心的时候，你的心里应该要有个很清楚的逆向推理的过程，我们的终极目标是获得一个物自体模型的理想近似，而我们在这个过程中每一步都有模型性能的劣化。</p> <p>我的模型性能不好，是不是仅仅是由于自己采用了错误的观测手段，我是否应当设计新的实验来检验模型内部已经取得的能力？或者说，我是否应该以当前的观测手段作为唯一的评价依据？比如说，当初国内那一众大模型，哪个不是在SuperGLUE上暴打GPT3，可是chatGPT并不是一个需要用这些个数据集来评价的东西。另外，如果市面上没有一个合理的手段，那么我自己是否能提出的提出一套更合理的，再去认定自己的模型的性能？ 如果确认了观测的方案本身没有问题，那么下一步就要思考是不是在取得程度上出了问题，也就是说，数据足够不足够，数据噪音去除了没有，优化器选的对不对，优化器参数对不对，训练时长足够不足够等等等等，一般被戏谑为炼丹的项目。然而事实上，目前大部分人的层级也就停留在这一步了，最忌讳的就是在模型都没有充分取得的情况下，直接去改实例化甚至是认知。我见过太多学生在根本没有充分了解到自己在可取得性的问题的时候就开始瞎改网络构架，乱加Loss，甚至胡乱创造概念，在这里奉劝大家不要抱有侥幸心理，因为即便你通过这些手段，成功让你的模型在观测性能上提升了，也很容易会被当作是水文。 那么接下来，我们要处理的就是实例化的问题了。实例化这个事情很困难，因为我们需要实例化的观念通常来说不止一个，例如变形金刚里面，注意力的实现就需要兼顾“注意力机制本身”“正则化以便于训练”“多头注意力以增加多样性”等等诸多观念，这背后也有长序列的内存与计算复杂度的妥协，所以最终大家看到的结果是一个非常繁琐的工程框架。故而我们讲，我们在检查实例化模型的时候，首先应当理解我们融合了哪些观念，哪些更为重要亦或哪些可以用一个两层的MLP来替代，理解这么做可行的原因是什么。如果只是通过黑箱化的参数堆积，那么你在实例化一步造成的模型劣化就会相当严重。 接下来你要怀疑的就是，自己是不是一开始就想错了，物自体界里的运行逻辑其实不是我脑中的印象/观念/认知那样的。举一个例子，我们是否真的需要positional embedding，我们不知道的那个“什么”，真的只是位置关系，还是注意力本身的衰减(参考AliBi)？一般来说能理解到这个层面上的，已经脱离了水文的范畴了，因为没有什么比一个更清晰更准确的对那个“什么”的认知更重要的。但是要如何提升这个层面的认知，却不得不回到第一层的观测模型上来，因为所有的认知都需要我们的观察来获得，这也是为什么要坚持一线做工作的原因，脱离了这些观察，认知渐渐的会收到一层一层模型的劣化传导而变得扭曲起来。 讲到这里，我似乎大概又完成了一次对联结主义的传教。而我的朋友听了后脸上洋溢着幸福感，说谢谢你，我这就去找我的女神告诉她要如何在加州买大house。我合上了手中的《存在与时间》，抬头望向他：我这只是在教你，你对女朋友的认知跟实例化都有问题。朋友的思路突然就被从加州的大house里拉了回来，一脸错愕得反应道：我？りしれ供さ小？</p> <p>我认真地点点头：我说你啊，</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230906/640-480.webp 480w,/assets/img/20230906/640-800.webp 800w,/assets/img/20230906/640-1400.webp 1400w," type="image/webp" sizes="400px"/> <img src="/assets/img/20230906/640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年09月06日 19:33・北京]]></summary></entry><entry><title type="html">When Your Model Sucks as Much as Your Life (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/model-en/" rel="alternate" type="text/html" title="When Your Model Sucks as Much as Your Life (Gemini 2.5 Pro Translated Version)"/><published>2023-09-06T19:33:00+00:00</published><updated>2023-09-06T19:33:00+00:00</updated><id>https://minogame.github.io/blog/2023/model-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/model-en/"><![CDATA[<p>I have a friend.</p> <p>He told me, very distressed, that his ‘goddess’ (crush) always ignored him, her reason being she was busy tuning parameters and training a model, and the model’s performance was currently terrible. He said this situation felt, perhaps, even more vexing than his goddess’s model. Because her excuse was like a ‘red-header document’ (an official, unchallengeable directive), instilling a sense of awe and fear. Over time, he no longer dared to look directly at her profile picture, afraid his words might affect her hyperparameters, thereby affecting her experimental results, her paper, her graduation, her million-dollar offer, her big house in California.</p> <p>After listening, I calmly sat on the edge of the bed, casually pulled out ‘Being and Time’ from the box where I keep my sleeping pills, flipped to the bookmarked page, and stared at each symbol on the paper with the concentration of someone doing cross-stitch. After a long while, amidst my friend’s uneasy breathing, I slowly said: “When you say a model sucks, you’d better first figure out which model you’re talking about. Although this system isn’t something I created independently, you might as well let me teach it to you again. Of course, you’re free to question this system; it’s just a heuristic methodology. If possible, I hope you can come up with something better yourself. In short, as a researcher whose mathematical abilities are ‘so good’ they can only do black-box deep learning, the ‘model’ you speak of can have five meanings. Let’s use NLP tasks as examples here:</p> <p>\(\mathcal{M}_D\) (Ding an sich / Thing-in-itself): This model borrows Kant’s concept of the ‘thing-in-itself,’ an objective, fundamental existence independent of all our observations and understanding. Using an NLP example, we can imagine that behind the world of NLP, there’s a perfect model supporting the operational logic of all NLP tasks. Although we don’t know what this model is, we can perceive certain phenomena in the NLP world, like predicting the next token after an input sequence. However, these perceived phenomena ultimately cannot tell us what this perfect model truly ‘is.’ This unknown ‘something,’ which nevertheless affects our cognition, is the so-called thing-in-itself model.</p> <p>\(\mathcal{M}_P\) (Perception): Although we cannot directly apply a model that is an unknown ‘something,’ we can, based on our senses, form cognitions, impressions, or concepts about its underlying operational logic. In other words, this second-layer model is an experience-based approximation of that unknown model from the first layer. Continuing with the NLP example, this approximate experience could be called ‘Attention is All You Need.’ That is, the attention mechanism is our modeling of the underlying operational logic of NLP—that perfect, yet unknown, ‘something.’ Obviously, this cognitive model is formed based on human senses and experience, and is thus naturally a degradation of the thing-in-itself model.</p> <p>\(\mathcal{M}_I\) (Instantiation): After establishing a cognitive model, our next natural step is to instantiate it. In other words, we need to transform this generalized concept into something that can actually be operated or computed. Returning to the NLP example, after we believe ‘Attention is All You Need,’ we then start writing code for Transformers. We implement the attention mechanism using multi-head normalized softmax QKV, and we stack many fully connected layers to increase the model’s capacity. Clearly, we don’t always know why we do things a certain way. Part of it is experience gained through induction, but more often it’s heuristic guesswork. Therefore, an instantiated model is a degradation compared to our cognitive model.</p> <p>\(\mathcal{M}_R\) (Reachable): Once we have an instantiated model framework, its maximum potential performance is actually already determined, because at best, it can only approximate our cognitive model. Looking back, we actually cannot reach this upper performance limit because we can only initialize parameters, not ‘enlighten’ them. Thus, we always need a model training process. That is to say, after we’ve ‘pulled’ (git cloned) the Transformer code, we need to start working on data, tuning hyperparameters, etc. (obviously, a model’s performance directly depends on this). When we’ve done everything we can, we obtain a ‘reachable model.’ Clearly, this model is also a degradation compared to the instantiated model.</p> <p>\(\mathcal{M}_O\) (Observation): Finally, after you’ve trained your model, you still need a method to test the actual performance of this model you’ve obtained. Typically, you’ll use a series of widely recognized tasks, carrying an ‘axiomatic’ connotation, to evaluate this model. But essentially, through performance evaluations on multiple tasks, we are merely using a relatively feasible inductive reasoning to infer whether the model’s performance is good or bad. But in fact, even if we exhaust all tasks available, we cannot fully reveal all the capabilities of the model you’ve obtained. Perhaps the random seed you used during model initialization allows this model to describe your sucky life with outstanding results, but you wouldn’t know. You’re only evaluating an observation-based model, which is clearly also a degradation from the reachable model.</p> <p>So, when you find your model’s performance sucks, you should have a very clear reverse reasoning process in mind. Our ultimate goal is to obtain an ideal approximation of the thing-in-itself model, and at every step in this process, there’s a degradation in model performance.</p> <p>Is my model’s poor performance solely due to using incorrect observation methods? Should I design new experiments to test the capabilities the model has already acquired internally? Or, should I rely on current observation methods as the sole basis for evaluation? For example, back then, among the various large models in China, which one didn’t thrash GPT-3 on SuperGLUE? But ChatGPT isn’t something that needs to be evaluated using these datasets. Furthermore, if there isn’t a reasonable method available on the market, can I myself propose a more reasonable one and then assess my model’s performance?</p> <p>If it’s confirmed that the observation method itself is not the problem, the next step is to consider if there’s an issue with ‘reachability’ – that is, is the data sufficient, has data noise been removed, is the optimizer correct, are the optimizer parameters right, is the training duration long enough, and so on and so forth. These are generally part of a project jokingly referred to as ‘alchemy’ (炼丹). However, in reality, most people’s efforts stop at this stage. The biggest taboo is to directly modify the instantiation or even the cognition when the model hasn’t been ‘fully reached’ (i.e., properly trained). I’ve seen too many students start blindly changing network architectures, randomly adding loss functions, or even haphazardly inventing concepts without fully understanding the issues with their model’s reachability. I advise everyone here not to hope to get lucky by chance, because even if you succeed in improving your model’s observed performance through these means, it will easily be dismissed as a ‘filler paper’ (水文 - low-quality publication).</p> <p>Next, we need to address the issue of instantiation. Instantiation is difficult because we usually need to instantiate more than one concept. For example, in Transformers, the implementation of attention needs to balance ‘the attention mechanism itself,’ ‘regularization for ease of training,’ ‘multi-head attention to increase diversity,’ and many other concepts. Behind this, there are also compromises related to memory and computational complexity for long sequences. So, the final result everyone sees is a very cumbersome engineering framework. Therefore, when examining an instantiated model, we should first understand which concepts we’ve integrated, which are more important, or which can be replaced by a two-layer MLP, and understand why doing so is feasible. If it’s just through black-box parameter stacking, the model degradation caused at the instantiation step will be quite severe.</p> <p>Next, you need to suspect whether you were wrong from the very beginning – that the operational logic in the realm of the thing-in-itself is not actually like the impressions/concepts/cognitions in your mind. For example, do we really need positional embedding? Is that unknown ‘something’ really just positional relationships, or is it the decay of attention itself (refer to Alibi)? Generally speaking, understanding at this level has already moved beyond the realm of ‘filler papers,’ because nothing is more important than a clearer and more accurate cognition of that ‘something.’ But to enhance cognition at this level, one must return to the first-layer observation model, because all cognition requires our observation to obtain. This is also why it’s important to persist in hands-on work; detached from these observations, cognition will gradually become distorted by the cascading degradation from layer after layer of models.</p> <p>Having said this, it seems I’ve probably completed another sermon on connectionism. And my friend, after listening, had a face beaming with happiness and said, “Thank you, I’ll go find my goddess right now and tell her how to buy a big house in California.”</p> <p>I closed ‘Being and Time’ in my hands, looked up at him, and said: “I was just teaching you that your cognition and instantiation regarding your girlfriend are problematic.”</p> <p>My friend’s train of thought was suddenly pulled back from the big house in California. With a look of utter bewilderment, he reacted: “Me? Say what now?!”</p> <p>I nodded seriously: “I’m talking about you,”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230906/640-480.webp 480w,/assets/img/20230906/640-800.webp 800w,/assets/img/20230906/640-1400.webp 1400w," type="image/webp" sizes="400px"/> <img src="/assets/img/20230906/640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · September 6, 2023, 19:33 · Beijing]]></summary></entry><entry><title type="html">视觉大模型一无是处</title><link href="https://minogame.github.io/blog/2023/vision-cn/" rel="alternate" type="text/html" title="视觉大模型一无是处"/><published>2023-06-29T16:40:16+00:00</published><updated>2023-06-29T16:40:16+00:00</updated><id>https://minogame.github.io/blog/2023/vision-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/vision-cn/"><![CDATA[<p>最近各种机缘巧合，反复被人问到一个让我非常反感的问题：你的视觉模型有多少参数？虽然Ph.D的职业训练能让我的表情依旧处在1940年欧洲版图那种毫无波兰的状态，但是刨根问底地讲，这个问题所诱发的，对meaninglessitude的观念已经让我形成了一种理性上的反感。</p> <p>一、22B与175B的差距差不多是175B</p> <p>首先我还是不否定，随着视觉模型参数量的提升，其在传统的视觉任务上的绝对数值表现也有相应的提升。近期出现的最好的一个例子就是Google的ViT-22B，虽然用了少量的没有开源的数据以及<del>保守的</del>炼丹技巧才让这个模型跑起来。但是这不妨碍圈内大体保持一个乐观的态度，认为计算机视觉依旧是配得上大模型的。然而在我看来，这个存在感薄的宛如arxiv主页一般的视觉最大往生模型，恰巧完成了“堆积参数与数据量对于纯视觉这个领域毫无意义”的证明。</p> <p>稍微思考一下，这个模型的训练量实际上达到了(JFT-)4B * 256token/img * 3epoch约等于3T的tokens，这已经比LLaMa最大的模型的训练量还要大一倍。但是ViT-22B上并没有什么真的有意义的结论，除了在经典旧世中的数据集上又多涨了三五斗，也就是这个模型更偏向于sketch而不是texture这些泛空的内容。拿着这个ROI堪比春夏季大反攻的结论，视觉还会有接下来的一个大模型吗，我看是不会有了。然而别忘了，22B与175B的差距差不多是175B。</p> <p>二、视觉模型再大也依旧愚钝</p> <p>在进入下一段的形上学内容之前，我先从经验主义的角度出发，复述三个“现代计算机视觉头顶的乌云”。当然，这些问题并不是没有得到重视，因为每年都持续的会有海量相关的文章这些问题上吃SR——这也确实印证了它们是难以被解决的。</p> <p>首先要拿出来讲的自然是陈词滥调的adversarial attack问题。平心而论虽然NLP领域也有这个问题，但是当NLP迈入(generative)LLM时代之后这个问题也几乎不再被重视了。反观CV领域，这个问题就像是怨魂一样纠缠着每一个倒霉的审稿人，因为没有人站出来写这样一篇文章，说给他们一个大模型他们就能终结这个领域。而且甚至于现在都没有人愿意去测试这些视觉大模型的对抗攻击问题，因为人们已经普遍接受了这么一个理念，就是视觉模型就应该被攻击。</p> <p>其次则是房间里的大象的问题，于2018年被提出来用以质疑detection模型与算法的有效性。大意是说，即便我将一只大象p到一个普通房间模样的照片中，我们充满智慧的模型也可以IOU&gt;0.9地将它检测出来，即便这从经验之理中来看是荒谬的。这其实是个充满哲理而且可能影响到CV根基的问题，只不过解决它与否并不影响一个算法在coco上的性能（或者说只会有负影响），所以人们也只管当它是房间里的大象。</p> <p>最后第三朵乌云就更现实一点，我愿意称之为样本空间的无序化问题。换言之，视觉信息在样本的特征空间中无法形成体系性结构。这里，视觉信息就是指图片中存在的可以抽象出的语义，体系性结构指的则是这些语义相互之间的关系。即便现在先进的自监督训练技术，可以很好的让某一类事物在“不需要知晓其语义的”情况下抱团取暖（体现为t-SNE的良好可区分性），但是从语义的角度来看，航天飞机的一个很小的r-neighbourhood内既可以有短毛橘猫也可以有果酱面包。往好的讲，这提高了基于视觉特征的创造力，往不好的讲，这倒逼视觉的研究者只能不断去寻求基于提高（特定benchmark的性能）力来创造视觉特征。</p> <p>三、NLP的知识与CV的知识</p> <p>首先我们需要确认两个概念，因为这两个概念的精准定义是一个长久以来都纠缠不清的问题，所以这里我们采取ChatGPT提供的通识性定义：</p> <p>Representation（表象，也作表征）：在心灵哲学和认识论中，”representation”指的是一种心智状态或实在，用以指代某一件事物。表象的定义中蕴涵了我们的心灵具备引用或表达存在于世界中的物体、思想或情况的能力。表象可以采取多种形式，包括心理图像、信念、思想或语言符号，可以被看作是心灵与外部世界之间的中介，使我们能够对世界有知识、感知和理解。</p> <p>Concept（概念）：另一方面在认识论中，”concept”是一个抽象或一般性的心理表象，包括一种或一类的事物、事件或观念。概念是思维和语言的基本构建单元，使我们能够对经验和知识进行分类和组织。对照单一的表象，概念的范围更为抽象和概括。概念通过抽象和概括的过程形成，我们在多个实例中识别共同的特征或属性，并创建一个心理范畴来代表这些共享的特征。</p> <p>武断地讲，“表象”和“概念”都涉及到表达和传达某些信息或意义，但它们的本质和来源是不同的。表象更多的可以被看作是一种心理层面的现象，而概念则可以被认为是一种思维层面的产物。</p> <p>事实上我认为搞明白这两个概念了之后，在认知层面上NLP与CV模型所获得的知识便很容易区分理解。我认为在语言（特别是generative）模型的框架内，我们为了训练一个模型，所提供的数据实质上展现的是Relations of Concepts（这里借用了Hume的理念，但是区别于纯粹的演绎，语言模型中的Relations依旧是通过归纳获得）。在此之上，对于某个Concept本身的理解，语言模型也是通过其与其他的Concepts之间的关联来获得一个间接的认知，而并没有一个直接的类似于人类心灵上的直接理解（特别是诸如“时间”等概念）。换而言之，语言模型所学习到的，是诸多由Concepts之间的关联所组成的Representations，以及由这些表象之间的关联所呈现出来更深刻更复杂的表象，而人类（语言）世界中所存在的或者所能被理解的表象多若繁星，我们需要万亿的参数去记忆这些表象，以及理解这些表象之间的关联性（或者说一个由表象为量子的N-gram）。</p> <p>反观视觉模型，在不论有监督或者自/无监督的（几乎所有的）训练框架内，其优化目标实质上是将特定的具体的数据，转化为特定的唯一的Representation，并经由特定的视觉任务（即人为的经验）最终将这些表象抽象为Concept，换言之是一个学习Matter of Facts的过程。这里再额外解释一下为什么需要经由特定的视觉任务，有过（自监督）视觉大模型训练经验的人大多会有这样一个认知，就是视觉模型的效果很难去做evaluatation，大家只是从k-NN/Linear Probe/Attentive Probe/Finetune（分类或者其他下游任务）中选择性能好的来写论文，但实际上这里的本质原因是，在没有人为的经验来帮助视觉模型去做抽象的时候，一个算法不会去主动的来做这样的抽象（它没有Loss也没有义务去做），它只需要将数据与表象去做对应即可。反过来讲如果在算法的设计上就已经考虑到了抽象Concept的路径，那么这个模型在Linear Probe的性能自然会好一些（例如杨立昆君的I-JEPA，下面会再讲到）。</p> <p>四、参数量解决不了抽象概念的问题</p> <p>现在再回到那该死的参数量上，对于现阶段的视觉任务，其对应模型的参数量恐怕并不能成为让「视觉大模型出现chatGPT时刻」的主要指标。再回到现代计算机视觉头顶的那三朵乌云：</p> <p>参数量只能在概念归纳的强度上获得边际的收益。一般来讲，视觉模型在经由表象来归纳概念时，脱离不了一个大致的聚类的概念。换句话说，我们预先会有一个“同一概念的表象会聚集在一起”的先验知识，例如Gaussian Prior，我们再经由实际的表象的分布来获得后验的结果。但是通过增加参数量来更精准的表达表象在向量空间的位置，其收益注定会逐渐边际化，因为参数量并不会与误差有一个线性的关系。以至于在某一个时刻开始，在有限的样本数量下，误差更多来源于先验的分布，最终提升参数量只能表现为ImageNet上那0.1%的性能提升。除此之外，增加参数量所引发的副效果，更高的向量空间的维度，也会增加归纳一个概念时所需的表象的数量（维度诅咒），从而也就会为对抗样本提供了更多的存在的空间。</p> <p>参数量并不能脱离归纳来知晓表象或概念之间的存在逻辑。从道理上来讲，神经网络可以Universal Approaximation，而且这个能力会随着参数量的增加而变强。事实上以我个人的经验，大体上来说一个参数量更多的模型，是可以用单一的特征向量来表达某一个相对复杂的场景的，例如一个盘子里面装着各种不同的水果摆件，只不过我们无法提供高质量的人为的经验去概念化此类东西，在这个例子里面通常来说我们只能提供诸如“静物”“果盘”或者“一个盘子里面装着各种不同的水果摆件”之类的概念或其组合。只不过这种概念对于实际上模型所能表达的表象来说过于的抽象，模型通常只能去概括的统计更初级表象的量来组合成此类高度复杂的表象。所以如果缺乏可以枚举“与”“或”“非”等关联的数据的引导，模型自然也不会去主动因为存在或者不存在某一个初级表象，就将一个复杂的表象去彻底归为不同的概念。例如，在缺乏数据的情况下，模型不太会因为一个“森林”的图片里面有一辆自行车，就把它归类为“公园”，或者因为缺少一条河流，就把它归类为“青山”。</p> <p>参数量也不能解决样本空间的无序化问题，从而无法让模型在脱离人为的经验的情况下，自发地学到由概念之间的关联而产生的新的概念。在自然语言当中，概念本身是有很好的层级化关联结构的，故而由概念或者表象的关联而形成的表象（例如一句话）也会因此获得一个结构性的表达，基于这种结构性的表达，我们会很容易继续创造新的概念或者表象。反观视觉中信息缺乏此类的层级化关联结构，视觉感官上相似的表象对应的概念可以完全不一样（例如字母l与数字1），所以在缺乏人为经验的情况下（自/无监督），在（表象的）特征空间里面也难以自发地形成结构性的表达，继而无法自发地形成新的概念。一个显著的例子就是，在跨模特的Align模型出现之前，几乎没有什么视觉算法可以做generalized zero-shot learning，即便是在某个特定的domain上（CUB鸟或者oxford花），zero-shot的性能也很难看。在大语言模型可以通过纯粹的语言概念解读画出独角兽的同时，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”<del>（除非毛子们能给它提供足够多的人为的经验）。</del></p> <p>五、Naive Cross-Modality Alignment并不解决根本问题</p> <p>有碰过Stable Diffusion的孩子们应该会注意到这样一个现象，就是绝大多数的prompt都是宛如咒语一般的独立的单词，而真正用自然语言描述的场景却很难被准确的作画。有直接碰过CLIP的孩子们也应该会注意到这样一个现象，绝大多数图文匹配的分数（无论正负样本）都分布在一个较为狭小的区间，而且难以采用一个直观的阈值来判断两者的匹配度。排除掉模型训练相关的原因，这凸显了两个问题：</p> <p>视觉端能精准学到的Concept的量是有限的，且更多的是简单的单词/短语级别的Concept。实际上在本人实际训练中文CLIP的时候，就遇到了学习到的top 100万个Concept里面里面有80万都是人名的状况（是的，再往后人名的比重会更高）。而这个其实是符合真实世界的分布，诸君可以现场站起来，用语言描述一下自己所能看见的物件，就会发现真实生活中接触到的相对简单的视觉Concept相当的贫乏，而更多的是身边的人的名字。</p> <p>视觉端并不是不可以学到一个相对复杂的表象，但是视觉端无法像文本端一样把这种复杂的表象归纳称为概念，这就导致了视觉文本两侧的匹配会出现偏差，而实际上训练数据中能供视觉端归纳出一个复杂表象所对应的概念的数据是有限的，也就是说，即便有语言的帮助，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”（除非毛子们能给它提供足够多的人为的经验）。</p> <p>所以，为什么Lock image Tuning现在会成为一个比较合理的CLIP训练模式，因为用语言将视觉端学到的表象归纳为概念这件事情的难度，远小于用视觉去理解语言中由多个抽象的概念构成的复杂表象。</p> <p>六、皈依 or not皈依语言模型</p> <p>最后讨论一下视觉模型往大做的出路在哪里。当然这里是要排除掉纯粹以学习表象为目的的模型的，例如围棋或者天气的模型，这些模型本身的能力本就展现在学习到人类无法用语言抽象的神秘的表象上，这些模型随着数据规模的增大，参数量自然需要scale-up来提升表象的能力（例如在99x99的围棋盘上，一个千亿参数的模型的性能理论上来讲必然好过一个百亿的）。</p> <p>现在的学术大致上寻找到了两个出路，其一便是让视觉模型彻底的皈依大的语言模型的能力（例如Google丢出来的PaLM-E以及今年的CVPR最佳论文Visual Programming），让视觉模型回归一个为概念提供视觉表象的功能，让大语言模型来完成更为复杂的概念的解读。这个时候视觉模型的参数量即可以用来学习一些人类无法用语言抽象的表象上（例如depth map，optical flow，hyperspectral singal），从而补齐语言模型在空间推理上的一些弱点，让其更加的接近真实世界的AGI。但是这种模式依旧依赖于毛子去打坦克搜集大量的数据用以提供人为的经验，未来的路就仿佛路易十六39岁时的样子——一眼望不到头。</p> <p>另外一个思路则体现在杨立昆君的I-JEPA以及feifei大佬的SiamMAE里面，我们强行的去让视觉模型去理解表象之间的关联，而这种任务本身对有注意力机制的ViT模型来说并不是特别困难。这样做最大的优点在于，它可以部分地解决前述的第二三朵乌云，但是这些方案由于突出强调个体表象，就比较难以学习到基于多种表象与概念而形成的复杂表象，特别体现在ImageNet的Finetune效果会比较差（ImageNet有一些相对复杂的场景类别）。而事实上，当下视觉的学术圈不怎么容得下一个性能不怎么好的算法，任何一点开放的探索都会被操着一口Chinglish的审稿人百般刁难，新的算法仿佛被困在了一个无尽的黑洞中难以诞生。</p> <p>当然如果你们关心我是怎么回答“你的视觉模型有多少参数？”这个问题的话，我一般是会冷漠地讲“ViT-B，88M，再大的模型他们不给上线了”。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年06月29日 21:15 · 广东]]></summary></entry><entry><title type="html">Vision Large Models Are Utterly Useless (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/vision-en/" rel="alternate" type="text/html" title="Vision Large Models Are Utterly Useless (Gemini 2.5 Pro Translated Version)"/><published>2023-06-29T16:40:16+00:00</published><updated>2023-06-29T16:40:16+00:00</updated><id>https://minogame.github.io/blog/2023/vision-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/vision-en/"><![CDATA[<p>Recently, due to various coincidences, I’ve been repeatedly asked a question that I find extremely repulsive: How many parameters does your vision model have? Although my Ph.D. professional training allows my expression to remain in that “1940 map of Europe without Poland” state (i.e., completely impassive/poker-faced), to get to the root of it, the concept of “meaninglessitude” induced by this question has already formed a rational aversion in me.</p> <p>I. The Gap Between 22B and 175B is Approximately 175B</p> <p>First, I don’t deny that as the parameter count of vision models increases, their absolute numerical performance on traditional vision tasks also improves accordingly. The best recent example is Google’s ViT-22B, although it required a small amount of non-open-source data and <del>conservative</del> “alchemy” skills (i.e., meticulous, perhaps arcane, empirical tuning) to get this model running. However, this doesn’t prevent the community from generally maintaining an optimistic attitude, believing that computer vision is still worthy of large models. Yet, in my view, this largest vision “afterlife model” (a model so large it’s as if sent to the great beyond, perhaps implying it’s impressive in scale but not practical impact), with an existence as thin as the arXiv homepage, has coincidentally proven that “piling up parameters and data volume is meaningless for the field of pure vision.”</p> <p>Think about it for a moment: the training volume of this model actually reached (JFT-)4B images * 256 tokens/img * 3 epochs, approximately 3T tokens. This is already double the training volume of the largest LLaMa model. But ViT-22B didn’t yield any truly meaningful conclusions, apart from a slight increase of “three to five pecks” (a tiny amount) on datasets from the “classic old world” (older, standard benchmarks). That is, the model is more biased towards sketch rather than texture—these rather vacuous findings. With this conclusion, whose ROI is comparable to a spring/summer counteroffensive (implying massive effort for little gain), will there be another large model in vision? I don’t think so. However, don’t forget, the gap between 22B and 175B is approximately 175B.</p> <p>II. Vision Models Remain Dull Even When Large</p> <p>Before moving on to the metaphysical content in the next section, I will first, from an empiricist perspective, recount three “dark clouds hanging over modern computer vision.” Of course, these problems have not gone unnoticed, as every year there is a continuous flood of related papers milking these issues for publications/citations—which indeed proves they are difficult to solve.</p> <p>The first to be discussed is naturally the clichéd problem of adversarial attacks. To be fair, although the NLP field also has this problem, it has largely been ignored since NLP entered the era of (generative) LLMs. In contrast, in the CV field, this problem haunts every unlucky reviewer like a vengeful spirit, because no one has stepped up to write a paper claiming that a large model could put an end to this area of research. Moreover, nowadays, no one is even willing to test the adversarial attack vulnerability of these large vision models, because people have generally accepted the idea that vision models <em>should</em> be attackable (i.e., it’s an inherent, accepted flaw).</p> <p>Secondly, there’s the “elephant in the room” problem, proposed in 2018 to question the effectiveness of detection models and algorithms. The gist is that even if I photoshop an elephant into a picture of an ordinary room, our “intelligent” models can detect it with an IOU &gt; 0.9, even though this is absurd from an experiential standpoint. This is actually a philosophical problem that could affect the very foundations of CV, but solving it or not doesn’t affect an algorithm’s performance on COCO (or it might only have a negative impact), so people just treat it as the elephant in the room.</p> <p>Finally, the third dark cloud is a bit more practical, which I like to call the problem of the disordered sample space. In other words, visual information fails to form a systematic structure in the feature space of samples. Here, visual information refers to the abstractable semantics present in images, and systematic structure refers to the relationships between these semantics. Even with advanced self-supervised training techniques that allow certain types of objects to “huddle together for warmth” (cluster well, reflected by good t-SNE separability) without needing to know their semantics, from a semantic perspective, a very small r-neighborhood of a space shuttle can contain both an orange tabby cat and a jam sandwich. On the bright side, this enhances creativity based on visual features. On the downside, it forces vision researchers to constantly seek to create visual features based on improving performance on specific benchmarks.</p> <p>III. Knowledge in NLP and CV</p> <p>First, we need to confirm two concepts. Since the precise definition of these two concepts has long been a tangled issue, here we adopt the general definitions provided by ChatGPT:</p> <p>Representation: In philosophy of mind and epistemology, “representation” refers to a mental state or entity that stands for something. The definition of representation implies that our minds have the ability to refer to or express objects, ideas, or situations existing in the world. Representations can take various forms, including mental images, beliefs, thoughts, or linguistic symbols, and can be seen as intermediaries between the mind and the external world, enabling us to have knowledge, perception, and understanding of the world.</p> <p>Concept: On the other hand, in epistemology, a “concept” is an abstract or general mental representation that includes a type or class of things, events, or ideas. Concepts are the basic building blocks of thought and language, enabling us to categorize and organize our experiences and knowledge. Compared to a single representation, concepts are more abstract and general in scope. Concepts are formed through processes of abstraction and generalization, where we identify common features or properties across multiple instances and create a mental category to represent these shared characteristics.</p> <p>To put it bluntly, both “representation” and “concept” involve expressing and conveying certain information or meaning, but their nature and origin are different. Representation can be seen more as a psychological-level phenomenon, while a concept can be considered a product at the thinking level.</p> <p>In fact, I believe that after understanding these two concepts, the knowledge acquired by NLP and CV models at the cognitive level becomes easy to distinguish and comprehend. I believe that within the framework of language (especially generative) models, the data we provide to train a model essentially exhibits Relations of Concepts (here I borrow Hume’s idea, but unlike pure deduction, Relations in language models are still obtained through induction). Beyond this, for the understanding of a Concept itself, language models also gain an indirect cognition through its associations with other Concepts, without a direct understanding akin to human mental comprehension (especially for concepts like “time”). In other words, what language models learn are numerous Representations formed by the associations between Concepts, as well as more profound and complex representations presented by the associations between these representations. The representations that exist or can be understood in the human (linguistic) world are as numerous as stars; we need trillions of parameters to memorize these representations and understand their interrelations (or, an N-gram where representations are the quanta).</p> <p>In contrast, for vision models, within (almost all) supervised or self/unsupervised training frameworks, the optimization objective is essentially to transform specific concrete data into a specific unique Representation, and then, through specific vision tasks (i.e., human experience), to ultimately abstract these representations into Concepts. In other words, it’s a process of learning Matters of Fact. Let me further explain why specific vision tasks are needed. Most people with experience training (self-supervised) large vision models will have this understanding: it’s very difficult to evaluate the effectiveness of vision models. People just choose the ones with good performance on k-NN/Linear Probe/Attentive Probe/Finetune (classification or other downstream tasks) to write papers. But the essential reason here is that without human experience to help the vision model perform abstraction, an algorithm won’t proactively do such abstraction (it has no Loss function or obligation to do so); it only needs to map data to representations. Conversely, if the path to abstracting Concepts is already considered in the algorithm’s design, then the model’s performance on Linear Probe will naturally be better (e.g., Yann LeCun’s I-JEPA, which will be discussed later).</p> <p>IV. Parameter Count Can’t Solve Conceptual Abstraction</p> <p>Now, let’s return to that damned parameter count. For current vision tasks, the parameter count of corresponding models probably cannot be the main indicator for a “ChatGPT moment” in large vision models. Back to those three dark clouds over modern computer vision:</p> <p>Parameter count can only yield marginal gains in the strength of concept induction. Generally, when vision models induce concepts from representations, they cannot escape a general notion of clustering. In other words, we have prior knowledge that “representations of the same concept will cluster together,” e.g., a Gaussian Prior, and then we obtain posterior results through the actual distribution of representations. However, the benefit of increasing parameter count to more precisely express the position of representations in vector space is destined to become marginal, as parameter count does not have a linear relationship with error. So much so that from a certain point, with a limited number of samples, errors originate more from the prior distribution, and ultimately, increasing parameter count only manifests as that 0.1% performance boost on ImageNet. Besides, the side effect of increased parameter count—higher vector space dimensionality—also increases the number of representations needed to induce a concept (curse of dimensionality), thereby providing more space for adversarial examples to exist.</p> <p>Parameter count cannot, independent of induction, know the existential logic between representations or concepts. Theoretically, neural networks can perform Universal Approximation, and this ability strengthens with increasing parameter count. In fact, from my personal experience, a model with more parameters can generally use a single feature vector to represent a relatively complex scene, such as a plate holding various fruit ornaments. However, we cannot provide high-quality human experience to conceptualize such things. In this example, we can usually only provide concepts like “still life,” “fruit plate,” or “a plate holding various fruit ornaments,” or combinations thereof. However, such concepts are too abstract for the representations the model can actually express. The model usually can only statistically generalize quantities of more primary representations to compose such highly complex representations. So, lacking guidance from data that can enumerate associations like “AND,” “OR,” “NOT,” etc., the model naturally won’t proactively categorize a complex representation into entirely different concepts just because a primary representation is present or absent. For example, lacking data, a model is unlikely to classify an image of a “forest” as a “park” just because there’s a bicycle in it, or classify it as “green mountains” just because a river is missing.</p> <p>Parameter count also cannot solve the problem of disordered sample space, thus preventing the model from spontaneously learning new concepts arising from associations between concepts, in the absence of human experience. In natural language, concepts themselves have good hierarchical association structures. Therefore, representations formed by the association of concepts or representations (e.g., a sentence) also acquire a structural expression. Based on this structural expression, we can easily continue to create new concepts or representations. In contrast, visual information lacks such hierarchical association structures. Visually similar representations can correspond to completely different concepts (e.g., the letter ‘l’ and the number ‘1’). So, in the absence of human experience (self/unsupervised), it’s difficult to spontaneously form structural expressions in the (representation’s) feature space, and consequently, new concepts cannot be formed spontaneously. A prominent example is that before the emergence of cross-modal Align models, almost no vision algorithm could perform generalized zero-shot learning. Even on specific domains (CUB birds or Oxford flowers), zero-shot performance was very poor. While large language models can draw unicorns by purely interpreting linguistic concepts, a large vision model probably will never recognize “a destroyed Leopard 2 tank” <del>(unless the Russkies can provide it with enough human experience).</del></p> <p>V. Naive Cross-Modality Alignment Doesn’t Solve Fundamental Issues</p> <p>Those who have played with Stable Diffusion should have noticed this phenomenon: most prompts are like incantations of independent words, while scenes described in true natural language are difficult to depict accurately. Those who have directly worked with CLIP should also have noticed this: the vast majority of image-text matching scores (for both positive and negative samples) are distributed within a relatively narrow range, and it’s difficult to use an intuitive threshold to determine their match. Excluding reasons related to model training, this highlights two problems:</p> <p>The number of Concepts that the visual end can accurately learn is limited, and they are mostly simple word/phrase-level Concepts. In fact, when I was actually training a Chinese CLIP, I encountered a situation where 800,000 out of the top 1 million learned Concepts were personal names (yes, the proportion of names becomes even higher further down). And this actually conforms to real-world distribution. Gentlemen, you can stand up right now and describe the objects you see using language; you’ll find that the relatively simple visual Concepts encountered in real life are quite scarce, while names of people around you are more common.</p> <p>It’s not that the visual end cannot learn a relatively complex representation, but the visual end cannot generalize such complex representations into concepts like the textual end can. This leads to discrepancies in matching between the visual and textual sides. In reality, the data in training sets that allows the visual end to generalize a concept corresponding to a complex representation is limited. That is to say, even with the help of language, a large vision model will probably never recognize “a destroyed Leopard 2 tank” (unless the Russkies can provide it with enough human experience).</p> <p>So, why has “Lock Image Tuning” now become a relatively reasonable CLIP training mode? Because the difficulty of using language to generalize representations learned by the visual end into concepts is far less than that of using vision to understand complex representations composed of multiple abstract concepts in language.</p> <p>VI. To Convert or Not to Convert to Language Models</p> <p>Finally, let’s discuss the way forward for scaling up vision models. Of course, we must exclude models purely aimed at learning representations, such as Go or weather models. The capabilities of these models themselves lie in learning mysterious representations that humans cannot abstract with language. As the data scale of these models increases, the parameter count naturally needs to scale up to enhance representational ability (e.g., on a 99x99 Go board, a model with hundreds of billions of parameters will theoretically perform better than one with tens of billions).</p> <p>Current academia has generally found two ways out. One is to have vision models completely convert to the capabilities of large language models (e.g., Google’s PaLM-E and this year’s CVPR best paper, Visual Programming), letting vision models return to the function of providing visual representations for concepts, and allowing large language models to perform the interpretation of more complex concepts. At this point, the parameter count of vision models can be used to learn some representations that humans cannot abstract with language (e.g., depth maps, optical flow, hyperspectral signals), thereby compensating for some weaknesses of language models in spatial reasoning, bringing them closer to real-world AGI. However, this mode still relies on the “Russkies” to destroy tanks and collect vast amounts of data to provide human experience. The road ahead is like Louis XVI at age 39—no end in sight.</p> <p>Another line of thought is reflected in Yann LeCun’s I-JEPA and Fei-Fei Li’s SiamMAE, where we forcibly make vision models understand the associations between representations. This task itself is not particularly difficult for ViT models with attention mechanisms. The biggest advantage of doing this is that it can partially solve the aforementioned second and third dark clouds. However, because these solutions emphasize individual representations, they find it relatively difficult to learn complex representations formed from multiple representations and concepts, which is particularly evident in poorer finetuning performance on ImageNet (ImageNet has some relatively complex scene categories). And in fact, the current vision academia is not very tolerant of algorithms with mediocre performance. Any bit of open exploration will be relentlessly harassed by reviewers speaking Chinglish. New algorithms seem trapped in an endless black hole, struggling to be born.</p> <p>Of course, if you’re interested in how I answer the question “How many parameters does your vision model have?”, I generally say coolly, “ViT-B, 88M. They won’t let us deploy anything bigger.”</p>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · June 29, 2023, 21:15 · Guangdong]]></summary></entry><entry><title type="html">冷嘲热讽一下某0.3秒差距之外的公司</title><link href="https://minogame.github.io/blog/2023/lightyear-cn/" rel="alternate" type="text/html" title="冷嘲热讽一下某0.3秒差距之外的公司"/><published>2023-06-26T13:32:00+00:00</published><updated>2023-06-26T13:32:00+00:00</updated><id>https://minogame.github.io/blog/2023/lightyear-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/lightyear-cn/"><![CDATA[<p>首先要澄清三个事情：</p> <p>0.3是个约数，秒差距的准确定义是648000/pi个天文单位（冷知识）；</p> <p>抑郁症是不好的，作为曾经被抑郁症困扰的人，如果这个病是真的，我希望它是假的，如果是假的，那我希望它是真的；</p> <p>没有什么特别的利害关系，之所以想冷嘲热讽纯粹只是因为之前被这个公司评价“不够优秀”，本着对等原则我也得做一个类似的评价。</p> <p>反正这个公司有这一天我猜大家也都料想过，只不过这么快是我也没料想过的，5000万刀一张张烧（物理）不也得烧两年。当然我也不敢在这里公然诽谤这公司就彻底垮垮了没没了，但是本着中华文化中“不可言勇不可图存”的观念，一个公司经历了此般的挫折，未来想要再修复（可能本来就不是太好的）口碑估计会相当的艰难，只是可怜了已经进去的那些算法小哥了。</p> <p>其实本来写这个文章的欲望并不是很强，但也翻了一下某个人均年收百万的社区的评价，发现没有人把这个公司出的问题讲清楚，本着为了拯救国内泛用人工智慧创业圈的好的初心，就扯一下自己的观点。</p> <p>问题其实很简单，对标OpenAI，不对标几年前在生死线挣扎被人冷嘲热讽却不断坚持输出高学术价值的OpenAI，却只想对标已经站在成功的风口即将开启商业化变现的OpenAI。甚至于还把后者做了公式化处理，成功=大语言模型=硬件基建+软件构架+顶级算法+海量数据，然后顺着这个公式去搭建团队执行工作：疯狂买老黄的初级工业垃圾 + 收编（普遍认为）贡献不大的构架团队 + 不切实际的高招聘标准（我甚至怀疑为了满足老板的喜好，他们的团队宁可不招人也不敢多招人） + 招进去的算法自己动手撸数据（猜的，有消息请更正我）。</p> <p>这么做有问题吗，从资本的角度来看大概是没有的，但是从怀疑论的角度来看，这个成功公式只是从OpenAI这一家公司的结论中非常朴素的归纳而来，这并不是一个确定的事情。实际上要我来讲，这个成功公式里面可能最重要的一项，奥古斯丁的神光（Divine illumination），是直接被忽视了的。或者用土话来讲，这个公司啥都做对了，就是没请个大法师来开光。我知道你们都唯物，神光这种东西听起来就不靠谱，然而OpenAI在长期的挣扎与学术输出中，它的内部确实可能形成了这么一种类似于神光的文化，让每一个初入的菜坤算法同学都可以在它的照耀下产生智慧，这些智慧最终凝结起来才产生了当今可能最接近通用人工智能的神迹（最接近钱的产品，不寒碜）。</p> <p>最后再澄清一下，封面这个图有“光”，有“年味”，又有世“外”桃源感觉的图，是出自驴肉火烧同款逻辑的差两个月大模型。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年06月26日 13:32 · 广东]]></summary></entry><entry><title type="html">Scoffing at a Certain Company 0.3 Parsecs Away (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2023/lightyear-en/" rel="alternate" type="text/html" title="Scoffing at a Certain Company 0.3 Parsecs Away (Gemini 2.5 Pro Translated Version)"/><published>2023-06-26T13:32:00+00:00</published><updated>2023-06-26T13:32:00+00:00</updated><id>https://minogame.github.io/blog/2023/lightyear-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/lightyear-en/"><![CDATA[<p>First, three clarifications:</p> <p>0.3 is an approximate number. The precise definition of a parsec is 648,000/π astronomical units (a bit of trivia).</p> <p>Depression is not good. As someone who has struggled with depression, if this illness (being referred to) is real, I wish it were fake. If it’s fake, then I wish it were real (implying a wish for deserved comeuppance).</p> <p>There are no particular stakes involved. The reason I want to scoff is purely because this company previously judged me as “not outstanding enough.” Based on the principle of reciprocity, I must offer a similar evaluation.</p> <p>Anyway, I guess everyone expected this company would see this day, though I didn’t expect it to be this soon. Burning through 50 million dollars, even if physically setting the bills on fire one by one, should take at least two years. Of course, I wouldn’t dare to publicly slander that this company has completely collapsed and disappeared. However, in line with the Chinese cultural concept of “one should neither be reckless in valor nor merely aim for survival” (implying perhaps that after such a display of recklessness, mere survival is the only concern, or that they shouldn’t have been so bold if they couldn’t sustain it), after a company experiences such a setback, restoring its (perhaps never very good) reputation in the future will likely be exceedingly difficult. It’s just a pity for those algorithm guys who already joined.</p> <p>Actually, my desire to write this article wasn’t initially very strong. But I browsed through the comments on a certain community where the average annual income is supposedly in the millions, and found that no one had clearly explained the problems with this company. Out of a noble intention to save the domestic general-purpose artificial intelligence startup scene, I’ll share my views.</p> <p>The problem is actually very simple: they benchmarked themselves against OpenAI. Not the OpenAI from a few years ago that was struggling on the brink, scoffed at by others, yet persistently produced high academic value. Instead, they only wanted to benchmark against the OpenAI that was already on the cusp of success, about to monetize commercially. They even turned the latter into a formula: Success = Large Language Model = Hardware Infrastructure + Software Architecture + Top-tier Algorithms + Massive Data. Then they built a team and executed work according to this formula: frantically buying “Old Huang’s” (Jensen Huang of Nvidia) primary industrial waste (i.e., GPUs) + recruiting architecture teams (widely considered) of little contribution + unrealistically high hiring standards (I even suspect that to satisfy the boss’s preferences, their team would rather not hire anyone than hire too many people) + hired algorithm engineers having to wrangle data themselves (my guess, please correct me if you have information).</p> <p>Is there a problem with doing this? From a capital perspective, probably not. But from a skeptical viewpoint, this success formula is just a very naive induction from the conclusions of OpenAI alone; it’s not a certainty. Actually, if you ask me, perhaps the most important item in this success formula, Augustine’s Divine Illumination, was directly overlooked. Or, to put it colloquially, this company did everything right, except they didn’t invite a high priest to consecrate it. I know you’re all materialists, and something like “divine illumination” sounds unreliable. However, through its long period of struggle and academic output, OpenAI may indeed have formed such a culture internally, akin to divine illumination, allowing every new “Cai Kun” (a popular celebrity’s name, used humorously here to mean “newbie” or “rookie”) algorithm student to generate wisdom under its radiance. This accumulated wisdom ultimately produced the miracle that is perhaps closest to general artificial intelligence today (the product closest to money, nothing to be ashamed of).</p> <p>Finally, one more clarification: the cover image, which has “light,” a “festive New Year atmosphere,” and an “out-of-this-world” (Shangri-La/utopia) feel, was generated by a large model that is “two months different from the one with the same logic as the ‘donkey meat burger’ model.” (This is a very specific, possibly metaphorical or inside joke reference, implying a particular kind or quality of AI generation).</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · June 26, 2023, 13:32 · Guangdong]]></summary></entry></feed>