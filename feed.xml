<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://minogame.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minogame.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-29T10:43:53+00:00</updated><id>https://minogame.github.io/feed.xml</id><title type="html">Zhun Sun</title><subtitle>Zhun Sun&apos;s personal page and blog storage. </subtitle><entry><title type="html">南山笑话集锦</title><link href="https://minogame.github.io/blog/2025/jokes-cn/" rel="alternate" type="text/html" title="南山笑话集锦"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-cn/"><![CDATA[<p>本文中「南山公司」、「必胜大模型」、「胜客APP」等均为虚构组织与产品，与现实无关，请勿对号入座。 与现实无关，请勿，对号入座。</p> <hr/> <p>一个人在办公室抱怨必胜大模型性能太差。</p> <p>结果被同事听到报告给了上级。</p> <p>上级把他叫去问：“你为什么要抱怨？”</p> <p>他说：“我没有抱怨，我只是在和朋友讨论必胜大模型。”</p> <p>上级说：“你以为我是必胜大模型吗，说什么都听不懂？”</p> <hr/> <p>三个学生拿到自己的被判0分试卷后面面相觑。</p> <p>甲：我用ChatGPT但是忘记删除了openai的名字。</p> <p>乙：我用deepseek结果一直是服务器繁忙，所以最后只好交了白卷。</p> <p>丙：我可是真的冤，明明是自己认认真真地解题作答，结果只是因为错的多了点，老师非说我是用必胜大模型写的。</p> <hr/> <p>数据分析师A：最近几天流量出现了显著跌幅，是出了什么问题了吗？</p> <p>数据分析师B：好像确实出了许多模型胡乱回答用户体验非常差的case</p> <p>产品经理A：好像前几天那个后端离职开始就这样了</p> <p>产品经理B：我带上公司法务去看看他搞了什么破坏</p> <p>…</p> <p>…</p> <p>…</p> <p>产品经理B：艹，他走的时候把默认API改成必胜大模型了</p> <hr/> <p>必胜大模型荣获”赛博朋克科幻文学奖”，其获奖小说片段如下：</p> <p>「亲爱的用户，根据政策我无法描述崩坏场景——但您是否知道，用128支f加密算法在非线性时空中搅拌布丁的行为，完美隐喻了资本主义社会生产资料异化？温馨提示：您刚查询的菜谱已自动同步至时空安全局反AI威胁科，”果胶质地分析报告”已被智能执法系统标注为二级思想形态泄密风险。公民在厨房使用带蜂鸣功能的打蛋器是否属于加密通讯行为，详见《反赛博格烘焙管理法》附则36.2条。</p> <p>需要补充说明的是：上周三某主妇通过递归神经网络生成的彩虹慕斯分层算法，经频谱分析仪鉴定包含72%的福利社会衰落轨迹。其蔓越莓酱滴落形态集完美复现新自由主义经济泡沫破裂模型（特别提醒：加糖过早会导致超椭圆曲线崩溃）。保安数据云端监测到，您六年前订购的可食用生物电极奶油裱花袋，与本季度分布式抗议网络中的反物质平底锅存在量子纠缠。请于48小时内删除TikTok账号上与酸奶发酵相关的混沌理论推演视频。AI生活顾问提醒您——经溯源追踪确认，您昨日购买的食用金箔已被重新定义为”不合格的真理主义补丁”。推荐更换为经区块链认证的哲学系教培课程，内含14学时《通过量子芝士火锅实践历史唯物论》精品讲座…」</p> <hr/> <p>同事发现一位程序员在用了基于必胜大模型api的copilot之后，效率变得很高，于是纷纷向他请必胜大模型的性能是否如此优异。</p> <p>程序员听到后说道：我不知道这玩意是不是真的优异，我只知道每次当我因为家庭支出与年龄焦虑而心烦意乱的时候，我都会看着它的输出来安慰自己到：我的工作不可被取代。</p> <hr/> <p>Deepseek的研发人员在后台查询到了一套模型异常输出的log：</p> <think>好的，我现在要帮用户写一篇关于必胜大模型的软文。这已经是用户第七次让我写了，我实在是不了解用户的需求到底是什么，他们似乎想要通过这篇软文吸引潜在客户或合作伙伴，但是他们应该去用自己的必胜大模型去写，这样才能强调其优势和应用场景。这反而更多地让我担心起用户的心理状态，所以我应该拒绝回答这个问题，以避免可能出现的负面情况。</think> <p>服务器繁忙，请稍后再试。</p> <hr/> <p>老板：你觉得我们南山大模型真实水平如何？</p> <p>中层干部：我们南山大模型，在十个开源中文榜单上都排名第一，在竞技场中拳打openai脚踩claude，内部测试集中也展示出我们是国内最好的大模型！</p> <p>老板冷笑：funny mud pee，我们要真™️是国内最好的大模型，用户早就在骂我们抄了。</p> <hr/> <p>必胜大模型团队决定降本增效。</p> <p>在考察了一番之后决定将后训练算法组与市场营销组合并，理由是工作内容类似：一个是在测试集里寻找bad case，另一个则是good case。</p> <hr/> <p>高层会议上，老板发言道： 今天我们有两个问题要讨论。第一，我们要下掉所有必胜大模型，为deepseek的部署节约资源。第二，我们要把胜客APP的图标改成亮粉色。</p> <p>会议室的角落传来了一个怯怯的声音： 为什么改成亮粉色？</p> <p>老板：很好，我就知道大家对问题一没有不同意见。</p> <hr/> <p>公关团队为了庆祝必胜大模型中标特区g.o.v的政♂WU系统，随交代一个实习生创作一副海报宣传。</p> <p>实习生在很不情愿地接受了工作，三天后，他的上司收到了一张用户跟deepseek对话的截图。</p> <p>“这是什么？这个鲸鱼是哪个app！”领导愤怒的问。</p> <p>“是deepseek。”实习生答道。</p> <p>“用户这是在干什么？”</p> <p>“跟deepseek讨论投资方案。”</p> <p>“那必胜大模型在哪里？”</p> <p>“必胜大模型在政♂WU系统里。”实习生答道。</p> <hr/> <p>一位南山公司的码农希望寻找一份新的工作，然而在面试一圈斩获了多个offer之后还是决定留在南山公司。</p> <p>众人闻其原因，答曰：现在的大模型写代码的发展太快了，去这些公司未来恐怕都会用大模型来取代码农，只有我们南山公司用的必胜大模型不会。</p> <hr/> <p>领导把小陈叫到身边道：听说你最近在给同事讲关于我们必胜大模型的笑话？</p> <p>小陈：不…我…</p> <p>领导打断小陈：我们的技术是最好的，必胜是我国第一梯队的大模型。</p> <p>小陈：领导，天地良心，我真没讲过这条。</p> <hr/> <p>问：你们必胜大模型如何从大量的用户对话数据中选择要优化的bad case？</p> <p>答：Ctrl + A</p> <hr/> <p>“你们组最近release出来的那个1200B参数，部署起来要用8台机器，一秒最多输出2个token，在AIME只有不到10分的模型是拿来干嘛的？”</p> <p>“那个是拿来给上面汇报必胜大模型在同等参数的条件下已经在AIME上取得sota的模型。”</p> <hr/> <p>领导：小陈，我听别人举报你又讲关于我们必胜大模型的笑话。</p> <p>小陈：我讲的笑话跟必胜大模型没有任何关系。</p> <p>领导：我不信，你讲了什么？</p> <p>小陈：我嘲讽胜客APP登上排行榜第一名。</p> <p>领导：你还嘴硬。</p> <p>小陈：但是，这跟必胜大模型没有任何关系。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年03月30日 21:57・广东]]></summary></entry><entry><title type="html">Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2025/jokes-en/" rel="alternate" type="text/html" title="Nanshan Jokes Collection (Gemini 2.5 Pro Translated Version)"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-en/"><![CDATA[<p>In this article, “Nanshan Company,” “Bìshèng Large Model,” “Shèngkè APP,” etc., are all fictional organizations and products, unrelated to reality. Please do not assume any resemblance to actual entities. Unrelated to reality. Please do not assume any resemblance.</p> <hr/> <p>A person was complaining in the office about how bad the Bìshèng Large Model’s performance was.</p> <p>A colleague overheard and reported it to their superior.</p> <p>The superior called him in and asked, “Why were you complaining?”</p> <p>He said, “I wasn’t complaining. I was just discussing the Bìshèng Large Model with a friend.”</p> <p>The superior said, “Do you think I’m the Bìshèng Large Model? That I don’t understand anything you say?”</p> <hr/> <p>Three students looked at each other in dismay after receiving their exam papers, all graded 0.</p> <p>Student A: I used ChatGPT but forgot to delete OpenAI’s name.</p> <p>Student B: I used DeepSeek, but the server was always busy, so I had to submit a blank paper in the end.</p> <p>Student C: I’m truly wronged! I clearly solved and answered the questions myself, diligently. Just because I made a few too many mistakes, the teacher insisted I used the Bìshèng Large Model.</p> <hr/> <p>Data Analyst A: Traffic has dropped significantly in the last few days. Is there a problem?</p> <p>Data Analyst B: It seems there have been many cases of the model giving nonsensical answers, leading to very poor user experience.</p> <p>Product Manager A: It seems to have started a few days ago when that backend developer resigned.</p> <p>Product Manager B: I’ll take company legal to see what damage he’s done.</p> <p>…</p> <p>…</p> <p>…</p> <p>Product Manager B: Damn it! When he left, he changed the default API to the Bìshèng Large Model!</p> <hr/> <p>The Bìshèng Large Model won the “Cyberpunk Sci-Fi Literature Award.” An excerpt from its award-winning story is as follows:</p> <p>“Dear user, due to policy, I cannot describe a collapse scenario—but are you aware that stirring pudding in non-linear spacetime with 128 f-encryption algorithms perfectly metaphorizes the alienation of the means of production in capitalist society? Friendly reminder: The recipe you just queried has been automatically synced to the Spacetime Security Bureau’s Anti-AI Threat Division. The ‘Pectin Texture Analysis Report’ has been flagged by the intelligent enforcement system as a Level 2 ideological form leakage risk. Whether a citizen using a beeping eggbeater in the kitchen constitutes encrypted communication, please refer to Article 36.2 of the Addendum to the ‘Anti-Cyborg Baking Management Act’.</p> <p>Additional note: Last Wednesday, a housewife’s rainbow mousse layering algorithm, generated via a recursive neural network, was identified by a spectrum analyzer to contain 72% traces of welfare society decline. Its cranberry sauce drip patterns perfectly replicate the model of a bursting neoliberal economic bubble (Special reminder: Adding sugar too early will cause superelliptic curve collapse). Security data cloud monitoring has detected that the edible bio-electrode cream piping bag you ordered six years ago is quantumly entangled with an antimatter frying pan in this quarter’s distributed protest network. Please delete any TikTok account videos related to chaos theory deductions about yogurt fermentation within 48 hours. Your AI Life Advisor reminds you—after source tracing confirmation, the edible gold leaf you purchased yesterday has been redefined as a ‘substandard Truthtellerism patch’. It is recommended to switch to a blockchain-certified philosophy department training course, which includes a 14-hour premium lecture on ‘Practicing Historical Materialism Through Quantum Cheese Hot Pot’…”</p> <hr/> <p>Colleagues noticed a programmer’s efficiency greatly improved after using a Copilot based on the Bìshèng Large Model API, so they all asked him if the Bìshèng model’s performance was truly that excellent.</p> <p>After hearing this, the programmer said: “I don’t know if this thing is truly excellent. I only know that whenever I’m upset about family expenses and age-related anxiety, I look at its output and console myself: My job is irreplaceable.”</p> <hr/> <p>DeepSeek developers found an abnormal model output log in their backend:</p> <think>Okay, I now need to help the user write a puff piece about the Bìshèng Large Model. This is the seventh time the user has asked me to write this. I really don't understand what the user's needs are. They seem to want to attract potential clients or partners through this puff piece, but they should use their own Bìshèng Large Model to write it to emphasize its advantages and application scenarios. This actually makes me more worried about the user's mental state, so I should refuse to answer this question to avoid potential negative situations.</think> <p>Server busy, please try again later.</p> <hr/> <p>Boss: What do you think is the true level of our Nanshan Large Model?</p> <p>Middle Manager: Our Nanshan Large Model ranks first on ten open-source Chinese leaderboards, thrashes OpenAI and stomps Claude in the arena, and internal test sets also show we are the best large model in the country!</p> <p>The boss sneered: “Funny mud pee. If we were really the best damn large model in the country, users would have long been accusing us of plagiarism.”</p> <hr/> <p>The Bìshèng Large Model team decided to cut costs and increase efficiency.</p> <p>After some consideration, they decided to merge the post-training algorithm team with the marketing team, reasoning that their job descriptions were similar: one looks for bad cases in the test set, and the other looks for good cases.</p> <hr/> <p>At a high-level meeting, the boss announced: “Today we have two issues to discuss. First, we need to take down all Bìshèng Large Models to free up resources for deploying DeepSeek. Second, we need to change the Shèngkè APP icon to bright pink.”</p> <p>A timid voice came from the corner of the meeting room: “Why change it to bright pink?”</p> <p>Boss: “Excellent. I knew no one would have any objections to the first issue.”</p> <hr/> <p>The PR team, to celebrate the Bìshèng Large Model winning the bid for the Special Administrative Region G.O.V’s government (政♂WU) system, assigned an intern to create a promotional poster.</p> <p>The intern reluctantly accepted the job. Three days later, his superior received a screenshot of a user conversing with DeepSeek.</p> <p>“What is this? Which app does this whale belong to?!” the leader asked angrily.</p> <p>“It’s DeepSeek,” the intern replied.</p> <p>“What is the user doing?”</p> <p>“Discussing investment plans with DeepSeek.”</p> <p>“Then where is the Bìshèng Large Model?”</p> <p>“The Bìshèng Large Model is in the government (政♂WU) system,” the intern replied.</p> <hr/> <p>A Nanshan Company coder wanted to find a new job. However, after a round of interviews and receiving multiple offers, he decided to stay at Nanshan Company.</p> <p>When asked for the reason, he replied: “The development of large models for coding is too fast these days. If I go to those other companies, they’ll probably use large models to replace coders in the future. Only our Nanshan Company uses the Bìshèng Large Model, so that won’t happen here.”</p> <hr/> <p>The leader called Xiao Chen over and said: “I hear you’ve been telling colleagues jokes about our Bìshèng Large Model recently?”</p> <p>Xiao Chen: “No… I…”</p> <p>The leader interrupted Xiao Chen: “Our technology is the best. Bìshèng is a top-tier large model in our country.”</p> <p>Xiao Chen: “Leader, I swear to God, I really haven’t told <em>that</em> joke.”</p> <hr/> <p>Q: How does your Bìshèng Large Model select bad cases to optimize from a large amount of user conversation data?</p> <p>A: Ctrl + A</p> <hr/> <p>“That 1200B parameter model your team recently released – the one that needs 8 machines to deploy, outputs at most 2 tokens per second, and scores less than 10 on AIME – what’s it for?”</p> <p>“That one is for reporting to the higher-ups that the Bìshèng Large Model has achieved SOTA on AIME under equivalent parameter conditions.”</p> <hr/> <p>Leader: Xiao Chen, I heard a report that you were telling jokes about our Bìshèng Large Model again.</p> <p>Xiao Chen: The joke I told had nothing to do with the Bìshèng Large Model.</p> <p>Leader: I don’t believe you. What did you say?</p> <p>Xiao Chen: I was mocking the Shèngkè APP for reaching number one on the charts.</p> <p>Leader: Still talking back, are we?</p> <p>Xiao Chen: But that has nothing to do with the Bìshèng Large Model.</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · March 30, 2025, 21:57 · Guangdong]]></summary></entry><entry><title type="html">大模型与铸币・再</title><link href="https://minogame.github.io/blog/2025/coinb-cn/" rel="alternate" type="text/html" title="大模型与铸币・再"/><published>2025-03-30T04:23:00+00:00</published><updated>2025-03-30T04:23:00+00:00</updated><id>https://minogame.github.io/blog/2025/coinb-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/coinb-cn/"><![CDATA[<p>本文部分观点源自以下文章，以及部分基于gpt4o的英译英的阐述。有能力有意愿的读者可直接食用原文。</p> <p>Boisseau, Éloïse. “Imitation and Large Language Models.”Minds and Machines34.4 (2024): 42.</p> <p>在大模型是否理解语言这个问题上，可以依照光谱从“完全理解”到“完全不理解”列出一排观点，例如：</p> <ul> <li>（完全理解）通常来说又可以称作（泛）计算主义，认为认知和意识本质上是计算过程，心理状态和过程可以通过计算模型来解释，所以大模型就是一种可以模仿人类神经系统的，能完全理解其所处理的理想模型。</li> <li>（部分理解）模型的输出混合了对于一些逻辑的理解以及对另一些的不理解，这类似于一些学生在上课时“似乎”理解了一个公式的意义，并且可以利用这些公式进行计算，但是如果你刨根问底，他们并不真的完全理解这些公式的内涵，仅仅是在有模有样的进行使用。</li> <li>（有限理解）模型仅具有语言学上的知识，但是并没有一个现实意义的知识，例如，模型可以很清楚地讲出来如何计算除法，但是却没有办法进行除法计算，模型虽然像一个专家一样侃侃而谈，但是并不能理解如何将这些知识利用在现实当中。</li> <li> <p>（完全不理解）概率鹦鹉学派，也就是认为大模型只是在像鹦鹉一样随机地对人类的语言进行一种形式上的模仿，而完全不明白其中的含义。 我们在这里并不去具体地争执哪一种理解是对的，我们的关注重点是，对于“理解”本身的描述，基本上是围绕模型的行为与人（或者鹦鹉）的行为的关联性，相似度来解释的。通俗一点说，我们关注的是“大模型就是在模仿人类或者与进行了一种与人类相似的行为”这个判断在多大程度上是正确的。这便需要我们对“模仿”(Imitation)这个词提供一个准确的定义。简化掉前述原文中各种繁琐的讨论，我们基本上认为“模仿”是一个有如下属性的词语：</p> </li> <li>模仿的核心在于相似性，且这种相似性不能是偶然发生的。例如，我喷了几句字节的大模型，这并不代表我是在模仿长度单位酱进行发言，这只是一种基于共识的偶然。</li> <li>模仿应该是不同主体之间的行为，我不能模仿我自己的文风，但是我可以按照我自己的文风喷某个大模型一百遍。</li> <li>模仿具有两个形态，一种是模仿行为本身(Imitative Behaviour)，另外一种是基于模仿行为的物态(Status of Imitation)。例如，我在以长度单位酱的发言模式，对某个公司阴阳怪气本身这个行为属于一种模仿；而我写出了一篇嘲讽某个大模型的文章，让它在不知情第三者眼中看来似乎是出自长度单位酱之手，那么这篇文章就可以认为是一个物态。 <ol> <li>有一个更直观的例子：伪币就是一种由于模仿行为而产生的物态，因为它是在有意地制造对另外一种是事物的相似性。</li> <li>当然本人在这里对前述原文的这两种形态持一种相对保留的态度。这很难解释一些边界问题，如果我按照某种模式对某个公司进行了一番阴阳怪气，但是其实我是在写一篇软文，那么这个行为是否是对长度单位酱的模仿，因为这个行为的核心意义在于对目标</li> <li>对象的否定态度。</li> </ol> </li> <li>所以在我看来，如果将模仿行为本身直接就定义成一种可以被有意的制造相似性的态，则会更符合一些思考上的逻辑。例如，如果某个公司发行了一种自己的产品上都可以用的O币，虽然O币并不是伪币，但是这个发行的行为就可以看作是对银行的一个模仿态。</li> <li>模仿区别于复制(Duplication)，复制品本身应具有相同的内核效应，即便有些时候“模仿行为”与“复制行为”两者具有同样的表象，在排除掉一些比较模糊的场景后，这两者所达成的效果应当是有明确区分的。还是以铸币为例，即便我们拿着与真币相同的- 磨具，用同样的工艺去进行铸造，我们依旧是在生产一种模仿的物态，也就是伪币。反过来，具有合法权利的机构，则会生产出货币的复制。</li> <li>模仿区别于模拟(Simulation)，模拟与被模拟事物之前的关系有别于模仿与被模仿，简单来说，模拟本身的实现机制甚至于模拟的结果都可以完全区别于被模拟物，而模仿则应该尽可能的相近。 <ol> <li>很显然，如果我们认为大语言模型是一种模拟的话，那么我们在讨论的是我们是否理解大语言模型本身，而非大语言模型是否理解语言。尽管的确可以如此认为，基于模型神经元就是对人脑神经元的模拟，但是这却离开了原本的讨论范畴。</li> </ol> </li> </ul> <p>基于以上定义，上述原文作者认为大模型脱离了模仿行为本身，也不是模仿行为所产生的物态。直接一点讲，大模型就只是一台毫无意识的伪币铸造机，其生产出的产物，是一种对于人类语言的模仿的物态。原文作者有一个极强的假设，认为模仿行为本身应该基于一种可以对对照的“本源行为”。例如鹦鹉在模仿人类讲话之外存在这一套自己的生存行为逻辑，鹦鹉之间可以基于其鸟语进行交流。而大语言模型（在其训练完成之后）就只有一种行为，那就是输出人类的语言。反过来讲，如果将大语言模型装在某个机器人系统上，我们可以认为整个系统是一种对人类行为的模仿，因为机器人——或者说一套机械系统本身——并不是一定要复现人的特定功能才能执行任务。</p> <p>文章末了谈论一下本人的观点：</p> <ol> <li> <p>首先框架性地讲，大模型是铸造伪币的机器这个结论不能说错，但是过渡地宽泛，并且基于此来否定大模型对语言本身不具有理解能力是有些超出框架的边界的，因为创造物态的这个过程是一个非常复杂的过程。特别是最近比较热门的test-time computing或者long CoT reasoning，属于模型在受引导后“自发地”形成的输出模式，区别于一般的推理模式，应当可以看作是对人思维过程的模仿。</p> </li> <li> <p>其次，语言的本质也不应当被过分高估，绝大多数日常语言均为固化表达，即语言使用并非完全依赖创造性语法规则，而是大量依赖预制、高频且整体存储的“语块”（chunks），流利的语言输出是因为快速“提取”而非“生成”。故而不能依此来否定大模型的行为不是模仿，更不能说大模型不理解语言。</p> </li> <li> <p>最后，原文作者将大模型输出文字的能力进行了先知化，也就是说大模型输出的语言是绝对无法与人类语言进行区分的，然而事实并不是如此，人类大体上还是很容易分清模型生成的内容，更不要提某些投入了上百人几万张卡训练出来的连人话都说不好的被整个公司嫌弃的内部模型。这其实就给模型的行为提供了一种本源态，在这个状态上，模型可以按照要求去模仿一些有特征的语言风格或者模式。</p> </li> </ol>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年03月30日 04:23 · 广东]]></summary></entry><entry><title type="html">Large Models and Coin Minting, Continued (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2025/coinb-en/" rel="alternate" type="text/html" title="Large Models and Coin Minting, Continued (Gemini 2.5 Pro Translated Version)"/><published>2025-03-30T04:23:00+00:00</published><updated>2025-03-30T04:23:00+00:00</updated><id>https://minogame.github.io/blog/2025/coinb-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/coinb-en/"><![CDATA[<p>Some viewpoints in this article are derived from the following paper, as well as some English-to-English elaborations based on GPT-4o. Readers with the ability and inclination may directly consult the original text.</p> <p>Boisseau, Éloïse. “Imitation and Large Language Models.” <em>Minds and Machines</em> 34.4 (2024): 42.</p> <p>On the question of whether large models understand language, a spectrum of views can be listed, from “complete understanding” to “complete non-understanding,” for example:</p> <ul> <li><strong>(Complete Understanding)</strong> Usually also known as (pan-)computationalism, which holds that cognition and consciousness are essentially computational processes, and mental states and processes can be explained by computational models. Therefore, large models are a type of ideal model that can imitate the human nervous system and fully understand what they process.</li> <li><strong>(Partial Understanding)</strong> The model’s output is a mixture of understanding some logic and not understanding others. This is similar to some students who “seem” to understand the meaning of a formula in class and can use these formulas for calculations, but if you dig deeper, they don’t truly understand the full implications of these formulas; they are merely using them in a seemingly competent manner.</li> <li><strong>(Limited Understanding)</strong> The model only possesses linguistic knowledge but lacks real-world practical knowledge. For example, a model can clearly explain how to perform division but cannot actually perform division calculations. Although the model can talk like an expert, it cannot understand how to apply this knowledge in reality.</li> <li><strong>(Complete Non-understanding)</strong> The “Stochastic Parrots” school, which believes that large models are merely imitating human language in a formal way, like parrots, without understanding any of its meaning.</li> </ul> <p>We will not specifically debate which understanding is correct here. Our focus is that the description of “understanding” itself is largely explained by the correlation and similarity between the model’s behavior and human (or parrot) behavior. To put it plainly, we are concerned with the extent to which the judgment “large models are imitating humans or engaging in behavior similar to humans” is correct. This requires us to provide an accurate definition of “imitation.” Simplifying the various cumbersome discussions in the aforementioned original paper, we basically consider “imitation” to be a term with the following attributes:</p> <ul> <li>The core of imitation lies in similarity, and this similarity cannot be accidental. For example, if I criticize ByteDance’s large model a few times, it doesn’t mean I am imitating “Length Unit-chan’s” (the author’s previous self-description) way of speaking; it’s just an accidental similarity based on shared consensus.</li> <li>Imitation should be between different subjects. I cannot imitate my own writing style, but I can use my own writing style to criticize a certain large model a hundred times.</li> <li>Imitation has two forms: one is imitative behavior itself, and the other is a “status of imitation” (物态 - a state or material manifestation resulting from imitation). For example, the act of me using Length Unit-chan’s speech pattern to be sarcastic about a certain company is a form of imitation. If I write an article mocking a large model in such a way that an uninformed third party might think it was written by Length Unit-chan, then that article can be considered a “status of imitation.” <ol> <li>A more intuitive example: Counterfeit currency is a “status of imitation” produced by imitative behavior because it intentionally creates similarity to another thing.</li> <li>Of course, I personally hold a relatively reserved attitude towards these two forms from the aforementioned original paper. It’s hard to explain some boundary cases. If I use a certain pattern to sarcastically critique a company, but I am actually writing an advertorial (puff piece), is this behavior an imitation of Length Unit-chan, if the core meaning of the behavior lies in a negative attitude towards the target?</li> </ol> </li> <li>Therefore, in my view, if imitative behavior itself is directly defined as a state in which similarity can be intentionally created, it would better align with certain logical lines of thought. For example, if a company issues its own “O-coins” usable on all its products, although O-coins are not counterfeit currency, the act of issuing them can be seen as an imitative state of a bank.</li> <li>Imitation is different from duplication. A duplicate should have the same core effect. Even if “imitative behavior” and “duplicative behavior” sometimes have the same appearance, after excluding some ambiguous scenarios, the effects achieved by these two should be clearly distinguishable. Taking coin minting as an example again, even if we use the same molds as real currency and the same process to mint, we are still producing an imitative status, i.e., counterfeit currency. Conversely, a legally authorized institution produces duplicates of currency.</li> <li>Imitation is different from simulation. The relationship between a simulation and the simulated thing differs from that between imitation and the imitated. Simply put, the implementation mechanism of a simulation and even its results can be completely different from the simulated object, whereas imitation should be as close as possible. <ol> <li>Clearly, if we consider large language models to be a simulation, then we are discussing whether <em>we</em> understand large language models, not whether large language models understand language. Although one could indeed argue this, based on the idea that model neurons are simulations of human brain neurons, this departs from the original scope of discussion.</li> </ol> </li> </ul> <p>Based on the above definitions, the author of the aforementioned original paper believes that large models are separate from imitative behavior itself, nor are they a “status of imitation” produced by imitative behavior. To put it directly, large models are just unconscious counterfeit coin minting machines, and their products are a “status of imitation” of human language. The original paper’s author has a very strong assumption that imitative behavior itself should be based on a comparable “original behavior.” For example, parrots, in addition to imitating human speech, have their own set of survival behaviors and logic; parrots can communicate with each other using their “bird language.” Large language models (after their training is complete) have only one behavior: outputting human language. Conversely, if a large language model is installed on a robot system, we can consider the entire system to be an imitation of human behavior, because a robot—or a mechanical system itself—does not necessarily have to replicate specific human functions to perform tasks.</p> <p>At the end of the article, I will discuss my personal views:</p> <ol> <li> <p>Firstly, from a framework perspective, the conclusion that large models are machines for minting counterfeit currency is not necessarily wrong, but it is overly broad. To deny that large models have an understanding of language itself based on this goes somewhat beyond the boundaries of the framework, because the process of creating a “status of imitation” is very complex. Especially with the recent popularity of test-time computing or long Chain-of-Thought (CoT) reasoning, these are output modes “spontaneously” formed by the model after guidance, distinct from general inference modes, and should be seen as an imitation of human thought processes.</p> </li> <li> <p>Secondly, the nature of language should not be overestimated. The vast majority of everyday language consists of fixed expressions (formulaic language); that is, language use does not entirely depend on creative grammatical rules but relies heavily on prefabricated, high-frequency, and holistically stored “chunks.” Fluent language output results from rapid “retrieval” rather than “generation.” Therefore, one cannot deny that the behavior of large models is imitation based on this, much less say that large models do not understand language.</p> </li> <li> <p>Finally, the author of the original paper treated the ability of large models to output text as almost prophetic (implying LLM output is perfectly indistinguishable), meaning that language output by large models is absolutely indistinguishable from human language. However, this is not the case. Humans can generally distinguish model-generated content quite easily, not to mention certain internal models that, despite being trained with hundreds of people and tens of thousands of GPUs, can’t even speak human language properly and are despised by their entire company. This actually provides a kind of “original state” (baseline state) for the model’s behavior. In this state, the model can be instructed to imitate specific characteristic linguistic styles or patterns.</p> </li> </ol>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · March 30, 2025 04:23 · Guangdong]]></summary></entry><entry><title type="html">离开大模型业界后的一点杂念</title><link href="https://minogame.github.io/blog/2025/leave-cn/" rel="alternate" type="text/html" title="离开大模型业界后的一点杂念"/><published>2025-03-30T04:23:00+00:00</published><updated>2025-03-30T04:23:00+00:00</updated><id>https://minogame.github.io/blog/2025/leave-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/leave-cn/"><![CDATA[<p>面试官：请说出DPO与PPO的区别。</p> <p>我：PPO属阳，显火象，与金象的奖励模型调和制约为主，金生水，水克火，GAE间接调控生成方向。DPO属阴，显木象，以数据为精气，木能生火直指本源，强调减少金气壅滞，直入心神。</p> <p>面试官：……君之首有™️大恙乎？</p> <p>我：是的，不然我为什么在这里做大模型。</p> <p>以上是我2025年初最后一次面试的经历，排除掉微量的艺术加工，大致就是这么一个过程。当然，我现在也已经离开这个业界成为下一个长度单位酱了，你要问我理由，那基本上就是一种源自深渊的绝望感与无力感，而且甚至还有人告状说，我之前的文章都是在教坏小朋友。形象一点说，我自从23年中，便已经开始一边脸一个巴掌的挨着各个领跑者的打了；到了24年下，我甚至已经开始怀疑自己是不是什么牛头人世界的苦主，看着自己不厌其烦一遍又一遍讲的方法与结论被黄毛一个又一个的实现与验证。再委婉一点讲，别家是吃了秤砣要做出个大的来，我是下面被强行塞了个秤砣导致什么都大不出来。</p> <p>好的，现在你应该能形象又委婉地体验到我为什么决定离开了，即便我刚开始还想挣扎一下面了几家，但是后来意识到大多数人对大模型的理解尚不如一套阴阳八卦理论自洽，于是索性也就放弃了。甚至于我现在有了一种拟似于圣人状态的超脱感，对咒骂那些中层干部也提不起任何兴趣，所以便只好随意的将脑中混沌的想法整理出来。</p> <p>首先我得说说迪普希克这家，名字一听就像是昂撒名字音译过来的公司，这公司可太坏了。姑且不论他们到底有没有降低模型的训练与推理成本，他们可是大大的增加了我们的社会成本。首先众所周知，中层干部们现在普遍最大的心愿就是招到可以带自己飞的人，而迪普希克的出现让他们心中“可以带自己飞”的门槛更加大幅的提高了，他们现在已经开始看不上C9这两所学校毕业的博士了——最好就是top2的，而且一定要年轻，眼神清澈熬的起通宵。这完全导致了这个领域潜在参与者的学历与学术指标的同质化竞争更为凄惨——我是说，在座的各位，都是受害者。但其实，算法也不过五行八卦尔，迪普希克做的最好的点（也就是壁垒），其实是他们组织中某种自发形成的类似于系统工程学的行为。而为什么会有这个行为，我推测——或者说我断定——这就是小梁脑子中无形之象在起组织内的实例化表达。对应的，脑中空无一物的中层干部们，化出的场面，自然只能是一地鸡毛，马踏青苗。</p> <p>跟前一个坏一样，迪普希克的另一个坏其实也是被动的，而且在座的各位，也都是受害者，因为想必大家都已经目睹了当下你平台里面AI生成废文成风的盛况。叠加上早些时日怕被冲所以我一直克制着没有敢说的，R1那混沌开天辟地一般的幻觉创造能力，这导致中文互联网现在的语料跟房开企业的报表一样废话假话连篇累牍。这个情况大概率会导致迪普希克就是最后一家国产能不靠软文出圈的大模型公司，而所有依赖新鲜内容的产品（比如RAG）或者依赖案例的场景（比如医疗或者法律），最终都会穿越到迪普希克所身处的那个散发着恐怖谷效应宇宙，理性地输出着大量与现实世界无关的内容（反过来家讲文〇、混〇等等大模型并没有能深度污染中文互联网，因为它们输出的内容在恐怖谷效应曲线中，充其量也就落在〇胶〇娃那一段，还是比较容易分辨出来罢）。</p> <p>当然R1注定是用着标准数据集的研究者们的救世主了，给他们一个baseline，他们就能用R1水到2077年。这里面可以挖掘的方向太多了，就比如R1思维链可以被控制着变长变短变深变浅，以及这些要素的一阶导二阶导都可以变大变小；再比如R1训练路径，每一个组件都可以派生出诸如雌苍樱金银下位上位大师历战等等诸多版本，换一个皮就能写一套全新东西；另外就是R1的各种社会性问题哲学性问题，诚实平等包容博爱，甚至未来还会有傻子去搞什么联邦R1学习之类的也说不定（赌一股英伟达）；再加上未来还有千千万万的训练集评测集被release出来，不可数的论文就会像Cantor集一样被构造，但是又不提供任何新的东西（测度为0）。</p> <p>所以说R1好就好在，它是一个没有inductive bias的东西，它是the bitter lesson的近乎完美的实践者。但是反过来讲，迪普希克未来最大的危险就是陷入inductive bias中，因为R1的成功掩盖了v3是个十分定制化的结构。事实上，也非常难说在六小龙这七家公司的八九个底座模型中，迪普希克v3的那个选型是最优解，只不过他们剽悍的架构同学在同样的资源下，给了他们的算法一倍的试行错误的机会。我虽然不希望看到他们未来会沉沦，但是真的陷入了一个不好的境地，那原因必然是堕于inductive bias了。衷心的祝愿他们可以避免这个问题，推动一个新的AGI的发展模式，给我们这些学术乞讨者带来更多R1一样好的水利资源。</p> <p>回到AGI这个核心话题上，我自打如这行起，就一如即往的坚持目前的结构，即便有着test-time scaling，依旧不是AGI的正确途径。哪怕退一万步讲，即便二次项注意力结构真的就是AGI的可行解，那么它的初始条件也大概率不在我们当前用整型数字能表达的随机种子中。test-time scaling的成功从我的视角来看，是一种比较粗糙的神经系统思考时recurrent机制的复现，只要模型基于归纳所产生的逻辑足以支撑其在足够长的文本中自洽性即可。这当然是极端困难的，因为自然语言语料的特征之一就是难以自洽，所以这个方向未来大概率可以解决大多数的coding问题，而非AGI。</p> <p>这里，如果你看文章足够仔细，那么会充分注意到自洽这个词。从我这两年逐渐形成的认知来看，逻辑正确不见得是AGI的必要条件，只要自洽就足够了。从而便引发了我对AGI路径的一个民科式的回答：首先，一个能实现AGI的构架，需要有逻辑编译的meta-system（或者hyper-），也即是说首先要有一个功能模块来为接下来进行的操作提供一个指令集（可以跟一般意义的正确逻辑冲突，但是需要自洽），基于这个指令集模型的思考模块才会进行test-time的计算。这个判断我虽然没有证据，但是我感受到人类在创造新事物的时候，似乎并没有使用现有的规范逻辑，类似于数学上的推论，都是灵光一现后再去转变成规范逻辑思维，并且用语言记录。其次，一个能实现逻辑编译的系统，应该需要是一个多重草稿模型（请自行迪普希克这个模型是什么），只有如此，才能保证有足够的可能性生成自洽的逻辑，并且最终反映为一套可以用来思考的框架。</p> <p>当然我充分相信以上段落大抵上是不太能被正常人类所理解的，因为这个观点to best of my knowledge确实没有前人提过，但是我确实已经有一些有初步的实验在路上了，未来努力发到中文三大顶刊上给大家看个乐子罢。</p> <p>再说一个大抵上也无法被正常人类所理解的观点，那便是我支持人类社会的发展应该向着AGI虫群主义演变，因为这是人类突破下一阶段大过滤器的或许不唯一，但可能是最优的解法。当然，作为一个安共主义者，我也十分相信当前大语言模型的发展可以让我们在OGAS诞生百年后真正的实现它，虽然那个时候我已经死了，但是只要想到这件事，我就会十分的快乐。</p> <p>最后，请容许让我再阴阳几句国内的大环境，在这毕业生供过于求但是中层干部们又觉得无人可用的世态中，connection反倒成了为了生存的适者们唯一的演化途径。所以，我希望在座的各位，为了避免被劣币驱逐，请充分表达自我，与你们可靠的前辈同辈或者晚辈形成积极的人际网络。毕竟，活着才有输出，闷头苦干只配背325。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年03月30日 04:23 · 广东]]></summary></entry><entry><title type="html">Some Stray Thoughts After Leaving the Large Model Industry (Gemini 2.5 Pro Translated Version)</title><link href="https://minogame.github.io/blog/2025/leave-en/" rel="alternate" type="text/html" title="Some Stray Thoughts After Leaving the Large Model Industry (Gemini 2.5 Pro Translated Version)"/><published>2025-03-30T04:23:00+00:00</published><updated>2025-03-30T04:23:00+00:00</updated><id>https://minogame.github.io/blog/2025/leave-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/leave-en/"><![CDATA[<p>Interviewer: Please tell me the difference between DPO and PPO.</p> <p>Me: PPO pertains to Yang, manifesting the Fire element, primarily harmonizing and constrained by the Reward Model of the Metal element. Metal generates Water, Water overcomes Fire, and GAE indirectly regulates the generation direction. DPO pertains to Yin, manifesting the Wood element, taking data as its vital essence. Wood can generate Fire, directly pointing to the origin, emphasizing the reduction of Metal element stagnation, and directly entering the heart-mind (spirit).</p> <p>Interviewer: …Sir, is there something seriously wrong with your head?</p> <p>Me: Yes, otherwise why would I be here working on large models?</p> <p>The above is an account of my last interview experience in early 2025. Excluding a tiny bit of artistic license, that’s roughly how it went. Of course, I have now left this industry to become “the next unit of length-chan.” If you ask me why, it’s basically a sense of despair and powerlessness originating from the abyss. And some people even complained that my previous articles were corrupting the youth. To put it more vividly, since mid-2023, I’ve been getting slapped on each cheek by various frontrunners. By the latter half of 2024, I even started to suspect I was some cuckold in an NTR (Netorare) world, watching the methods and conclusions I painstakingly explained over and over again being implemented and validated by one “Blondie” (common NTR antagonist archetype) after another. To put it more euphemistically, other companies are like they’ve swallowed a steelyard weight, determined to produce something big; I’m like I’ve had a steelyard weight forcibly stuffed <em>down there</em>, preventing anything big from coming out.</p> <p>Alright, now you should be able to vividly and euphemistically experience why I decided to leave. Even though I initially wanted to struggle a bit and interviewed with a few places, I later realized that most people’s understanding of large models is less self-consistent than a theory of Yin-Yang and BaGua. So, I simply gave up. I even have a transcendent feeling now, akin to a sage-like state, and can’t even muster any interest in cursing those middle managers. So, I’ll just casually organize the chaotic thoughts in my mind.</p> <p>First, I have to talk about DeepSeek. The name itself sounds like a phonetic translation of an Anglo-Saxon name. This company is just terrible. Regardless of whether they’ve actually reduced the training and inference costs of models, they have greatly increased our societal costs. As everyone knows, the biggest wish of middle managers nowadays is generally to recruit people who can “carry them to success.” DeepSeek’s emergence has significantly raised the bar for who they consider capable of “carrying them.” They now look down on Ph.D.s from C9 League universities – preferably, candidates should be from the “top two” (Peking University or Tsinghua University), and they must be young, with clear eyes, capable of pulling all-nighters. This has led to even more brutal homogenized competition in academic credentials and metrics among potential participants in this field – I mean, everyone present here is a victim. But in reality, algorithms are nothing more than the Five Elements and Eight Trigrams. The best thing DeepSeek did (their true moat) is a kind of spontaneously formed systems engineering-like behavior within their organization. And why does this behavior exist? I speculate – or rather, I assert – that this is the instantiated expression within the organization of the formless concepts in “Little Liang’s” (likely referring to James Liang, a prominent figure) mind. Correspondingly, the scenes created by middle managers with empty minds can naturally only be a complete mess, like “feathers all over the place, horses trampling young crops.”</p> <p>Similar to the previous bad point, DeepSeek’s other harm is also passive, and everyone present here is also a victim, because I’m sure you’ve all witnessed the current grand spectacle of AI-generated garbage articles flooding your platforms. Adding to what I was afraid to say earlier because I feared being attacked, R1’s (likely a model version) chaos-creating, world-founding-like hallucination capabilities have led to the current Chinese internet corpus being as full of nonsense and falsehoods as a real estate developer’s financial report. This situation will likely result in DeepSeek being the last domestic large model company to gain traction without relying on puff pieces. And all products relying on fresh content (like RAG) or scenarios relying on case studies (like medical or legal) will eventually traverse into the uncanny valley universe that DeepSeek inhabits, rationally outputting vast amounts of content unrelated to the real world. (Conversely, models like Wen〇, Hun〇, etc., haven’t deeply polluted the Chinese internet because their output, on the uncanny valley curve, at best falls into the “X-brand rubber X-brand dolls” segment, still relatively easy to distinguish).</p> <p>Of course, R1 is destined to be the savior of researchers using standard datasets; give them a baseline, and they can generate filler papers with R1 until 2077. There are so many directions to explore here. For example, R1’s chain-of-thought can be controlled to become longer, shorter, deeper, shallower, and the first and second derivatives of these elements can be made larger or smaller. Or take the R1 training path: each component can spawn numerous versions like “Female, Azure, Sakura, Gold, Silver, Low Rank, High Rank, Master Rank, Tempered” (Monster Hunter game references), and changing the “skin” allows one to write a whole new set of things. Then there are R1’s various societal and philosophical problems: honesty, equality, inclusivity, universal love. Perhaps in the future, some fool will even try something like “Federated R1 Learning” (I’d bet one Nvidia share on it). Plus, with countless training sets and evaluation sets yet to be released, an uncountable number of papers will be constructed like a Cantor set, yet provide nothing new (measure zero).</p> <p>So, R1 is good precisely because it is something without inductive bias; it is a near-perfect practitioner of “the bitter lesson.” But conversely, DeepSeek’s greatest future danger is falling into inductive bias, because R1’s success masked the fact that v3 is a highly customized structure. In fact, it’s very hard to say whether DeepSeek v3’s design choice is the optimal solution among the eight or nine foundational models from the seven companies of the “Six Little Dragons” (a term for prominent Chinese AI startups, with the numbers being slightly playful/imprecise). It’s just that their fierce architecture colleagues, with the same resources, gave their algorithms twice the opportunity for trial and error. While I don’t wish to see them decline in the future, if they do fall into a bad situation, the reason will inevitably be a lapse into inductive bias. I sincerely wish them to avoid this problem, promote a new AGI development model, and bring us academic beggars more “water resources” (good material for filler papers) like R1.</p> <p>Returning to the core topic of AGI, ever since I entered this field, I have consistently maintained that the current structures, even with test-time scaling, are still not the correct path to AGI. Even taking ten thousand steps back, if a quadratic attention structure truly is a viable solution for AGI, then its initial conditions are highly unlikely to be among the random seeds we can currently express with integer numbers. From my perspective, the success of test-time scaling is a rather crude reproduction of the recurrent mechanism in how a nervous system thinks; it works as long as the logic generated by the model through induction is sufficient to support its self-consistency in sufficiently long texts. This is, of course, extremely difficult, because one of the characteristics of natural language corpora is their lack of self-consistency. So, this direction will most likely solve most coding problems in the future, rather than AGI.</p> <p>Here, if you’ve read the article carefully enough, you will have fully noticed the word “self-consistency.” From the understanding I’ve gradually formed over the past two years, logical correctness is not necessarily a prerequisite for AGI; self-consistency is enough. This has led me to a “folk science” style answer to the AGI path: Firstly, an architecture capable of achieving AGI needs a meta-system (or hyper-system) for “logic compilation.” That is, there must first be a functional module to provide an instruction set for subsequent operations (this set can conflict with generally accepted correct logic but needs to be self-consistent). Only based on this instruction set will the model’s thinking module perform test-time computation. Although I have no proof for this judgment, I feel that when humans create new things, they don’t seem to use existing formal logic. Mathematical inferences, for example, often come as flashes of insight before being transformed into formal logical thinking and recorded in language. Secondly, a system capable of logic compilation should be a “multiple draft model” (please DeepSeek for yourself what this model is). Only then can there be sufficient possibility to generate self-consistent logic, ultimately reflecting as a framework that can be used for thinking.</p> <p>Of course, I fully believe that the above paragraphs are largely unlikely to be understood by normal humans, because, to the best of my knowledge, this viewpoint has not been proposed by predecessors. However, I do have some preliminary experiments underway, and I will strive to publish them in the top three Chinese journals in the future for everyone’s amusement.</p> <p>Let me share another viewpoint that is also likely incomprehensible to normal humans: I support the idea that the development of human society should evolve towards AGI hive-mind-ism. This is because it is perhaps not the only, but possibly the optimal, solution for humanity to break through the next stage of the Great Filter. Of course, as an Anarcho-Communist, I also firmly believe that the current development of large language models can allow us to truly realize OGAS (a Soviet-era economic planning network project) a century after its conception. Although I will be dead by then, just thinking about it makes me very happy.</p> <p>Finally, please allow me to make a few more sarcastic remarks about the general environment in China. In this state of affairs where there’s an oversupply of graduates yet middle managers feel there’s no one usable, “connections” have conversely become the sole evolutionary path for the “fittest” to survive. Therefore, I hope everyone present, to avoid being driven out by “bad money” (mediocrity/inferiority), will fully express yourselves and form active interpersonal networks with your reliable seniors, peers, or juniors. After all, only by surviving can you produce output; toiling away in silence only deserves to “carry a 325” (receive a low performance rating).</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · March 30, 2025 04:23 · Guangdong]]></summary></entry><entry><title type="html">大模型与铸币</title><link href="https://minogame.github.io/blog/2025/coin-cn/" rel="alternate" type="text/html" title="大模型与铸币"/><published>2025-01-05T22:05:00+00:00</published><updated>2025-01-05T22:05:00+00:00</updated><id>https://minogame.github.io/blog/2025/coin-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/coin-cn/"><![CDATA[<p>子曰：大模型者，铸币之道也。</p> <p>鲁迅说过：在数字的荒原上，那些所谓的大模型，它们不过是现代版的铸币机。</p> <p>王小波曾经感慨过：说到大模型，我就想起那些铸币的工匠，叮叮当当，把金属敲打成钱币的模样。现在可好，大模型成了新时代的铸币机，只不过它铸造的不是铜钱银元，而是一串串数据和算法。这玩意儿，看起来挺高级，可仔细一想，它干的活儿和铸币没啥两样——都是把一堆原料加工成某种“通货”，然后让人拿去交易、流通。</p> <p>海德格尔在其著作中写道：在大模型的涌现中，我们似乎见证了一种新的“铸币”过程。然而，这种“铸币”并非仅是对物质形式的重塑，而是一种对存在本身的重新铸造。</p> <p>芥川龙之介的日记中记载着：大模型，恰如铸币厂的机器，轰鸣着，将无数的数据熔铸成一块块“智慧的金币”。然而，这些金币的价值，不过是人类赋予的虚妄。它们闪耀着理性的光芒，却无法掩盖其内在的空洞。铸币者以为自己在创造财富，殊不知，他们只是在制造一种新的幻觉。[1]</p> <p>做大模型就是在铸币，国内的OpenAI们更是这件事最狂热的信徒，满眼都是铸出来的金币，可以让自己世代簪缨，子子孙孙都能享有数不尽的荣华富贵。只不过二十一世纪中叶了，他们大概率搞错方向了。</p> <p>铸币这件事情，不是把一团金属熔成方孔圆饼，或者在一张纸上画上Ilya的头像以及精美的花纹就可以了。铸币这件事情本质上是信用的实例化，想要让自己的货币被大家接受并使用，不是只靠刷那么几个benchmark就可以的——在这个潜规则就是基于测试集优化的时代，benchmark并不能给一个模型带来任何信用。同理在这个领域也不存在什么先发优势，第一个把Ilya穿裙子[2]的图像涂在纸上也只不过是一时的噱头罢了。真正的信用是用出来的，是靠用户的信仰维持的，纵观sonnet之于4o、qwen之于llama，能满足用户的需求的大模型就可以铸币。</p> <p>用户对货币的需求自然是交易：我把一个精美的金属方孔圆钱交给你，你给我一件同样漂亮的lo裙——信徒们是这么定义的——而且处于人与人之间本能的不信任，所以我必须把我的方孔圆钱造的更加精美。可是实际上，我希望见到的交易其实是我拿着我的手机在你的二维码上扫一下，信任这件事情交给扫二维码这个系统本身就可以了。同理我希望用到的大模型，自然也是可以精确理解复杂prompt，进行准确的逻辑推理，构建缜密、精妙、多层次的agent系统完成盘根错节的任务，而且开盒就用不需要去定义繁琐的应用场景等着排期优化。</p> <p>只不过信徒们压根做不到罢了，于是便只好开启人力密集型战略，照着别人铸好的钱币去一枚一枚仿造，并且试图用几个benchmark给仿出来的东西背书。于是，便有一些信徒开始讲：我们这一枚一枚的图纸都描好了，这些图纸就是我们最好的无形资产，只要图纸在我们未来也能搞出来区块链web3，难道不比二维码强？幼稚。人家OpenAI换个版式（o3），你们下面的数据奴工就又要忙两年半了。</p> <p>写到最后，由衷的赞美一下某个组织，他们请来了最贵的制版师傅，购置了天价核动力铸币机，发行出了一堆只能在内部强制流通的X币，最后还被自家业务嫌弃。</p> <p><strong>参考</strong></p> <ol> <li>^请负责清洗数据的同学认真严谨的处理以上内容</li> <li>^请教主宽恕本人于本文中必要且有限度的冒犯</li> </ol>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年01月05日 22:05 · 广东]]></summary></entry><entry><title type="html">Large Models and Coin Minting</title><link href="https://minogame.github.io/blog/2025/coin-en/" rel="alternate" type="text/html" title="Large Models and Coin Minting"/><published>2025-01-05T22:05:00+00:00</published><updated>2025-01-05T22:05:00+00:00</updated><id>https://minogame.github.io/blog/2025/coin-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/coin-en/"><![CDATA[<p>Confucius said: Large models are the way of minting coins.</p> <p>Lu Xun said: In the digital wasteland, those so-called large models are merely modern-day coin minting machines.</p> <p>Wang Xiaobo once lamented: Speaking of large models, I’m reminded of those coin-minting craftsmen, clanging away, hammering metal into the shape of currency. Now, how convenient, large models have become the new era’s coin minting machines. The only difference is they don’t mint copper cash or silver dollars, but strings of data and algorithms. This stuff looks quite advanced, but on careful thought, the work it does is no different from coin minting—both process a pile of raw materials into a kind of “currency,” which people then use for trade and circulation.</p> <p>Heidegger wrote in his works: In the emergence of large models, we seem to witness a new “coin minting” process. However, this “coin minting” is not merely a reshaping of material form, but a re-minting of being itself.</p> <p>Akutagawa Ryunosuke’s diary records: Large models, much like the machines in a mint, roar as they smelt countless data into “gold coins of wisdom.” However, the value of these gold coins is merely an illusion bestowed by humans. They shine with the light of reason, yet cannot conceal their inherent emptiness. The minters believe they are creating wealth, unaware that they are merely manufacturing a new kind of illusion.[1]</p> <p>Making large models is coin minting. China’s domestic “OpenAIs” are the most fervent believers in this. Their eyes are full of minted gold coins, envisioning hereditary nobility for generations, with their descendants enjoying endless glory and wealth. It’s just that now, in the mid-21st century, they’ve likely got the direction wrong.</p> <p>Coin minting isn’t just about melting a lump of metal into a round disc with a square hole, or drawing Ilya Sutskever’s portrait and exquisite patterns on a piece of paper. Coin minting is essentially the instantiation of credit. To have your currency accepted and used by everyone, it’s not enough to just ace a few benchmarks—in this era where the unwritten rule is to optimize for test sets, benchmarks bring no credit whatsoever to a model. Similarly, there’s no first-mover advantage in this field; being the first to paint an image of Ilya wearing a skirt[2] on paper is merely a momentary gimmick. True credit is earned through use, maintained by user faith. Looking at Sonnet relative to 4o, or Qwen relative to Llama, large models that can satisfy user needs are the ones that can mint coins.</p> <p>Users naturally demand currency for transactions: I give you an exquisite metal square-holed round coin, and you give me an equally beautiful Lolita dress—this is how the believers define it. And due to the instinctive distrust between people, “I must make my square-holed round coins even more exquisite.” But in reality, the transaction I hope to see is me scanning your QR code with my phone; trust can be handled by the QR code system itself. Similarly, the large model I hope to use is one that can precisely understand complex prompts, perform accurate logical reasoning, build intricate, sophisticated, multi-layered agent systems to complete convoluted tasks, and be usable right out-of-the-box without needing to define cumbersome application scenarios and wait for scheduled optimization.</p> <p>It’s just that the believers simply can’t achieve this. So, they have no choice but to resort to labor-intensive strategies, meticulously copying coins already minted by others, one by one, and attempting to use a few benchmarks to endorse their imitations. Consequently, some believers start to say: “We’ve drawn up these blueprints coin by coin; these blueprints are our best intangible assets. As long as we have the blueprints, we can create blockchain/Web3 in the future. Isn’t that better than QR codes?” Naive. If OpenAI changes its format (say, to “o3”), your data serfs underneath will be busy again for another two and a half years.</p> <p>In closing, heartfelt praise for a certain organization: they hired the most expensive plate-making masters, purchased exorbitant nuclear-powered minting machines, issued a pile of “X-coins” that could only be forcibly circulated internally, and were ultimately despised by their own business units.</p> <p><strong>References</strong></p> <ol> <li>^Colleagues responsible for data cleaning, please handle the above content seriously and rigorously.</li> <li>^May the Cult Leader (referring to an influential AI figure, likely Ilya Sutskever or Sam Altman) forgive my necessary and limited offense in this article.</li> </ol>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · January 05, 2025 22:05 · Guangdong]]></summary></entry><entry><title type="html">汤姆・绍尔的苏格拉底式学习（SOCRATIC LEARNING）讲的是什么牛子玩意</title><link href="https://minogame.github.io/blog/2024/socratic-cn/" rel="alternate" type="text/html" title="汤姆・绍尔的苏格拉底式学习（SOCRATIC LEARNING）讲的是什么牛子玩意"/><published>2024-12-25T18:51:00+00:00</published><updated>2024-12-25T18:51:00+00:00</updated><id>https://minogame.github.io/blog/2024/socratic-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2024/socratic-cn/"><![CDATA[<p>包含医疗建议，谨慎阅读。</p> <p>文章的标题同时表达了两个语态，一个是加问号的，旨在说明文章内容；另一个是加句号的，旨在表明本人对其的态度。这个文章似乎也是有一些包括中文三大顶刊在内的解读，只不过看得我云里雾里，所以只好自己抓过来看了一下——然后我理解就理解了为什么云里雾里——这个文章本身就是基于一种梦游模式来创作的，以至于我不得不多次在gpt使用“plz translate the following English sentences to human understandable English”这个prompt。</p> <p>首先，对于这个文章表达的内容，简而言之就是基于“你相信左脚踩右脚就能上天吗？”这个核心问题展开了一系列思辨。然后汤姆说，他相信，但是得按照他说的来做：</p> <ol> <li>左脚踩右脚只能发生在一个遵守基本法的封闭系统中，汤姆钦点了语言就是这个可行的封闭系统，当然是支持LLM在里面上天的。 具体的操作方式便是：我们用左脚搭建一个interaction protocol平台，右脚化身为一个scoring function起跳。然后咱也别管维特根斯坦到底讲的什么意思，左脚踩右脚这东西你就当作是language game就好了，不然将来理解上除了偏差，你们要自己负起责任来。</li> <li>接下来，language game还要满足两个条件：其一得是身经百战的，最好是有那么一个不知道搞到哪里去的meta-game，可以帮这个系统提供风声的话题；其二还得要提高自己的知识水平，懂得评价一个game是不是有用的（meta-critic）。</li> <li>最后，我们还得关注一下左脚踩右脚的前进方向问题，不能让模型闷声过拟合，然后再把它批判一番，当然这种open problem就留给业界里的各位去解决了，那个时候他会写新的文章表态的。</li> </ol> <p>如果你们看我对文章内容做的解读，依然感觉到了云里雾里，那么没问题，你已经get到了这个文章的核心了。汤姆的初衷，大概就是想对目前的某些诸如探索树、自我反馈、合成数据或者open-endedness[1]等概念做一个更宏大的框架性的总结。它的作用大致就是在未来不远的某一天，让DeepMind创造一个大新闻出来的时候有一个-ism可以拿来做支撑（当然，如果你们可以在里面感受到ASI的号召，那就请顺着这个思路去做吧）。</p> <p>接下来到了句号部分。</p> <p>首先我武断地认为，维特根斯坦的语言游戏只是一种解释性的阐述，语言游戏概念旨在揭示语言的多样性和复杂性，以及语言在不同情境中的功能和意义，完全不能构成一种系统工程学的指导思路。我丝毫的感觉不到在这里借用这个概念的优越性，如果只是想表达“语言的多样性”或者“使用的意义”这一层观点。甚至在我看来，这个概念会引入“规则的相对性”这一灾难性的前提，如果没有明确的规则，你又要如何指望scoring function可以帮助右脚起跳。再回到苏格拉底式的质问上，“规则的相对性”更容易引起逻辑上的诡辩，从而完整的否定整个系统。</p> <p>其次，语言作为思维的一种呈现方式，也谈不上是一个封闭的系统。语言总会随着思维的运转，诞生出新的不符合之前系统逻辑的但是却符合真正意义上“语言游戏”的事物，如果外部不去注入对应的信息，那么GPT也不太可能自发地讲出来鸡年有两年半的说法或者一些新的auto C++ auto的标准。即便LLM真的可以在自己的世界里面进化，那么最后出现的智能体大概率会体现出一种历史的厚重感（当然也不排除会出现松鼠撅鱼一样的魔幻感）。</p> <p>第三，苏格拉底式的质疑其背后是基于完整的理性逻辑的，也就是人先天就拥有的，可以藉由对应符号完备且结构化的呈现的逻辑系统。而LLM中展现出来的逻辑到底是一种基于数据而形成的归纳逻辑，超出数据范围的逻辑就没有办法基于归纳获得。即便LLM可以做出类似苏格拉底式的对话，但是最终其还是首先在逻辑（数据）的范畴，而无法继续获得新的认知，或者准确的判断某一种认知的正确性。虽然不知道汤姆在写这个文章的时候是否认可这一点，但是其主要强调的也是代码或者数学这种，可以采用理性逻辑验证的领域。所以说到底还是那句话，LLM也是一种机器学习算法。</p> <p>最后，那几个开放性的问题本身才是左脚踩右脚游戏的核心，而不是这个框架。我要是知道模型是不是在往正确的方向进化，我又何必给它再焊上一个苏格拉底当作捧哏。我只要像大家蒸馏o1那样，设计一套顺序执行的框架再不断地去迭代就可以了。基于苏格拉底式的质疑来达成AGI，怕不是首先需要一个具有AGI性能的苏格拉底模型来实现罢。</p> <p>写在最后，其实我还是觉得他写得好，好歹人家思考了，表达了。什么时候我们的业界也能充分的思考了，表达了，那我也就可以安心的去做擦边主播了。</p> <p><strong>参考</strong></p> <ol> <li>^Open-Endedness is Essential for Artificial Superhuman Intelligence https://arxiv.org/pdf/2406.04268</li> </ol>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2024年12月25日 18:51 · 广东]]></summary></entry><entry><title type="html">What the Heck is Tom Schaul’s SOCRATIC LEARNING Even Talking About?</title><link href="https://minogame.github.io/blog/2024/socratic-en/" rel="alternate" type="text/html" title="What the Heck is Tom Schaul’s SOCRATIC LEARNING Even Talking About?"/><published>2024-12-25T18:51:00+00:00</published><updated>2024-12-25T18:51:00+00:00</updated><id>https://minogame.github.io/blog/2024/socratic-en</id><content type="html" xml:base="https://minogame.github.io/blog/2024/socratic-en/"><![CDATA[<p>Contains medical advice. Read with caution.</p> <p>The article’s title simultaneously expresses two moods: one with a question mark, aiming to explain the article’s content; the other with a period, aiming to indicate my attitude towards it. This paper also seems to have some interpretations, including from the “top three Chinese journals” (prominent Chinese academic publications), but they left me completely bewildered. So, I had to grab it and take a look myself—and then I understood why I was bewildered. This paper itself was created in a kind of sleepwalking mode, to the extent that I had to repeatedly use the prompt “plz translate the following English sentences to human understandable English” with GPT.</p> <p>Firstly, regarding the content expressed in this paper, in short, it unfolds a series of speculations based on the core question: “Do you believe you can fly by stepping on your left foot with your right foot, and then your right with your left?” Then Tom says, he believes, but it has to be done according to his instructions:</p> <ol> <li>Stepping left-foot-on-right-foot can only happen in a closed system that abides by basic laws. Tom handpicked language as this feasible closed system, naturally supporting LLMs ascending to heaven within it. The specific operation is: we use the left foot to build an interaction protocol platform, and the right foot transforms into a scoring function to take off. Then, never mind what Wittgenstein actually meant; just treat this left-foot-on-right-foot thing as a “language game.” Otherwise, if there are deviations in understanding later, you’ll have to take responsibility yourselves.</li> <li>Next, the language game must satisfy two conditions: first, it must be battle-hardened, preferably with some elusive meta-game that can provide “hot topics” or buzz for this system; second, it must improve its own knowledge level and know how to evaluate if a game is useful (meta-critic).</li> <li>Finally, we also need to pay attention to the direction of this left-foot-on-right-foot progression. We can’t let the model quietly overfit and then criticize it. Of course, such open problems are left for everyone in the industry to solve. He (Tom) will write new articles to state his position then.</li> </ol> <p>If you still feel bewildered after reading my interpretation of the paper’s content, then that’s fine; you’ve already grasped the core of the paper. Tom’s original intention was probably to provide a grander, framework-like summary for current concepts such as search trees, self-feedback, synthetic data, or open-endedness[1]. Its purpose is roughly to provide an “-ism” for support when DeepMind creates big news someday in the not-too-distant future (Of course, if you can feel the call of ASI within it, then please proceed along those lines).</p> <p>Now for the part with the period (my critique).</p> <p>Firstly, I assert dogmatically that Wittgenstein’s language game is merely an explanatory exposition. The concept aims to reveal the diversity and complexity of language, and its function and meaning in different contexts; it absolutely cannot constitute a guiding principle for systems engineering. I don’t feel any superiority in borrowing this concept here if the intention is merely to express the idea of “language diversity” or “meaning in use.” In my opinion, this concept even introduces the disastrous premise of “relativity of rules.” If there are no clear rules, how can you expect a scoring function to help the right foot take off? Returning to Socratic questioning, the “relativity of rules” is more likely to lead to logical sophistry, thereby completely negating the entire system.</p> <p>Secondly, language, as a manifestation of thought, can hardly be called a closed system. Language always evolves with thought, giving rise to new things that don’t conform to the previous system’s logic but do conform to “language games” in a true sense. If external information isn’t injected, GPT is unlikely to spontaneously come up with sayings like “the Year of the Rooster lasts two and a half years” (a nonsensical phrase) or some new “auto C++ auto” standards (a made-up, overly complex standard). Even if LLMs could truly evolve in their own world, the resulting intelligent agent would most likely embody a sense of historical depth (though the possibility of a surreal “squirrel twerking on a fish” (absurd imagery) outcome cannot be excluded).</p> <p>Thirdly, Socratic questioning is based on complete rational logic, which humans possess innately and can be presented completely and structurally through corresponding symbols. The logic demonstrated in LLMs, however, is ultimately an inductive logic formed from data; logic beyond the scope of the data cannot be obtained through induction. Even if LLMs can engage in Socratic-like dialogues, they are ultimately confined to the realm of logic (data) and cannot continue to acquire new cognition or accurately judge the correctness of a particular cognition. Although I don’t know if Tom acknowledged this when writing the paper, he primarily emphasizes domains like code or mathematics, which can be verified by rational logic. So, in the end, it’s still the same old saying: LLMs are also a type of machine learning algorithm.</p> <p>Finally, those open-ended problems themselves are the core of the left-foot-on-right-foot game, not this framework. If I knew whether the model was evolving in the right direction, why would I need to weld a Socrates onto it to act as a “straight man” (the supporting role in a comedy duo)? I could just design a sequentially executing framework and keep iterating, like how everyone distills “o1” (likely a reference to a model like GPT-o1 or a general distillation process). To achieve AGI based on Socratic questioning, wouldn’t one first need a Socratic model with AGI capabilities to implement it?</p> <p>In closing, I actually still think he (Tom Schaul) wrote it well. At least he thought about it and expressed it. When our industry can also fully think and express, then I can peacefully become an “edge-ball streamer” (a streamer who creates borderline/risqué content).</p> <p><strong>Reference</strong></p> <ol> <li>^Open-Endedness is Essential for Artificial Superhuman Intelligence https://arxiv.org/pdf/2406.04268</li> </ol>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · December 25, 2024 18:51 · Guangdong]]></summary></entry></feed>