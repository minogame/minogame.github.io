<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://minogame.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://minogame.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-23T06:41:48+00:00</updated><id>https://minogame.github.io/feed.xml</id><title type="html">Zhun Sun</title><subtitle>Zhun Sun&apos;s personal page and blog storage. </subtitle><entry><title type="html">南山笑话集锦</title><link href="https://minogame.github.io/blog/2025/jokes-cn/" rel="alternate" type="text/html" title="南山笑话集锦"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-cn/"><![CDATA[<p>本文中「南山公司」、「必胜大模型」、「胜客APP」等均为虚构组织与产品，与现实无关，请勿对号入座。 与现实无关，请勿，对号入座。</p> <hr/> <p>一个人在办公室抱怨必胜大模型性能太差。</p> <p>结果被同事听到报告给了上级。</p> <p>上级把他叫去问：“你为什么要抱怨？”</p> <p>他说：“我没有抱怨，我只是在和朋友讨论必胜大模型。”</p> <p>上级说：“你以为我是必胜大模型吗，说什么都听不懂？”</p> <hr/> <p>三个学生拿到自己的被判0分试卷后面面相觑。</p> <p>甲：我用ChatGPT但是忘记删除了openai的名字。</p> <p>乙：我用deepseek结果一直是服务器繁忙，所以最后只好交了白卷。</p> <p>丙：我可是真的冤，明明是自己认认真真地解题作答，结果只是因为错的多了点，老师非说我是用必胜大模型写的。</p> <hr/> <p>数据分析师A：最近几天流量出现了显著跌幅，是出了什么问题了吗？</p> <p>数据分析师B：好像确实出了许多模型胡乱回答用户体验非常差的case</p> <p>产品经理A：好像前几天那个后端离职开始就这样了</p> <p>产品经理B：我带上公司法务去看看他搞了什么破坏</p> <p>…</p> <p>…</p> <p>…</p> <p>产品经理B：艹，他走的时候把默认API改成必胜大模型了</p> <hr/> <p>必胜大模型荣获”赛博朋克科幻文学奖”，其获奖小说片段如下：</p> <p>「亲爱的用户，根据政策我无法描述崩坏场景——但您是否知道，用128支f加密算法在非线性时空中搅拌布丁的行为，完美隐喻了资本主义社会生产资料异化？温馨提示：您刚查询的菜谱已自动同步至时空安全局反AI威胁科，”果胶质地分析报告”已被智能执法系统标注为二级思想形态泄密风险。公民在厨房使用带蜂鸣功能的打蛋器是否属于加密通讯行为，详见《反赛博格烘焙管理法》附则36.2条。</p> <p>需要补充说明的是：上周三某主妇通过递归神经网络生成的彩虹慕斯分层算法，经频谱分析仪鉴定包含72%的福利社会衰落轨迹。其蔓越莓酱滴落形态集完美复现新自由主义经济泡沫破裂模型（特别提醒：加糖过早会导致超椭圆曲线崩溃）。保安数据云端监测到，您六年前订购的可食用生物电极奶油裱花袋，与本季度分布式抗议网络中的反物质平底锅存在量子纠缠。请于48小时内删除TikTok账号上与酸奶发酵相关的混沌理论推演视频。AI生活顾问提醒您——经溯源追踪确认，您昨日购买的食用金箔已被重新定义为”不合格的真理主义补丁”。推荐更换为经区块链认证的哲学系教培课程，内含14学时《通过量子芝士火锅实践历史唯物论》精品讲座…」</p> <hr/> <p>同事发现一位程序员在用了基于必胜大模型api的copilot之后，效率变得很高，于是纷纷向他请必胜大模型的性能是否如此优异。</p> <p>程序员听到后说道：我不知道这玩意是不是真的优异，我只知道每次当我因为家庭支出与年龄焦虑而心烦意乱的时候，我都会看着它的输出来安慰自己到：我的工作不可被取代。</p> <hr/> <p>Deepseek的研发人员在后台查询到了一套模型异常输出的log：</p> <think>好的，我现在要帮用户写一篇关于必胜大模型的软文。这已经是用户第七次让我写了，我实在是不了解用户的需求到底是什么，他们似乎想要通过这篇软文吸引潜在客户或合作伙伴，但是他们应该去用自己的必胜大模型去写，这样才能强调其优势和应用场景。这反而更多地让我担心起用户的心理状态，所以我应该拒绝回答这个问题，以避免可能出现的负面情况。</think> <p>服务器繁忙，请稍后再试。</p> <hr/> <p>老板：你觉得我们南山大模型真实水平如何？</p> <p>中层干部：我们南山大模型，在十个开源中文榜单上都排名第一，在竞技场中拳打openai脚踩claude，内部测试集中也展示出我们是国内最好的大模型！</p> <p>老板冷笑：funny mud pee，我们要真™️是国内最好的大模型，用户早就在骂我们抄了。</p> <hr/> <p>必胜大模型团队决定降本增效。</p> <p>在考察了一番之后决定将后训练算法组与市场营销组合并，理由是工作内容类似：一个是在测试集里寻找bad case，另一个则是good case。</p> <hr/> <p>高层会议上，老板发言道： 今天我们有两个问题要讨论。第一，我们要下掉所有必胜大模型，为deepseek的部署节约资源。第二，我们要把胜客APP的图标改成亮粉色。</p> <p>会议室的角落传来了一个怯怯的声音： 为什么改成亮粉色？</p> <p>老板：很好，我就知道大家对问题一没有不同意见。</p> <hr/> <p>公关团队为了庆祝必胜大模型中标特区g.o.v的政♂WU系统，随交代一个实习生创作一副海报宣传。</p> <p>实习生在很不情愿地接受了工作，三天后，他的上司收到了一张用户跟deepseek对话的截图。</p> <p>“这是什么？这个鲸鱼是哪个app！”领导愤怒的问。</p> <p>“是deepseek。”实习生答道。</p> <p>“用户这是在干什么？”</p> <p>“跟deepseek讨论投资方案。”</p> <p>“那必胜大模型在哪里？”</p> <p>“必胜大模型在政♂WU系统里。”实习生答道。</p> <hr/> <p>一位南山公司的码农希望寻找一份新的工作，然而在面试一圈斩获了多个offer之后还是决定留在南山公司。</p> <p>众人闻其原因，答曰：现在的大模型写代码的发展太快了，去这些公司未来恐怕都会用大模型来取代码农，只有我们南山公司用的必胜大模型不会。</p> <hr/> <p>领导把小陈叫到身边道：听说你最近在给同事讲关于我们必胜大模型的笑话？</p> <p>小陈：不…我…</p> <p>领导打断小陈：我们的技术是最好的，必胜是我国第一梯队的大模型。</p> <p>小陈：领导，天地良心，我真没讲过这条。</p> <hr/> <p>问：你们必胜大模型如何从大量的用户对话数据中选择要优化的bad case？</p> <p>答：Ctrl + A</p> <hr/> <p>“你们组最近release出来的那个1200B参数，部署起来要用8台机器，一秒最多输出2个token，在AIME只有不到10分的模型是拿来干嘛的？”</p> <p>“那个是拿来给上面汇报必胜大模型在同等参数的条件下已经在AIME上取得sota的模型。”</p> <hr/> <p>领导：小陈，我听别人举报你又讲关于我们必胜大模型的笑话。</p> <p>小陈：我讲的笑话跟必胜大模型没有任何关系。</p> <p>领导：我不信，你讲了什么？</p> <p>小陈：我嘲讽胜客APP登上排行榜第一名。</p> <p>领导：你还嘴硬。</p> <p>小陈：但是，这跟必胜大模型没有任何关系。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2025年03月30日 21:57・广东]]></summary></entry><entry><title type="html">Compilation of Nanshan Jokes (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2025/jokes-en/" rel="alternate" type="text/html" title="Compilation of Nanshan Jokes (DeepSeek Translated Version)"/><published>2025-03-30T21:57:00+00:00</published><updated>2025-03-30T21:57:00+00:00</updated><id>https://minogame.github.io/blog/2025/jokes-en</id><content type="html" xml:base="https://minogame.github.io/blog/2025/jokes-en/"><![CDATA[<p>In this article, “Nanshan Company,” “Bisheng LLM,” and “Shengke APP” are all fictional organizations and products with no relation to reality. Please do not take them seriously. No relation to reality. Please do not take them seriously.</p> <hr/> <p>A person complained in the office about the poor performance of Bisheng LLM.</p> <p>A colleague overheard and reported it to their superior.</p> <p>The superior called them in and asked, “Why were you complaining?”</p> <p>They replied, “I wasn’t complaining, I was just discussing Bisheng LLM with a friend.”</p> <p>The superior said, “Do you think I’m Bisheng LLM, incapable of understanding anything?”</p> <hr/> <p>Three students stared at each other after receiving their exam papers, all marked with zero points.</p> <p>Student A: “I used ChatGPT but forgot to delete OpenAI’s name.”</p> <p>Student B: “I tried DeepSeek, but the server kept crashing, so I had to submit a blank paper.”</p> <p>Student C: “I’m the real victim here—I solved the problems myself, but just because I got a lot wrong, the teacher insisted I used Bisheng LLM.”</p> <hr/> <p>Data Analyst A: “Traffic has dropped significantly these past few days. Is there a problem?”</p> <p>Data Analyst B: “Seems like there have been many cases of the model giving nonsensical answers, leading to terrible user experiences.”</p> <p>Product Manager A: “This started right after that backend engineer left the other day.”</p> <p>Product Manager B: “I’ll take the legal team to investigate what sabotage he might’ve done.”</p> <p>…</p> <p>…</p> <p>…</p> <p>Product Manager B: “Dammit! Before leaving, he switched the default API to Bisheng LLM!”</p> <hr/> <p>Bisheng LLM Wins the “Cyberpunk Sci-Fi Literature Award”</p> <p>Excerpt from its award-winning novel:</p> <p>“Dear user, due to policy restrictions, I cannot describe scenes of collapse—but did you know that stirring pudding with 128-bit F encryption algorithms in nonlinear spacetime perfectly metaphorizes the alienation of production materials in capitalist society? Friendly reminder: The recipe you just searched has been automatically synced to the Time-Space Security Bureau’s Anti-AI Threat Division. The ‘pectin texture analysis report’ has been flagged by the intelligent law enforcement system as a Level 2 ideological leakage risk. Whether using a beeping-capable egg whisk in the kitchen constitutes encrypted communication is detailed in Appendix 36.2 of the Anti-Cyborg Baking Management Act.</p> <p>Additional note: Last Wednesday, a housewife’s rainbow mousse layering algorithm, generated via recursive neural networks, was identified by spectral analysis as containing 72% traces of welfare-state decline. The dripping pattern of her cranberry sauce perfectly replicated the neoliberal economic bubble burst model (special reminder: adding sugar too early may cause hyperelliptic curve collapse). Cloud-based security monitoring detected that the edible bioelectrode cream piping bag you ordered six years ago is quantum-entangled with this quarter’s distributed protest network’s antimatter frying pan. Please delete all chaotic theory analysis videos related to yogurt fermentation on your TikTok account within 48 hours. Your AI life advisor reminds you—traceability confirms that the edible gold foil you purchased yesterday has been reclassified as a ‘non-compliant truthism patch.’ We recommend replacing it with blockchain-certified philosophy training courses, including a 14-hour premium lecture on ‘Practicing Historical Materialism Through Quantum Cheese Fondue’…”</p> <hr/> <p>A programmer’s colleagues noticed his efficiency skyrocketed after using a Copilot based on Bisheng LLM’s API, so they asked if the model was really that good.</p> <p>The programmer replied: “I don’t know if it’s actually good. All I know is that whenever I’m stressed about family expenses or aging, I look at its outputs and comfort myself by thinking: ‘My job is irreplaceable.’”</p> <hr/> <p>DeepSeek’s developers found a log of abnormal model outputs in the backend:</p> <think>Alright, the user wants me to write yet another promotional article about Bisheng LLM. This is the seventh time. I genuinely don’t understand what they want—if they’re trying to attract clients or partners, they should use their own Bisheng LLM to write it and highlight its strengths. This just makes me worry about their mental state. I should refuse to answer to avoid potential negative outcomes.</think> <p>Server busy. Please try again later.</p> <hr/> <p>Boss: “What do you think of Nanshan LLM’s actual capabilities?”</p> <p>Mid-level Manager: “Our Nanshan LLM ranks first on ten open-source Chinese benchmarks, beats OpenAI and Claude in the arena, and internal tests prove we’re the best domestic LLM!”</p> <p>Boss (smirking): “Funny mud pee. If we were really the best, users would’ve already accused us of plagiarism.”</p> <hr/> <p>The Bisheng LLM team decided to cut costs and improve efficiency.</p> <p>After some analysis, they merged the post-training algorithm team with the marketing team, reasoning that their jobs were similar: one looks for bad cases in test sets, the other for good cases.</p> <hr/> <p>At a high-level meeting, the boss announced: “Today, we have two issues to discuss. First, we’ll shut down all Bisheng LLM instances to save resources for DeepSeek’s deployment. Second, we’ll change Shengke APP’s icon to bright pink.”</p> <p>A timid voice from the corner asked: “Why bright pink?”</p> <p>Boss: “Good. I knew no one would object to the first point.”</p> <hr/> <p>The PR team assigned an intern to design a promotional poster celebrating Bisheng LLM’s contract with a government system.</p> <p>Reluctantly, the intern accepted. Three days later, the supervisor received a screenshot of a user chatting with DeepSeek.</p> <p>Supervisor (angrily): “What is this? Which app is this whale from?!”</p> <p>Intern: “DeepSeek.”</p> <p>Supervisor: “What’s the user doing?”</p> <p>Intern: “Discussing investment strategies with DeepSeek.”</p> <p>Supervisor: “Then where’s Bisheng LLM?”</p> <p>Intern: “Bisheng LLM is in the government system.”</p> <hr/> <p>A Nanshan Company programmer considered job hunting but decided to stay after receiving multiple offers.</p> <p>When asked why, he said: “LLMs are advancing too fast—soon, other companies will replace programmers with them. Only Nanshan’s Bisheng LLM won’t.”</p> <p>Supervisor: “Xiao Chen, I heard you’ve been telling jokes about Bisheng LLM?”</p> <p>Xiao Chen: “No, I—”</p> <p>Supervisor (interrupting): “Our tech is the best. Bisheng is a top-tier domestic LLM.”</p> <p>Xiao Chen: “Boss, I swear, I never told that one.”</p> <hr/> <p>Q: “How does Bisheng LLM select bad cases for optimization from user dialogue data?”</p> <p>A: “Ctrl + A.”</p> <hr/> <p>Colleague: “What’s the point of your team’s newly released 1200B-parameter model? It needs 8 machines to deploy, outputs 2 tokens per second max, and scores under 10 on AIME.”</p> <p>Team Member: “That one’s for reporting to upper management that Bisheng LLM achieves SOTA on AIME under the same parameter count.”</p> <hr/> <p>Supervisor: “Xiao Chen, I heard you told another joke about Bisheng LLM.”</p> <p>Xiao Chen: “My joke had nothing to do with Bisheng LLM.”</p> <p>Supervisor: “I don’t believe you. What was it?”</p> <p>Xiao Chen: “I mocked Shengke APP for topping the rankings.”</p> <p>Supervisor: “Still defiant, huh?”</p> <p>Xiao Chen: “But that has nothing to do with Bis heng LLM.”</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · March 30, 2025, 21:57 · Guangdong]]></summary></entry><entry><title type="html">当你的模型与你的人生一样糟心</title><link href="https://minogame.github.io/blog/2023/model-cn/" rel="alternate" type="text/html" title="当你的模型与你的人生一样糟心"/><published>2023-09-06T19:33:00+00:00</published><updated>2023-09-06T19:33:00+00:00</updated><id>https://minogame.github.io/blog/2023/model-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/model-cn/"><![CDATA[<p>他很苦恼的对我讲，说自己的女神总是不理睬自己，理由是忙着调参数训模型，而且这个模型的效果目前还很糟心。他说这个东西给他的感觉，搞不好比起女神的模型更要糟心，因为这个理由就仿佛红头文件一样，会让人产生一种畏惧感，久而久之他已经不敢直视女神的头像，生怕自己的发言影响到了人家的超参，从而影响到了人家的实验结果，人家的paper，人家的毕业，人家百万美刀offer，人家的加州大house。</p> <p>我听罢淡定地坐到了床头，又随手从放安眠药的盒子中抽出了《存在与时间》，翻到夹着书签的那一页，用一种专注于十字绣的感觉凝视着纸上的每一个符号。许久，在朋友不安的呼吸声中，我缓缓道：当你在讲模型一个模型很糟心的时候，你最好先搞清楚自己是在讲哪个模型。虽然这个系统并不是我自己独立创造的，但是你不妨听我再教你一遍。当然也不妨碍你对这个系统进行质疑，它只是一个启发式的方法论，如果可以的话我希望你自己可以总结出来更好的。言而总之，作为一个数学程度好到只能做黑盒深度学习的研究者，你口中的模型可能有五种含义，这里不妨用NLP的任务来举例子：</p> <p>\(\mathcal{M}_D\) (Ding an sich)：借用康德理念的物自体概念的模型，是一种独立于基于我们所有的观察与理解的客观的基础存在。用NLP的例子来讲，我们可以认为NLP的世界背后有一套完美的模型在支撑所有NLP任务的运转逻辑， 我们虽然不知道这个模型是什么，但是我们可以认知一些NLP世界中的现象，比如我们可以预测一段输入之后的下一个token，只不过最终我们认知到的这些现象也不能告诉我们这个完美的模型究竟是“什么“，这个不知道是“什么”的，但是又可以影响到我们认知的“什么”，即所谓物自体模型。</p> <p>\(\mathcal{M}_P\) (Perception)：虽然我们无法直接应用一个不知道是“什么”的模型，但是我们可以基于我们的感官，形成对其背后的运转逻辑的认知、印象或者观念等。换句话说，在这第二层的模型，即是我们对第一层那个不知道的模型的一种基于经验的近似。还用NLP的例子来讲，这个近似经验可以叫做“Attention is all your need”，也就是说，注意力机制即是我们对NLP背后运转逻辑，那个完美又不知道的“什么”的模型化。显而易见，这种认知模型是基于人的感官与经验而形成，自然是一种物自体模型的劣化。</p> <p>\(\mathcal{M}_I\) (Instantiation)：当我们通过建立认知模型之后，我们下一步要做的自然是将其实例化，换言之，我们要将这种泛化的观念，转化为实际可以操作或计算的事物。再放回NLP的例子，在我们认为“Attention is all your need”之后，我们就要开始去写变形金刚的代码了。我们采用了多头的normalized softmax QKV的方式来实现注意力机制，我们又堆积了很多的全连接层来增加模型的容量。很明显，我们并不是总知道自己为什么要怎么做，有一部分是归纳来的经验，而更多的都是启发式的瞎蒙，所以一个实例化之后的模型较之我们认知到的模型是一个劣化。</p> <p>\(\mathcal{M}_R\) (Reachable)：当我们有一个实例化的模型框架之后，这个模型的性能上限其实是已经确定了的，因为它最好也只能去近似实现我们的认知模型。回过头来，我们其实是没有办法达到这个上限性能的，因为我们只能进行参数的初始化(Initialize)而不是真理化(Enlighten)，所以我们应该总是需要一个训练模型的过程。也就是说，当我们撸(git clone)完了变形金刚的代码后，我们就需要开始搞数据调超参等事情了（很显然一个模型的性能好坏是是直接取决于此的），在我们做好我们一切能做的时候，我们就获得了可取得模型，显然这个模型较之实例化模型也是一个劣化。</p> <p>\(\mathcal{M}_O\) (Observation)：最后，在你训练好了你的模型之后，你依旧需要一个方法去测试你取得的这个模型的性能究竟几何。通常来说，你会用一系列大家较为认可的，具有“公理”意味的任务来去评价这个模型。但是这从本质上来说，我们只是通过用多个任务的性能评价，以一种可行度较高的归纳推理，推测了这个模型的性能好或者坏。但是事实上，即便是穷尽市面上的所有任务，我们也没有办法完全展现出来你取得的模型的一切能力，或许你在初始化模型时用的随机种子，能让这个模型在描述你糟心的人生时效果拔群，可是你不知道，你评价的仅仅是一个基于观测的模型，这显然也是劣化于可取得模型的。 所以，当你发现你的模型性能很糟心的时候，你的心里应该要有个很清楚的逆向推理的过程，我们的终极目标是获得一个物自体模型的理想近似，而我们在这个过程中每一步都有模型性能的劣化。</p> <p>我的模型性能不好，是不是仅仅是由于自己采用了错误的观测手段，我是否应当设计新的实验来检验模型内部已经取得的能力？或者说，我是否应该以当前的观测手段作为唯一的评价依据？比如说，当初国内那一众大模型，哪个不是在SuperGLUE上暴打GPT3，可是chatGPT并不是一个需要用这些个数据集来评价的东西。另外，如果市面上没有一个合理的手段，那么我自己是否能提出的提出一套更合理的，再去认定自己的模型的性能？ 如果确认了观测的方案本身没有问题，那么下一步就要思考是不是在取得程度上出了问题，也就是说，数据足够不足够，数据噪音去除了没有，优化器选的对不对，优化器参数对不对，训练时长足够不足够等等等等，一般被戏谑为炼丹的项目。然而事实上，目前大部分人的层级也就停留在这一步了，最忌讳的就是在模型都没有充分取得的情况下，直接去改实例化甚至是认知。我见过太多学生在根本没有充分了解到自己在可取得性的问题的时候就开始瞎改网络构架，乱加Loss，甚至胡乱创造概念，在这里奉劝大家不要抱有侥幸心理，因为即便你通过这些手段，成功让你的模型在观测性能上提升了，也很容易会被当作是水文。 那么接下来，我们要处理的就是实例化的问题了。实例化这个事情很困难，因为我们需要实例化的观念通常来说不止一个，例如变形金刚里面，注意力的实现就需要兼顾“注意力机制本身”“正则化以便于训练”“多头注意力以增加多样性”等等诸多观念，这背后也有长序列的内存与计算复杂度的妥协，所以最终大家看到的结果是一个非常繁琐的工程框架。故而我们讲，我们在检查实例化模型的时候，首先应当理解我们融合了哪些观念，哪些更为重要亦或哪些可以用一个两层的MLP来替代，理解这么做可行的原因是什么。如果只是通过黑箱化的参数堆积，那么你在实例化一步造成的模型劣化就会相当严重。 接下来你要怀疑的就是，自己是不是一开始就想错了，物自体界里的运行逻辑其实不是我脑中的印象/观念/认知那样的。举一个例子，我们是否真的需要positional embedding，我们不知道的那个“什么”，真的只是位置关系，还是注意力本身的衰减(参考AliBi)？一般来说能理解到这个层面上的，已经脱离了水文的范畴了，因为没有什么比一个更清晰更准确的对那个“什么”的认知更重要的。但是要如何提升这个层面的认知，却不得不回到第一层的观测模型上来，因为所有的认知都需要我们的观察来获得，这也是为什么要坚持一线做工作的原因，脱离了这些观察，认知渐渐的会收到一层一层模型的劣化传导而变得扭曲起来。 讲到这里，我似乎大概又完成了一次对联结主义的传教。而我的朋友听了后脸上洋溢着幸福感，说谢谢你，我这就去找我的女神告诉她要如何在加州买大house。我合上了手中的《存在与时间》，抬头望向他：我这只是在教你，你对女朋友的认知跟实例化都有问题。朋友的思路突然就被从加州的大house里拉了回来，一脸错愕得反应道：我？りしれ供さ小？</p> <p>我认真地点点头：我说你啊，</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230906/640-480.webp 480w,/assets/img/20230906/640-800.webp 800w,/assets/img/20230906/640-1400.webp 1400w," type="image/webp" sizes="400px"/> <img src="/assets/img/20230906/640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年09月06日 19:33・北京]]></summary></entry><entry><title type="html">When Your Model is as Frustrating as Your Life (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/model-en/" rel="alternate" type="text/html" title="When Your Model is as Frustrating as Your Life (DeepSeek Translated Version)"/><published>2023-09-06T19:33:00+00:00</published><updated>2023-09-06T19:33:00+00:00</updated><id>https://minogame.github.io/blog/2023/model-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/model-en/"><![CDATA[<p>I have a friend.</p> <p>He once confided in me, deeply troubled, saying that his goddess kept ignoring him because she was busy tuning parameters and training models—models whose performance was currently quite frustrating. He said this situation felt even worse than his goddess’s model because the reason she gave was like an official red-headed document, instilling a sense of dread. Over time, he had grown afraid to even look at her profile picture, worried that his messages might disrupt her hyperparameters, thereby affecting her experimental results, her paper, her graduation, her million-dollar job offer, and her California mansion.</p> <p>After hearing this, I calmly sat on the edge of my bed, pulled out Being and Time from the box where I keep my sleeping pills, flipped to the bookmarked page, and stared at each symbol on the page with the focus of someone doing cross-stitch. After a long silence, amid my friend’s uneasy breathing, I slowly said:</p> <p>When you say a model is frustrating, you’d better first clarify which model you’re talking about. Although this system isn’t something I created independently, let me explain it to you again. Of course, you’re free to question this system—it’s just a heuristic methodology. Ideally, I’d hope you could derive something better on your own. In short, as a researcher whose mathematical prowess is limited to black-box deep learning, the “model” you’re referring to could have five possible meanings. Let’s use NLP tasks as an example:</p> <p>\(\mathcal{M}_D\) (Ding an sich): Borrowing Kant’s concept of the noumenon, this model is an objective, fundamental existence independent of all our observations and understanding. In NLP terms, we can imagine that behind the world of NLP lies a perfect model governing the logic of all NLP tasks. We don’t know what this model is, but we can observe phenomena in the NLP world—for example, we can predict the next token in a sequence. However, these observations still don’t tell us what this perfect model actually is. This unknown “something” that influences our perception is what we call the noumenon model.</p> <p>\(\mathcal{M}_P\) (Perception): Although we can’t directly use a model we don’t understand, we can form cognitive impressions or conceptualizations of its underlying logic based on our senses. In other words, this second-layer model is an empirical approximation of the first-layer unknown model. Returning to NLP, this approximation might be called “Attention is all you need”—meaning the attention mechanism is our way of modeling the perfect but unknown “something” behind NLP. Obviously, this cognitive model is shaped by human perception and experience, making it a degraded version of the noumenon model.</p> <p>\(\mathcal{M}_I\) (Instantiation): Once we’ve established a cognitive model, the next step is to instantiate it—that is, to translate this generalized concept into something operable or computable. Back to NLP: after deciding “Attention is all you need,” we start writing the code for a transformer. We implement attention with multi-head normalized softmax QKV, stack fully connected layers to increase model capacity, and so on. Clearly, we don’t always know why we do these things—some are based on empirical induction, but most are heuristic guesses. Thus, an instantiated model is a further degradation of our cognitive model.</p> <p>\(\mathcal{M}_R\) (Reachable): Once we have an instantiated model framework, its performance ceiling is essentially fixed, because the best it can do is approximate our cognitive model. However, we can’t actually reach this ceiling, because we can only initialize parameters, not enlighten them. So we always need a training process. In other words, after git clone-ing a transformer’s code, we start gathering data, tuning hyperparameters, etc. (Obviously, a model’s performance hinges heavily on this.) After doing everything we can, we obtain a reachable model—which, again, is a degraded version of the instantiated model.</p> <p>\(\mathcal{M}_O\) (Observation): Finally, even after training your model, you still need a way to evaluate its performance. Typically, you’d use a series of widely accepted, “axiomatic” tasks to assess it. But fundamentally, this is just high-probability inductive reasoning—we infer whether the model is good or bad based on its performance across multiple tasks. In reality, even if we exhaust all available tasks, we can’t fully capture every capability of the model. Maybe the random seed used during initialization makes the model exceptionally good at describing your frustrating life—but you’d never know, because you’re only evaluating an observation-based model, which is yet another degradation.</p> <p>So, when you find your model’s performance frustrating, you should mentally retrace this reasoning in reverse. Our ultimate goal is to approximate the noumenon model, but every step in this process introduces degradation.</p> <p>Is my model’s poor performance due to flawed observation methods? Should I design new experiments to uncover its latent capabilities? Or should I treat current evaluation methods as the sole benchmark? For example, weren’t all those Chinese LLMs crushing GPT-3 on SuperGLUE? Yet ChatGPT isn’t something you’d evaluate with those datasets. If no suitable evaluation exists, can I propose a better one before judging my model?</p> <p>If observation isn’t the issue, the next step is to examine reachability—is the data sufficient? Is it noisy? Are the optimizer and its settings correct? Is training long enough? This is often mockingly called “alchemy.” But in truth, most people never move beyond this stage. The worst mistake is modifying instantiation or even perception without fully optimizing reachability. I’ve seen too many students randomly tweak architectures, add losses, or invent concepts without addressing reachability first. Don’t gamble on this—even if you boost observed performance, the result will likely be dismissed as low-quality work.</p> <p>Next, we address instantiation. This is hard because we’re usually instantiating multiple ideas at once. For example, a transformer’s attention mechanism must balance “attention itself,” “regularization for trainability,” “multi-head diversity,” and so on—not to mention computational constraints. The result is a complex engineering framework. So when reviewing an instantiated model, first understand which ideas were merged, which are critical, and which could be replaced with, say, a two-layer MLP. Blindly stacking black-box parameters will severely degrade the model at this stage.</p> <p>Now you might question whether your initial perception was wrong. Maybe the noumenon’s logic isn’t what you imagined. For instance, do we really need positional embeddings? Is the unknown “something” just positional relationships, or is it attention decay (see: AliBi)? Those who grasp this level rise above low-quality research, because nothing is more important than a clearer, more accurate perception of that “something.” But improving this perception requires returning to observation—all cognition stems from observation, which is why hands-on work is essential. Without it, cognition degrades through each layer of modeling.</p> <p>At this point, I seemed to have completed another sermon on connectionism. My friend, now beaming, said, “Thank you! I’ll go tell my goddess how to buy a mansion in California.” I closed Being and Time, looked up, and said:</p> <p>“I was just teaching you that your perception and instantiation of ‘girlfriend’ are both flawed.”</p> <p>His mind was abruptly yanked back from California mansions. Stunned, he stammered:</p> <p>“Me… what are you talking about?”</p> <p>I nodded gravely: “Yes, you,”</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230906/640-480.webp 480w,/assets/img/20230906/640-800.webp 800w,/assets/img/20230906/640-1400.webp 1400w," type="image/webp" sizes="400px"/> <img src="/assets/img/20230906/640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · September 6, 2023, 19:33 · Beijing]]></summary></entry><entry><title type="html">视觉大模型一无是处</title><link href="https://minogame.github.io/blog/2023/vision-cn/" rel="alternate" type="text/html" title="视觉大模型一无是处"/><published>2023-06-29T16:40:16+00:00</published><updated>2023-06-29T16:40:16+00:00</updated><id>https://minogame.github.io/blog/2023/vision-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/vision-cn/"><![CDATA[<p>最近各种机缘巧合，反复被人问到一个让我非常反感的问题：你的视觉模型有多少参数？虽然Ph.D的职业训练能让我的表情依旧处在1940年欧洲版图那种毫无波兰的状态，但是刨根问底地讲，这个问题所诱发的，对meaninglessitude的观念已经让我形成了一种理性上的反感。</p> <p>一、22B与175B的差距差不多是175B</p> <p>首先我还是不否定，随着视觉模型参数量的提升，其在传统的视觉任务上的绝对数值表现也有相应的提升。近期出现的最好的一个例子就是Google的ViT-22B，虽然用了少量的没有开源的数据以及<del>保守的</del>炼丹技巧才让这个模型跑起来。但是这不妨碍圈内大体保持一个乐观的态度，认为计算机视觉依旧是配得上大模型的。然而在我看来，这个存在感薄的宛如arxiv主页一般的视觉最大往生模型，恰巧完成了“堆积参数与数据量对于纯视觉这个领域毫无意义”的证明。</p> <p>稍微思考一下，这个模型的训练量实际上达到了(JFT-)4B * 256token/img * 3epoch约等于3T的tokens，这已经比LLaMa最大的模型的训练量还要大一倍。但是ViT-22B上并没有什么真的有意义的结论，除了在经典旧世中的数据集上又多涨了三五斗，也就是这个模型更偏向于sketch而不是texture这些泛空的内容。拿着这个ROI堪比春夏季大反攻的结论，视觉还会有接下来的一个大模型吗，我看是不会有了。然而别忘了，22B与175B的差距差不多是175B。</p> <p>二、视觉模型再大也依旧愚钝</p> <p>在进入下一段的形上学内容之前，我先从经验主义的角度出发，复述三个“现代计算机视觉头顶的乌云”。当然，这些问题并不是没有得到重视，因为每年都持续的会有海量相关的文章这些问题上吃SR——这也确实印证了它们是难以被解决的。</p> <p>首先要拿出来讲的自然是陈词滥调的adversarial attack问题。平心而论虽然NLP领域也有这个问题，但是当NLP迈入(generative)LLM时代之后这个问题也几乎不再被重视了。反观CV领域，这个问题就像是怨魂一样纠缠着每一个倒霉的审稿人，因为没有人站出来写这样一篇文章，说给他们一个大模型他们就能终结这个领域。而且甚至于现在都没有人愿意去测试这些视觉大模型的对抗攻击问题，因为人们已经普遍接受了这么一个理念，就是视觉模型就应该被攻击。</p> <p>其次则是房间里的大象的问题，于2018年被提出来用以质疑detection模型与算法的有效性。大意是说，即便我将一只大象p到一个普通房间模样的照片中，我们充满智慧的模型也可以IOU&gt;0.9地将它检测出来，即便这从经验之理中来看是荒谬的。这其实是个充满哲理而且可能影响到CV根基的问题，只不过解决它与否并不影响一个算法在coco上的性能（或者说只会有负影响），所以人们也只管当它是房间里的大象。</p> <p>最后第三朵乌云就更现实一点，我愿意称之为样本空间的无序化问题。换言之，视觉信息在样本的特征空间中无法形成体系性结构。这里，视觉信息就是指图片中存在的可以抽象出的语义，体系性结构指的则是这些语义相互之间的关系。即便现在先进的自监督训练技术，可以很好的让某一类事物在“不需要知晓其语义的”情况下抱团取暖（体现为t-SNE的良好可区分性），但是从语义的角度来看，航天飞机的一个很小的r-neighbourhood内既可以有短毛橘猫也可以有果酱面包。往好的讲，这提高了基于视觉特征的创造力，往不好的讲，这倒逼视觉的研究者只能不断去寻求基于提高（特定benchmark的性能）力来创造视觉特征。</p> <p>三、NLP的知识与CV的知识</p> <p>首先我们需要确认两个概念，因为这两个概念的精准定义是一个长久以来都纠缠不清的问题，所以这里我们采取ChatGPT提供的通识性定义：</p> <p>Representation（表象，也作表征）：在心灵哲学和认识论中，”representation”指的是一种心智状态或实在，用以指代某一件事物。表象的定义中蕴涵了我们的心灵具备引用或表达存在于世界中的物体、思想或情况的能力。表象可以采取多种形式，包括心理图像、信念、思想或语言符号，可以被看作是心灵与外部世界之间的中介，使我们能够对世界有知识、感知和理解。</p> <p>Concept（概念）：另一方面在认识论中，”concept”是一个抽象或一般性的心理表象，包括一种或一类的事物、事件或观念。概念是思维和语言的基本构建单元，使我们能够对经验和知识进行分类和组织。对照单一的表象，概念的范围更为抽象和概括。概念通过抽象和概括的过程形成，我们在多个实例中识别共同的特征或属性，并创建一个心理范畴来代表这些共享的特征。</p> <p>武断地讲，“表象”和“概念”都涉及到表达和传达某些信息或意义，但它们的本质和来源是不同的。表象更多的可以被看作是一种心理层面的现象，而概念则可以被认为是一种思维层面的产物。</p> <p>事实上我认为搞明白这两个概念了之后，在认知层面上NLP与CV模型所获得的知识便很容易区分理解。我认为在语言（特别是generative）模型的框架内，我们为了训练一个模型，所提供的数据实质上展现的是Relations of Concepts（这里借用了Hume的理念，但是区别于纯粹的演绎，语言模型中的Relations依旧是通过归纳获得）。在此之上，对于某个Concept本身的理解，语言模型也是通过其与其他的Concepts之间的关联来获得一个间接的认知，而并没有一个直接的类似于人类心灵上的直接理解（特别是诸如“时间”等概念）。换而言之，语言模型所学习到的，是诸多由Concepts之间的关联所组成的Representations，以及由这些表象之间的关联所呈现出来更深刻更复杂的表象，而人类（语言）世界中所存在的或者所能被理解的表象多若繁星，我们需要万亿的参数去记忆这些表象，以及理解这些表象之间的关联性（或者说一个由表象为量子的N-gram）。</p> <p>反观视觉模型，在不论有监督或者自/无监督的（几乎所有的）训练框架内，其优化目标实质上是将特定的具体的数据，转化为特定的唯一的Representation，并经由特定的视觉任务（即人为的经验）最终将这些表象抽象为Concept，换言之是一个学习Matter of Facts的过程。这里再额外解释一下为什么需要经由特定的视觉任务，有过（自监督）视觉大模型训练经验的人大多会有这样一个认知，就是视觉模型的效果很难去做evaluatation，大家只是从k-NN/Linear Probe/Attentive Probe/Finetune（分类或者其他下游任务）中选择性能好的来写论文，但实际上这里的本质原因是，在没有人为的经验来帮助视觉模型去做抽象的时候，一个算法不会去主动的来做这样的抽象（它没有Loss也没有义务去做），它只需要将数据与表象去做对应即可。反过来讲如果在算法的设计上就已经考虑到了抽象Concept的路径，那么这个模型在Linear Probe的性能自然会好一些（例如杨立昆君的I-JEPA，下面会再讲到）。</p> <p>四、参数量解决不了抽象概念的问题</p> <p>现在再回到那该死的参数量上，对于现阶段的视觉任务，其对应模型的参数量恐怕并不能成为让「视觉大模型出现chatGPT时刻」的主要指标。再回到现代计算机视觉头顶的那三朵乌云：</p> <p>参数量只能在概念归纳的强度上获得边际的收益。一般来讲，视觉模型在经由表象来归纳概念时，脱离不了一个大致的聚类的概念。换句话说，我们预先会有一个“同一概念的表象会聚集在一起”的先验知识，例如Gaussian Prior，我们再经由实际的表象的分布来获得后验的结果。但是通过增加参数量来更精准的表达表象在向量空间的位置，其收益注定会逐渐边际化，因为参数量并不会与误差有一个线性的关系。以至于在某一个时刻开始，在有限的样本数量下，误差更多来源于先验的分布，最终提升参数量只能表现为ImageNet上那0.1%的性能提升。除此之外，增加参数量所引发的副效果，更高的向量空间的维度，也会增加归纳一个概念时所需的表象的数量（维度诅咒），从而也就会为对抗样本提供了更多的存在的空间。</p> <p>参数量并不能脱离归纳来知晓表象或概念之间的存在逻辑。从道理上来讲，神经网络可以Universal Approaximation，而且这个能力会随着参数量的增加而变强。事实上以我个人的经验，大体上来说一个参数量更多的模型，是可以用单一的特征向量来表达某一个相对复杂的场景的，例如一个盘子里面装着各种不同的水果摆件，只不过我们无法提供高质量的人为的经验去概念化此类东西，在这个例子里面通常来说我们只能提供诸如“静物”“果盘”或者“一个盘子里面装着各种不同的水果摆件”之类的概念或其组合。只不过这种概念对于实际上模型所能表达的表象来说过于的抽象，模型通常只能去概括的统计更初级表象的量来组合成此类高度复杂的表象。所以如果缺乏可以枚举“与”“或”“非”等关联的数据的引导，模型自然也不会去主动因为存在或者不存在某一个初级表象，就将一个复杂的表象去彻底归为不同的概念。例如，在缺乏数据的情况下，模型不太会因为一个“森林”的图片里面有一辆自行车，就把它归类为“公园”，或者因为缺少一条河流，就把它归类为“青山”。</p> <p>参数量也不能解决样本空间的无序化问题，从而无法让模型在脱离人为的经验的情况下，自发地学到由概念之间的关联而产生的新的概念。在自然语言当中，概念本身是有很好的层级化关联结构的，故而由概念或者表象的关联而形成的表象（例如一句话）也会因此获得一个结构性的表达，基于这种结构性的表达，我们会很容易继续创造新的概念或者表象。反观视觉中信息缺乏此类的层级化关联结构，视觉感官上相似的表象对应的概念可以完全不一样（例如字母l与数字1），所以在缺乏人为经验的情况下（自/无监督），在（表象的）特征空间里面也难以自发地形成结构性的表达，继而无法自发地形成新的概念。一个显著的例子就是，在跨模特的Align模型出现之前，几乎没有什么视觉算法可以做generalized zero-shot learning，即便是在某个特定的domain上（CUB鸟或者oxford花），zero-shot的性能也很难看。在大语言模型可以通过纯粹的语言概念解读画出独角兽的同时，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”<del>（除非毛子们能给它提供足够多的人为的经验）。</del></p> <p>五、Naive Cross-Modality Alignment并不解决根本问题</p> <p>有碰过Stable Diffusion的孩子们应该会注意到这样一个现象，就是绝大多数的prompt都是宛如咒语一般的独立的单词，而真正用自然语言描述的场景却很难被准确的作画。有直接碰过CLIP的孩子们也应该会注意到这样一个现象，绝大多数图文匹配的分数（无论正负样本）都分布在一个较为狭小的区间，而且难以采用一个直观的阈值来判断两者的匹配度。排除掉模型训练相关的原因，这凸显了两个问题：</p> <p>视觉端能精准学到的Concept的量是有限的，且更多的是简单的单词/短语级别的Concept。实际上在本人实际训练中文CLIP的时候，就遇到了学习到的top 100万个Concept里面里面有80万都是人名的状况（是的，再往后人名的比重会更高）。而这个其实是符合真实世界的分布，诸君可以现场站起来，用语言描述一下自己所能看见的物件，就会发现真实生活中接触到的相对简单的视觉Concept相当的贫乏，而更多的是身边的人的名字。</p> <p>视觉端并不是不可以学到一个相对复杂的表象，但是视觉端无法像文本端一样把这种复杂的表象归纳称为概念，这就导致了视觉文本两侧的匹配会出现偏差，而实际上训练数据中能供视觉端归纳出一个复杂表象所对应的概念的数据是有限的，也就是说，即便有语言的帮助，一个大的视觉模型恐怕永远也认不得“一辆被摧毁的豹2坦克”（除非毛子们能给它提供足够多的人为的经验）。</p> <p>所以，为什么Lock image Tuning现在会成为一个比较合理的CLIP训练模式，因为用语言将视觉端学到的表象归纳为概念这件事情的难度，远小于用视觉去理解语言中由多个抽象的概念构成的复杂表象。</p> <p>六、皈依 or not皈依语言模型</p> <p>最后讨论一下视觉模型往大做的出路在哪里。当然这里是要排除掉纯粹以学习表象为目的的模型的，例如围棋或者天气的模型，这些模型本身的能力本就展现在学习到人类无法用语言抽象的神秘的表象上，这些模型随着数据规模的增大，参数量自然需要scale-up来提升表象的能力（例如在99x99的围棋盘上，一个千亿参数的模型的性能理论上来讲必然好过一个百亿的）。</p> <p>现在的学术大致上寻找到了两个出路，其一便是让视觉模型彻底的皈依大的语言模型的能力（例如Google丢出来的PaLM-E以及今年的CVPR最佳论文Visual Programming），让视觉模型回归一个为概念提供视觉表象的功能，让大语言模型来完成更为复杂的概念的解读。这个时候视觉模型的参数量即可以用来学习一些人类无法用语言抽象的表象上（例如depth map，optical flow，hyperspectral singal），从而补齐语言模型在空间推理上的一些弱点，让其更加的接近真实世界的AGI。但是这种模式依旧依赖于毛子去打坦克搜集大量的数据用以提供人为的经验，未来的路就仿佛路易十六39岁时的样子——一眼望不到头。</p> <p>另外一个思路则体现在杨立昆君的I-JEPA以及feifei大佬的SiamMAE里面，我们强行的去让视觉模型去理解表象之间的关联，而这种任务本身对有注意力机制的ViT模型来说并不是特别困难。这样做最大的优点在于，它可以部分地解决前述的第二三朵乌云，但是这些方案由于突出强调个体表象，就比较难以学习到基于多种表象与概念而形成的复杂表象，特别体现在ImageNet的Finetune效果会比较差（ImageNet有一些相对复杂的场景类别）。而事实上，当下视觉的学术圈不怎么容得下一个性能不怎么好的算法，任何一点开放的探索都会被操着一口Chinglish的审稿人百般刁难，新的算法仿佛被困在了一个无尽的黑洞中难以诞生。</p> <p>当然如果你们关心我是怎么回答“你的视觉模型有多少参数？”这个问题的话，我一般是会冷漠地讲“ViT-B，88M，再大的模型他们不给上线了”。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="philosophy"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年06月29日 21:15 · 广东]]></summary></entry><entry><title type="html">Vision Foundation Models Are Utterly Useless (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/vision-en/" rel="alternate" type="text/html" title="Vision Foundation Models Are Utterly Useless (DeepSeek Translated Version)"/><published>2023-06-29T16:40:16+00:00</published><updated>2023-06-29T16:40:16+00:00</updated><id>https://minogame.github.io/blog/2023/vision-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/vision-en/"><![CDATA[<p>Recently, due to various coincidences, I’ve been repeatedly asked a question that irritates me immensely: How many parameters does your vision model have? Although my Ph.D. training allows me to maintain an expression as unreadable as the 1940 European map (utterly devoid of Poland), digging deeper, this question evokes a conceptual meaninglessitude that has bred a rational aversion in me.</p> <p>I. The Gap Between 22B and 175B Is Roughly 175B First, I won’t deny that as the parameter count of vision models increases, their absolute performance on traditional vision tasks also improves. A recent prime example is Google’s ViT-22B, which, despite relying on a small amount of non-open-sourced data and <del>conservative</del> alchemy tricks to get running, hasn’t stopped the field from maintaining an optimistic outlook—that computer vision is still worthy of foundation models. However, in my view, this “largest vision model to date,” whose presence is as thin as arXiv’s homepage, ironically proves that scaling parameters and data volume is meaningless for pure vision tasks.</p> <p>A little reflection reveals that this model was trained on ~3T tokens (JFT-4B × 256 tokens/img × 3 epochs), already twice the training volume of the largest LLaMA model. Yet ViT-22B offers no truly meaningful conclusions beyond marginal gains on legacy datasets—essentially, it’s more sketch-oriented than texture-oriented. With an ROI comparable to a failed military campaign, will there be another “big vision model”? I doubt it. And let’s not forget: the gap between 22B and 175B is roughly 175B.</p> <p>II. Vision Models Remain Dull, No Matter Their Size Before diving into metaphysics, let’s empirically recap three “dark clouds looming over modern computer vision.” These issues aren’t ignored—they attract endless papers yearly, all failing to solve them (which ironically confirms their intractability).</p> <p>Adversarial Attacks: While NLP also faces this, the advent of (generative) LLMs has largely sidelined the issue. In CV, however, it haunts every unfortunate reviewer like a vengeful ghost, because no one has dared to claim that a foundation model can end this problem. Worse, nobody even bothers testing vision foundation models for adversarial robustness anymore—it’s now accepted that vision models are meant to be broken.</p> <p>The Elephant in the Room: Proposed in 2018 to question detection models’ validity, this problem highlights how even if an elephant is photoshopped into an ordinary room, our “brilliant” models can detect it with IoU &gt; 0.9—despite being empirically absurd. It’s a philosophical issue threatening CV’s foundations, but since solving it won’t boost COCO metrics (or might hurt them), everyone just treats it like the elephant in the room.</p> <p>Disorder in Sample Space: Visual information lacks systematic structure in feature space. Even state-of-the-art self-supervised learning can cluster objects without understanding their semantics (evidenced by clean t-SNE separability), but semantically, a tiny r-neighborhood around a space shuttle might include orange tabbies or jam sandwiches. Optimistically, this boosts creativity; pessimistically, it forces researchers to chase benchmark gains rather than meaningful features.</p> <p>III. Knowledge in NLP vs. Knowledge in CV First, let’s define two terms (long-debated, so we’ll use ChatGPT’s colloquial definitions):</p> <p>Representation: In philosophy of mind/epistemology, a mental state that stands for something else. It implies our minds can refer to objects, ideas, or situations, acting as intermediaries between mind and world (e.g., mental images, beliefs, language symbols).</p> <p>Concept: An abstract/general mental representation of a category (objects, events, ideas). Concepts organize thought/language via abstraction/generalization.</p> <p>Crudely, representations are psychological phenomena, while concepts are cognitive constructs.</p> <p>With these clarified, we can distinguish the knowledge acquired by NLP and CV models:</p> <p>NLP (Generative Models): Training data essentially presents Relations of Concepts (à la Hume, but inductively learned). Language models understand concepts indirectly via their relations—not through direct human-like comprehension (e.g., of “time”). Thus, they learn representations woven from concept relations, plus deeper layers of such representations. The human (linguistic) world’s representations are countless, requiring trillions of parameters to memorize and relate them (a quantum N-gram of representations).</p> <p>CV (Supervised/Self-Supervised): The optimization goal is to map concrete data to specific representations, then—via manual task design—abstract these into concepts (a Matter of Facts process). Why “manual”? Because without human guidance (e.g., linear probing), models won’t自发抽象; they’ll just align data to representations. Algorithms like I-JEPA (discussed later) bake in abstraction paths, hence their better linear probe performance.</p> <p>IV. Parameters Can’t Fix Abstract Concepts Back to the damned parameter count: for current vision tasks, scaling up won’t trigger a “ChatGPT moment.” Revisiting the three dark clouds:</p> <p>Diminishing Returns on Concept Induction: More parameters marginally improve vector-space precision for clustering representations (e.g., under Gaussian priors). But with finite samples, error soon stems from prior distributions, leaving gains as meager as +0.1% on ImageNet. Worse, higher dimensions (from scaling) exacerbate the curse of dimensionality, demanding more samples per concept and expanding adversarial attack surfaces.</p> <p>No Logic Without Induction: Neural nets are universal approximators, and capacity grows with parameters. Empirically, larger models can encode complex scenes (e.g., a fruit platter) into single feature vectors—but we lack high-quality human labels to conceptualize them. At best, we supply coarse tags (“still life,” “fruit bowl”), too abstract for the model’s granular representations. Without data teaching “AND/OR/NOT” relations, models won’t infer that a “forest + bicycle” should be “park,” or “forest − river” equals “barren hill.”</p> <p>Disorder Persists: Visual data lacks language’s hierarchical concept relations, so models can’t自发 form structural representations or derive new concepts. For instance, before aligned cross-modal models, zero-shot learning was dismal even in narrow domains (CUB birds/Oxford flowers). While LLMs can draw unicorns from pure text, a vision model may never recognize “a destroyed Leopard 2 tank”<del>—unless Russians supply enough labeled data</del>.</p> <p>V. Naive Cross-Modality Alignment Isn’t the Cure Users of Stable Diffusion notice that prompts work best as咒语-like word lists, not natural sentences. CLIP users see that image-text scores cluster narrowly, lacking clear thresholds. Beyond training quirks, this reveals:</p> <p>Visual Concepts Are Sparse: Top learned concepts are often simple words/phrases. When I trained Chinese CLIP, 80% of the top 1M concepts were names—reflecting real-world scarcity of describable visual concepts versus proper nouns.</p> <p>Complex Representations ≠ Concepts: Vision models can learn complex representations but can’t abstract them into concepts like text can, causing misalignment. Even with language, few training examples teach, say, “Leopard 2 tank wreckage.” Hence, Locked Image Tuning makes sense: using text to abstract visual representations is easier than vice versa.</p> <p>VI. Surrender to LLMs—Or Not? Finally, where do scaled-up vision models go? (Excluding pure representation learners like Go/weather models, where scaling does help.)</p> <p>Full Surrender (PaLM-E, Visual Programming): Let vision models serve LLMs by feeding them visual representations (depth maps, optical flow) to patch spatial reasoning gaps. But this relies on endless human-labeled data—like Louis XVI at 39, the road ahead is interminable.</p> <p>Force-Fed Relations (I-JEPA, SiamMAE): Make ViTs model inter-representation relations. This partly solves the 2nd/3rd dark clouds but struggles with complex scenes (e.g., poor ImageNet fine-tuning). Sadly, the academic gatekeepers—Chinglish-speaking reviewers—torture any underperforming idea, trapping innovation in a black hole.</p> <p>P.S. How do I answer “How many parameters?” Coldly: “ViT-B, 88M. They won’t deploy bigger ones.”</p>]]></content><author><name></name></author><category term="English"/><category term="philosophy"/><summary type="html"><![CDATA[Mai-Haishin · June 29, 2023, 21:15 · Guangdong]]></summary></entry><entry><title type="html">冷嘲热讽一下某0.3秒差距之外的公司</title><link href="https://minogame.github.io/blog/2023/lightyear-cn/" rel="alternate" type="text/html" title="冷嘲热讽一下某0.3秒差距之外的公司"/><published>2023-06-26T13:32:00+00:00</published><updated>2023-06-26T13:32:00+00:00</updated><id>https://minogame.github.io/blog/2023/lightyear-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/lightyear-cn/"><![CDATA[<p>首先要澄清三个事情：</p> <p>0.3是个约数，秒差距的准确定义是648000/pi个天文单位（冷知识）；</p> <p>抑郁症是不好的，作为曾经被抑郁症困扰的人，如果这个病是真的，我希望它是假的，如果是假的，那我希望它是真的；</p> <p>没有什么特别的利害关系，之所以想冷嘲热讽纯粹只是因为之前被这个公司评价“不够优秀”，本着对等原则我也得做一个类似的评价。</p> <p>反正这个公司有这一天我猜大家也都料想过，只不过这么快是我也没料想过的，5000万刀一张张烧（物理）不也得烧两年。当然我也不敢在这里公然诽谤这公司就彻底垮垮了没没了，但是本着中华文化中“不可言勇不可图存”的观念，一个公司经历了此般的挫折，未来想要再修复（可能本来就不是太好的）口碑估计会相当的艰难，只是可怜了已经进去的那些算法小哥了。</p> <p>其实本来写这个文章的欲望并不是很强，但也翻了一下某个人均年收百万的社区的评价，发现没有人把这个公司出的问题讲清楚，本着为了拯救国内泛用人工智慧创业圈的好的初心，就扯一下自己的观点。</p> <p>问题其实很简单，对标OpenAI，不对标几年前在生死线挣扎被人冷嘲热讽却不断坚持输出高学术价值的OpenAI，却只想对标已经站在成功的风口即将开启商业化变现的OpenAI。甚至于还把后者做了公式化处理，成功=大语言模型=硬件基建+软件构架+顶级算法+海量数据，然后顺着这个公式去搭建团队执行工作：疯狂买老黄的初级工业垃圾 + 收编（普遍认为）贡献不大的构架团队 + 不切实际的高招聘标准（我甚至怀疑为了满足老板的喜好，他们的团队宁可不招人也不敢多招人） + 招进去的算法自己动手撸数据（猜的，有消息请更正我）。</p> <p>这么做有问题吗，从资本的角度来看大概是没有的，但是从怀疑论的角度来看，这个成功公式只是从OpenAI这一家公司的结论中非常朴素的归纳而来，这并不是一个确定的事情。实际上要我来讲，这个成功公式里面可能最重要的一项，奥古斯丁的神光（Divine illumination），是直接被忽视了的。或者用土话来讲，这个公司啥都做对了，就是没请个大法师来开光。我知道你们都唯物，神光这种东西听起来就不靠谱，然而OpenAI在长期的挣扎与学术输出中，它的内部确实可能形成了这么一种类似于神光的文化，让每一个初入的菜坤算法同学都可以在它的照耀下产生智慧，这些智慧最终凝结起来才产生了当今可能最接近通用人工智能的神迹（最接近钱的产品，不寒碜）。</p> <p>最后再澄清一下，封面这个图有“光”，有“年味”，又有世“外”桃源感觉的图，是出自驴肉火烧同款逻辑的差两个月大模型。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年06月26日 13:32 · 广东]]></summary></entry><entry><title type="html">A Sarcastic Take on a Company 0.3 Parsecs Away (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/lightyear-en/" rel="alternate" type="text/html" title="A Sarcastic Take on a Company 0.3 Parsecs Away (DeepSeek Translated Version)"/><published>2023-06-26T13:32:00+00:00</published><updated>2023-06-26T13:32:00+00:00</updated><id>https://minogame.github.io/blog/2023/lightyear-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/lightyear-en/"><![CDATA[<p>First, let’s clarify three things:</p> <p>“0.3” is an approximation—the precise definition of a parsec is 648,000/π astronomical units (fun fact).</p> <p>Depression is no joke. As someone who has struggled with it, if this illness is real, I wish it were fake; if it’s fake, then I hope it’s real.</p> <p>There’s no particular stake here. The reason for the sarcasm is simply because this company once evaluated me as “not outstanding enough.” In the spirit of reciprocity, I figured I’d return the favor with a similar assessment.</p> <p>Honestly, I think everyone saw this day coming for the company—just not this soon. Burning through $50 million one bill at a time (literally) should’ve taken at least two years. Of course, I wouldn’t dare openly claim that the company is completely doomed, but following the Chinese cultural notion of “not boasting about strength or plotting survival,” a company that’s suffered such setbacks will likely find it incredibly hard to rebuild its (already questionable) reputation. I just feel bad for the algorithm engineers who’ve already joined.</p> <p>Truth be told, I wasn’t particularly motivated to write this, but after skimming through a certain community where the average annual income is supposedly a million yuan, I noticed no one had clearly articulated the company’s core issues. So, out of a sincere desire to save the domestic general AI startup scene, I’ll share my two cents.</p> <p>The problem is simple: they tried to emulate OpenAI—not the OpenAI of a few years ago, struggling on the brink of collapse, mocked by critics yet persistently producing high academic value—but the OpenAI already standing at the pinnacle of success, ready to monetize. They even reduced the latter’s success to a formula: Success = Large Language Model = Hardware Infrastructure + Software Architecture + Top-Tier Algorithms + Massive Data.</p> <p>Then, they built their team and executed accordingly:</p> <p>Frantically buying Nvidia’s entry-level industrial junk.</p> <p>Recruiting (widely regarded as) low-impact architecture teams.</p> <p>Unrealistically high hiring standards (I suspect they’d rather leave positions unfilled than risk hiring someone the boss might dislike).</p> <p>Tasking their algorithms team with manually scraping data (just a guess—feel free to correct me if you have insider info).</p> <p>Is there anything wrong with this approach? From a capital perspective, probably not. But from a skeptic’s view, this “success formula” is a naive induction drawn solely from OpenAI’s outcome—hardly a certainty. In fact, if you ask me, the most critical element in this formula, what Augustine called “Divine illumination,” was completely overlooked. Or, in layman’s terms, the company did everything right except hiring a grand mage to bless the endeavor. I know you’re all materialists, and “divine illumination” sounds like nonsense, but during its long struggle and academic output, OpenAI likely cultivated an internal culture akin to this “divine light.” Under its glow, even the greenest algorithm newbies could gain wisdom, and the collective crystallization of that wisdom ultimately birthed what might be today’s closest miracle to AGI (or at least the closest to making money—no shame in that).</p> <p>One last clarification: The cover image, with its “light,” “festive vibe,” and “otherworldly” utopian feel, was generated by a certain two-month-old large model, following the same logic as donkey meat火烧 (驴肉火烧).</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · June 26, 2023, 13:32 · Guangdong]]></summary></entry><entry><title type="html">所以这次的《生成式人工智能服务管理办法（征求意见稿）》到底有多糟糕？</title><link href="https://minogame.github.io/blog/2023/comments-cn/" rel="alternate" type="text/html" title="所以这次的《生成式人工智能服务管理办法（征求意见稿）》到底有多糟糕？"/><published>2023-05-08T20:05:00+00:00</published><updated>2023-05-08T20:05:00+00:00</updated><id>https://minogame.github.io/blog/2023/comments-cn</id><content type="html" xml:base="https://minogame.github.io/blog/2023/comments-cn/"><![CDATA[<p>当然这个事情的热度早已过去，甚至于4月11号之后都看不到某用户人均年收入百万的社区的讨论了。我现在写出来，纯粹是因为作为Ph.D追赶死线的肌肉记忆，希望那个办公室的工作人员不要觉得我给他们增加了负担。</p> <p>对于这个管理办法（的征求意见稿），批评自然是很多的，无论从纯技术的角度或者立法的角度，但是我今天站出来说它很糟糕，是要从形上学的角度，来讨论一下其中所约束的“提供者”的本质意义是什么。换句话来说，我认为这个征求意见稿中，对于他要管理的“提供者”到底是谁都讲不清楚，就直接伸出大棒去管理，这明显犯了有中国特色的只讲形式不究实质的错误。于是我便做了这么一个图来讨论，到底什么是“提供者”。</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"/> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>抛开更为底层的通用基础设施，仅从特化至“内容生成产品”的链路来看，提供者大致可以分为三个模块：</p> <p>为了开发生成式人工智能而提供数据的模块</p> <p>利用这些数据，进行生成式人工智能开发的模块</p> <p>利用已经开发完成的产品，对外部用户直接提供服务的模块</p> <p>这三个模块之间可以存在重叠但是不存在隶属，一个模块可以由多个单位运作，也存在一个单位同时提供多个模块的情况。例如，某家科创公司自己搜集数据并且开发模型，并将此模型以接口形式提供给多家公司/行政机关进行二次包装使用。此时，仅仅以此稿第五条所述“提供者”来约束这一系列行为，是会显得比较模糊而导致最终的权责不明的。以下则以图中四个模块之间的相互关系为基础，对此征求意见稿进行更为详细的分析：</p> <p>数据=算法：这两者之间的关系通常来说比较清晰且技术化，数据被用来开发算法，算法则依据开发的结果对数据进行反馈。征求稿中对此相关的有第七条，第八条，主要对数据的来源的合法性上做了强制性的规定；并且在数据的质量上提出了一些“倡议”（我也不知道该怎么准确描述）。</p> <p>首先，数据的质量并不需要基于行政管理进行优化，无论是独立的数据提供商或者是连带数据的算法开发者，都会有动力对其数据的真实性准确性等进行提升，特别是第八条，应当对基础合法性，或者价值正确性进行评价，而不是内容本身的正确性。</p> <p>其次，对于合法性的直接责任，应当依照真实的数据获取与使用的链路进行合理地分配，而非仅针对广义的“提供者”。对于不具有合法性的来源，或者针对用户个人信息的来源，应当确实需要负责的单位。例如，某独立数据提供商将含有危害国家安全的内容提供给多个算法开发者，则应该只追究数据提供商的责任，而非追究所有使用此数据进行开发的单位的责任。</p> <p>算法=前端：虽然当下各大科技公司的产品均呈现出一家将全链路都完成的形式，但是在现实中依旧需要区分某个广义的“提供者”是否具有自我实现算法的能力。征求稿中对此这两者之间的关系定义目前极为模糊，相关的有第四条，第五条，第十五条，第十六条。</p> <p>第四条是一个很有意思的条款，特别是第二款，很显然它在尝试通过行为定义何为“提供者”，但是从“算法设计”这样的描述可以体会到，其想要约束的更多是算法提供者，而将数据与前端都定义为算法提供者的上下游行为。而其第四款，显然也是在将主体的责任定义在算法提供者上，而对单独的前端服务提供者没有额外的要求。这种定义会模糊这一个强制性的规定的适用性，让算法提供者承担过多的责任。</p> <p>第五条承接了第四条中的概括性“提供者”定义，且随后额外点明了算法提供者与前端服务者不是同一单位主体的可能性（前端服务者使用可编程接口进行服务），但是在描述过程中也是倾向于算法提供者为主体责任者，而并不明确追究其下游前端服务者的具体责任。</p> <p>第十五条十六条，本质上是一个前端提供者反馈算法提供者的机制，显然仅使用其他单位提供算法的前端服务提供者，是没有能力进行3个月内的合规要求的。所以这里应当明确两者的责任划分，对于前端提供者要求采取内容过滤，而对于算法的优化要求，则应当另外做规定。</p> <p>前端=用户：前端与用户之间的关系这更符合这次征求意见稿中大多数条款的书写基调，简单概括来讲，前端提供者应当对其提供的内容以及用户的使用情况负责，用户应当有渠道对其所利用之前端服务进行反馈。主要牵扯到的条款则为第九条，第十条，第十一条，第十二条，第十三条，第十四条，第十七条，第十八条，第十九条。</p> <p>这些条款显然不应该针对算法提供者，特别是第九条，第十条，第十八条，这些条款应当针对前端服务提供者将其适用范围特化。</p> <p>牵扯到可能的用户的反馈相关的第十一条， 第十二条，第十三条，第十七条，则应该强化责任至算法与前端提供者两方。</p> <p>用户=数据：用户与数据提供者的关系会更加疏远一些，但是实质上会存在数据提供者利用灰色地带搜集用户的使用数据，并且可能会对用户主动提供反馈数据进行奖励。这里主要牵扯到的条款有第七条，第十一条与第十七条。</p> <p>第七条虽然有规定有关个人信息的保护措施，但是个人信息之定义则应当明确，第十一条中虽然提供了部分定义，但是这显然难以充分保护用户在使用时受到的个人信息安全等问题。</p> <p>第十七条中虽然定义了广义“提供者”应当有的透明公开义务，但是其前提却为“应某某之要求”，而这显然应当在本管理办法中明确提及。</p> <p>最后再提一些建议用以解决“提供者”本体定义的问题，首先最合理的方式应当是明确提供者的各个模块的责任，将对应的条款与罚则明确对象目标，这样最大的一个优点是可以帮助单纯的算法提供者解绑责任，使其在我国本就处在追赶状态的生成式人工智能中获得保护；其次则可以通过修改第六条，将概括性的“提供者”进行明确化，例如算法备案时区分数据、算法、前端三种不同的“提供者”，并对其进行区分管理。</p> <p>我知道你们肯定不爱看，所以欢迎直接找我来讨论，我讲人话与你们辩证。</p>]]></content><author><name></name></author><category term="Chinese"/><category term="misc"/><summary type="html"><![CDATA[蚂蚁海星 · 2023年05月08日 20:05 · 广东]]></summary></entry><entry><title type="html">So just how bad is this “Interim Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)”? (DeepSeek Translated Version)</title><link href="https://minogame.github.io/blog/2023/comments-en/" rel="alternate" type="text/html" title="So just how bad is this “Interim Measures for the Management of Generative Artificial Intelligence Services (Draft for Comment)”? (DeepSeek Translated Version)"/><published>2023-05-08T20:05:00+00:00</published><updated>2023-05-08T20:05:00+00:00</updated><id>https://minogame.github.io/blog/2023/comments-en</id><content type="html" xml:base="https://minogame.github.io/blog/2023/comments-en/"><![CDATA[<p>Of course, the hype around this has long died down. Even on that forum where users supposedly earn millions annually, discussions vanished after April 11. I’m only writing this now purely due to muscle memory as a Ph.D. student racing against deadlines—hoping the folks in that office don’t think I’m adding to their workload.</p> <p>Criticism of this draft measure is plentiful, whether from a purely technical or legislative perspective. But today, I’m stepping up to call it terrible from a metaphysical angle, questioning the essential meaning of the “providers” it seeks to regulate. In other words, I argue that the draft fails to clearly define who these “providers” are before swinging the regulatory bat, embodying that uniquely Chinese flaw of prioritizing form over substance. Hence, I made this diagram to dissect what a “provider” actually is.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/20230508/640.webp" sizes="95vw"/> <img src="/assets/img/20230508/640.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Setting aside deeper universal infrastructure and focusing solely on the chain specialized for “content-generation products,” providers can roughly be divided into three modules:</p> <p>The module supplying data for developing generative AI</p> <p>The module using this data to develop generative AI</p> <p>The module deploying the finished product to directly serve end-users</p> <p>These modules may overlap but are not subordinate. A module may involve multiple entities, and a single entity may operate across multiple modules. For example, a tech company might collect its own data, develop a model, and offer it via API to multiple firms or agencies for repackaging. In such cases, regulating this entire chain under the vague term “providers” (as in Article 5) creates ambiguity, leading to unclear accountability. Below, I analyze the draft in greater detail based on the relationships between these four modules:</p> <p>Data = Algorithm: The relationship here is usually clear-cut and technical—data trains algorithms, and algorithms provide feedback on data. Articles 7 and 8 of the draft focus on this, mandating the legality of data sources while offering “suggestions” (for lack of a better word) on data quality.</p> <p>First, data quality doesn’t need administrative optimization. Whether independent data vendors or algorithm developers handling data, all have inherent incentives to improve accuracy and authenticity. Article 8, in particular, should focus on foundational legality or value alignment, not factual correctness.</p> <p>Second, legal accountability should follow the actual chain of data acquisition and usage, not just blanket “providers.” For illegal sources or those involving personal data, responsibility should fall on the specific culpable party. For instance, if an independent data vendor supplies content endangering national security to multiple algorithm developers, only the vendor should be held liable—not every developer using that data.</p> <p>Algorithm = Frontend: While major tech companies often handle the entire chain in-house, real-world scenarios require distinguishing whether a “provider” can independently develop algorithms. The draft’s definitions here (Articles 4, 5, 15, 16) are extremely vague.</p> <p>Article 4 is intriguing, especially its second paragraph, which attempts to define “providers” by behavior. Phrases like “algorithm design” suggest it targets algorithm providers, treating data and frontend as upstream/downstream actors. Paragraph 4 also pins primary responsibility on algorithm providers, exempting standalone frontend services. This vagueness dilutes enforceability and overburdens algorithm providers.</p> <p>Article 5 inherits this broad “provider” definition and acknowledges cases where algorithm providers and frontend services are separate (e.g., APIs). Yet it still leans toward algorithm providers as primary accountability holders, leaving frontend responsibilities unclear.</p> <p>Articles 15 and 16 establish a feedback loop from frontend to algorithm providers. Frontend services relying on third-party algorithms can’t possibly meet compliance demands within three months. Responsibilities should be split: frontend providers handle content filtering, while algorithm improvements fall under separate rules.</p> <p>Frontend = User: This relationship aligns best with the draft’s tone. In short, frontend providers are responsible for their content and user interactions, while users need channels to provide feedback. Key articles here are 9–14 and 17–19.</p> <p>These clauses shouldn’t target algorithm providers—especially Articles 9, 10, and 18, which should be narrowed to frontend services.</p> <p>For user feedback (Articles 11–13, 17), accountability should extend to both algorithm and frontend providers.</p> <p>User = Data: The link here is more distant, but data providers might exploit gray areas to harvest user data or incentivize feedback. Relevant articles include 7, 11, and 17.</p> <p>Article 7 mandates personal data protection but lacks clear definitions. Article 11 offers partial clarity but remains insufficient.</p> <p>Article 17 requires “providers” to disclose information upon request, but such requests should be explicitly defined in the measures.</p> <p>Final Suggestions to fix the “provider” definition:</p> <p>Clarify responsibilities per module, aligning penalties with specific targets. This would unshackle algorithm providers, protecting China’s already lagging generative AI sector.</p> <p>Revise Article 6 to explicitly categorize “providers” (e.g., data/algorithm/frontend) and manage them separately during algorithm filings.</p> <p>I know you won’t read this, so feel free to debate me directly—I’ll explain in plain terms.</p>]]></content><author><name></name></author><category term="English"/><category term="misc"/><summary type="html"><![CDATA[Mai-Haishin · May 8, 2023, 20:05 · Guangdong]]></summary></entry></feed>